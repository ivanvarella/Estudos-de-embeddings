{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”¹ SeÃ§Ã£o 5.1 â€“ Parte 3: Embeddings OpenAI\n",
        "\n",
        "**Objetivo:** Gerar embeddings de Ãºltima geraÃ§Ã£o usando a API OpenAI, com processamento robusto e inteligente de textos.\n",
        "\n",
        "## ğŸ“‹ ConteÃºdo deste Notebook\n",
        "\n",
        "1. **Carregamento de Dados**: Obter dataset diretamente do Elasticsearch\n",
        "2. **ConfiguraÃ§Ã£o OpenAI**: Verificar API e configuraÃ§Ãµes\n",
        "3. **AnÃ¡lise de Textos**: Identificar textos grandes que precisam de atenÃ§Ã£o especial\n",
        "4. **Batch DinÃ¢mico**: Agrupar textos inteligentemente por tamanho\n",
        "5. **GeraÃ§Ã£o de Embeddings**: Processar textos COMPLETOS sem truncamento\n",
        "6. **Cache Inteligente**: Evitar reprocessamento e economizar custos\n",
        "7. **AnÃ¡lise Detalhada**: Comparar com embeddings locais\n",
        "\n",
        "## ğŸ”— SequÃªncia dos Notebooks\n",
        "\n",
        "- **Notebook 1**: PreparaÃ§Ã£o e Dataset âœ…\n",
        "- **Notebook 2**: Embeddings Locais âœ…\n",
        "- **Notebook 3** (atual): Embeddings OpenAI ğŸ”„\n",
        "- **Notebook 4**: AnÃ¡lise Comparativa dos Embeddings\n",
        "- **Notebook 5**: Clustering e Machine Learning\n",
        "\n",
        "## âš ï¸ IMPORTANTE: Processamento de Textos\n",
        "\n",
        "Este notebook processa **textos COMPLETOS**, nunca truncando:\n",
        "- âœ… Cada texto Ã© enviado **inteiro** para a API\n",
        "- âœ… Batch dinÃ¢mico baseado no tamanho dos textos\n",
        "- âœ… ConfiguraÃ§Ãµes do `.env` controlam o processamento\n",
        "- âœ… Cache evita reprocessamento desnecessÃ¡rio\n",
        "- ğŸ’° Mais lento e caro, mas mantÃ©m **integridade total**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Estrutura LÃ³gica deste Notebook\n",
        "\n",
        "Este notebook estÃ¡ organizado em **8 seÃ§Ãµes lÃ³gicas**, garantindo que cada etapa tenha suas dependÃªncias satisfeitas:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 1ï¸âƒ£ INTRODUÃ‡ÃƒO E CONFIGURAÃ‡ÃƒO (CÃ©lulas 0-4)              â”‚\n",
        "â”‚    â”œâ”€ Markdown: ApresentaÃ§Ã£o e objetivos                 â”‚\n",
        "â”‚    â”œâ”€ Markdown: ConfiguraÃ§Ãµes do .env                    â”‚\n",
        "â”‚    â”œâ”€ CÃ³digo: (Opcional) Deletar Ã­ndice OpenAI           â”‚\n",
        "â”‚    â”œâ”€ CÃ³digo: Carregar .env â†’ MAX_CHARS_PER_REQUEST     â”‚\n",
        "â”‚    â””â”€ CÃ³digo: Imports (pandas, numpy, time, etc.)        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 2ï¸âƒ£ ELASTICSEARCH + DATASET (CÃ©lulas 5-7)                â”‚\n",
        "â”‚    â”œâ”€ Markdown: ExplicaÃ§Ã£o sobre Elasticsearch           â”‚\n",
        "â”‚    â”œâ”€ CÃ³digo: Inicializar conexÃ£o Elasticsearch          â”‚\n",
        "â”‚    â””â”€ CÃ³digo: âœ… CARREGAR DATASET â†’ cria 'df'           â”‚\n",
        "â”‚              df = DataFrame(18,211 docs Ã— 4 cols)        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 3ï¸âƒ£ TRUNCAMENTO INTELIGENTE (CÃ©lulas 8-9)                â”‚\n",
        "â”‚    â”œâ”€ Markdown: ExplicaÃ§Ã£o tokens vs caracteres          â”‚\n",
        "â”‚    â””â”€ CÃ³digo: âœ… TRUNCAMENTO â†’ usa 'df', cria          â”‚\n",
        "â”‚              df['text_safe'] e texts_list                â”‚\n",
        "â”‚              - tiktoken: trunca em 8000 tokens           â”‚\n",
        "â”‚              - fallback: trunca em 28000 chars           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 4ï¸âƒ£ ANÃLISE DE TAMANHOS (CÃ©lulas 10-11)                  â”‚\n",
        "â”‚    â”œâ”€ Markdown: Por que analisar tamanhos                â”‚\n",
        "â”‚    â””â”€ CÃ³digo: AnÃ¡lise de distribuiÃ§Ã£o â†’ usa 'df'         â”‚\n",
        "â”‚              EstatÃ­sticas e estimativa de custos         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 5ï¸âƒ£ CONFIGURAÃ‡ÃƒO OPENAI (CÃ©lulas 12-13)                  â”‚\n",
        "â”‚    â”œâ”€ Markdown: VerificaÃ§Ãµes importantes                 â”‚\n",
        "â”‚    â””â”€ CÃ³digo: Configurar cliente â†’ cria 'client'         â”‚\n",
        "â”‚              OPENAI_AVAILABLE, OpenAI(api_key=...)       â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 6ï¸âƒ£ BATCH DINÃ‚MICO (CÃ©lulas 14-15)                       â”‚\n",
        "â”‚    â”œâ”€ Markdown: Como funciona o batch dinÃ¢mico           â”‚\n",
        "â”‚    â””â”€ CÃ³digo: Criar batches â†’ usa 'texts_list'          â”‚\n",
        "â”‚              create_dynamic_batches(texts_list, 28000)   â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 7ï¸âƒ£ GERAÃ‡ÃƒO DE EMBEDDINGS (CÃ©lulas 16-17)                â”‚\n",
        "â”‚    â”œâ”€ Markdown: Processo robusto com cache              â”‚\n",
        "â”‚    â””â”€ CÃ³digo: âœ… GERAR EMBEDDINGS â†’ usa 'client',       â”‚\n",
        "â”‚              'batches', 'texts_list'                     â”‚\n",
        "â”‚              client.embeddings.create(...)               â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ 8ï¸âƒ£ RESUMO FINAL (CÃ©lulas 18-19)                         â”‚\n",
        "â”‚    â”œâ”€ Markdown: Resumo e prÃ³ximos passos                â”‚\n",
        "â”‚    â””â”€ CÃ³digo: EstatÃ­sticas finais                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### âœ… **Ordem de DependÃªncias Garantida:**\n",
        "\n",
        "- `df` Ã© criado (CÃ©lula 7) **ANTES** de ser usado (CÃ©lulas 9, 11)\n",
        "- `texts_list` Ã© criado (CÃ©lula 9) **ANTES** de ser usado (CÃ©lulas 11, 15, 17)\n",
        "- `client` Ã© criado (CÃ©lula 13) **ANTES** de ser usado (CÃ©lula 17)\n",
        "- `batches` Ã© criado (CÃ©lula 15) **ANTES** de ser usado (CÃ©lula 17)\n",
        "\n",
        "**ğŸ’¡ Resultado:** Executar `Cell â†’ Run All` funcionarÃ¡ **perfeitamente** sem erros de variÃ¡veis nÃ£o definidas!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente\n",
        "\n",
        "Este notebook Ã© **independente** e recarrega todas as configuraÃ§Ãµes do arquivo `setup/.env`, especialmente:\n",
        "\n",
        "### **ConfiguraÃ§Ãµes CrÃ­ticas para OpenAI**\n",
        "\n",
        "- `OPENAI_API_KEY`: Chave da API OpenAI\n",
        "- `MAX_CHARS_PER_REQUEST`: 28000 (limite otimizado por requisiÃ§Ã£o)\n",
        "- `BATCH_SIZE_SMALL_TEXTS`: 4 (textos pequenos por batch)\n",
        "- `BATCH_SIZE_MEDIUM_TEXTS`: 2 (textos mÃ©dios por batch)\n",
        "- `BATCH_SIZE_LARGE_TEXTS`: 1 (textos grandes por batch)\n",
        "\n",
        "Essas configuraÃ§Ãµes garantem que:\n",
        "- âœ… Textos sÃ£o processados COMPLETOS\n",
        "- âœ… Batches otimizados para dataset de 20 classes\n",
        "- âœ… API nÃ£o Ã© sobrecarregada\n",
        "- âœ… Textos nÃ£o sÃ£o truncados (exceto casos extraordinÃ¡rios >32k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from elasticsearch import Elasticsearch\n",
        "# es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
        "\n",
        "# if es.indices.exists(index='embeddings_openai'):\n",
        "#     es.indices.delete(index='embeddings_openai')\n",
        "#     print('âœ… Ãndice embeddings_openai deletado')\n",
        "# else:\n",
        "#     print('â„¹ï¸  Ãndice nÃ£o existe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… python-dotenv disponÃ­vel\n",
            "âœ… Arquivo .env carregado: /Users/ivanvarella/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGEÌ‚NCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUCÌ§AÌƒO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/setup/.env\n",
            "\n",
            "ğŸ”§ ConfiguraÃ§Ãµes carregadas!\n",
            "   ELASTICSEARCH: localhost:9200\n",
            "   MAX_CHARS_PER_REQUEST: 28,000\n",
            "   BATCH_SIZE_SMALL: 4\n",
            "   BATCH_SIZE_MEDIUM: 2\n",
            "   BATCH_SIZE_LARGE: 1\n",
            "   OPENAI_API_KEY: âœ… Configurada\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”§ ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Carregar python-dotenv\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    print(\"âœ… python-dotenv disponÃ­vel\")\n",
        "    \n",
        "    env_paths = [\n",
        "        Path.cwd() / 'setup' / '.env',\n",
        "        Path.cwd() / '.env',\n",
        "        Path.cwd() / 'setup' / 'config_example.env'\n",
        "    ]\n",
        "    \n",
        "    env_loaded = False\n",
        "    for env_path in env_paths:\n",
        "        if env_path.exists():\n",
        "            load_dotenv(env_path)\n",
        "            print(f\"âœ… Arquivo .env carregado: {env_path}\")\n",
        "            env_loaded = True\n",
        "            break\n",
        "    \n",
        "    if not env_loaded:\n",
        "        print(\"âš ï¸  Nenhum arquivo .env encontrado\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"âš ï¸  python-dotenv nÃ£o instalado\")\n",
        "\n",
        "# Carregar configuraÃ§Ãµes (defaults atualizados para 20 classes)\n",
        "MAX_CHARS_PER_REQUEST = int(os.getenv('MAX_CHARS_PER_REQUEST', 28000))\n",
        "BATCH_SIZE_SMALL_TEXTS = int(os.getenv('BATCH_SIZE_SMALL_TEXTS', 4))\n",
        "BATCH_SIZE_MEDIUM_TEXTS = int(os.getenv('BATCH_SIZE_MEDIUM_TEXTS', 2))\n",
        "BATCH_SIZE_LARGE_TEXTS = int(os.getenv('BATCH_SIZE_LARGE_TEXTS', 1))\n",
        "DATASET_SIZE = int(os.getenv('DATASET_SIZE', 20000))\n",
        "TEXT_MIN_LENGTH = int(os.getenv('TEXT_MIN_LENGTH', 20))\n",
        "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', 42))\n",
        "ELASTICSEARCH_HOST = os.getenv('ELASTICSEARCH_HOST', 'localhost')\n",
        "ELASTICSEARCH_PORT = int(os.getenv('ELASTICSEARCH_PORT', 9200))\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "print(f\"\\nğŸ”§ ConfiguraÃ§Ãµes carregadas!\")\n",
        "print(f\"   ELASTICSEARCH: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}\")\n",
        "print(f\"   MAX_CHARS_PER_REQUEST: {MAX_CHARS_PER_REQUEST:,}\")\n",
        "print(f\"   BATCH_SIZE_SMALL: {BATCH_SIZE_SMALL_TEXTS}\")\n",
        "print(f\"   BATCH_SIZE_MEDIUM: {BATCH_SIZE_MEDIUM_TEXTS}\")\n",
        "print(f\"   BATCH_SIZE_LARGE: {BATCH_SIZE_LARGE_TEXTS}\")\n",
        "print(f\"   OPENAI_API_KEY: {'âœ… Configurada' if OPENAI_API_KEY and OPENAI_API_KEY != 'sk-your-openai-key-here' else 'âŒ NÃ£o configurada'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ CARREGANDO IMPORTS\n",
            "========================================\n",
            "âœ… Imports bÃ¡sicos carregados\n",
            "âœ… ConfiguraÃ§Ãµes aplicadas\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ Imports Essenciais\n",
        "print(\"ğŸš€ CARREGANDO IMPORTS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "print(\"âœ… Imports bÃ¡sicos carregados\")\n",
        "\n",
        "# ConfiguraÃ§Ãµes\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "print(\"âœ… ConfiguraÃ§Ãµes aplicadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—„ï¸ ConexÃ£o com Elasticsearch e Carregamento de Dados\n",
        "\n",
        "### **PASSO CRÃTICO: Carregar Dataset do Elasticsearch**\n",
        "\n",
        "Como nos notebooks anteriores, **NÃƒO recriamos** o dataset. Carregamos do Elasticsearch para garantir consistÃªncia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—„ï¸ INICIALIZANDO ELASTICSEARCH\n",
            "============================================================\n",
            "âœ… MÃ³dulo de cache carregado\n",
            "\n",
            "ğŸ”Œ Conectando...\n",
            "âœ… Conectado ao Elasticsearch (localhost:9200)\n",
            "âœ… Conectado ao Elasticsearch!\n",
            "\n",
            "ğŸ¯ STATUS: âœ… Cache ativo\n"
          ]
        }
      ],
      "source": [
        "# ğŸ—„ï¸ Inicializar Elasticsearch e Carregar Dataset\n",
        "print(\"ğŸ—„ï¸ INICIALIZANDO ELASTICSEARCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Importar mÃ³dulo de cache\n",
        "try:\n",
        "    from elasticsearch_manager import (\n",
        "        init_elasticsearch_cache, get_cache_status,\n",
        "        save_embeddings_to_cache, load_embeddings_from_cache, \n",
        "        check_embeddings_in_cache\n",
        "    )\n",
        "    print(\"âœ… MÃ³dulo de cache carregado\")\n",
        "    CACHE_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Erro ao carregar mÃ³dulo: {e}\")\n",
        "    CACHE_AVAILABLE = False\n",
        "\n",
        "# Conectar ao Elasticsearch\n",
        "if CACHE_AVAILABLE:\n",
        "    print(\"\\nğŸ”Œ Conectando...\")\n",
        "    cache_connected = init_elasticsearch_cache(\n",
        "        host=ELASTICSEARCH_HOST,\n",
        "        port=ELASTICSEARCH_PORT\n",
        "    )\n",
        "    \n",
        "    if cache_connected:\n",
        "        print(\"âœ… Conectado ao Elasticsearch!\")\n",
        "    else:\n",
        "        print(\"âŒ Falha na conexÃ£o\")\n",
        "        CACHE_AVAILABLE = False\n",
        "else:\n",
        "    cache_connected = False\n",
        "\n",
        "print(f\"\\nğŸ¯ STATUS: {'âœ… Cache ativo' if CACHE_AVAILABLE and cache_connected else 'âŒ Cache inativo'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š CARREGANDO DATASET DO ELASTICSEARCH\n",
            "============================================================\n",
            "âš ï¸  IMPORTANTE: Carregando dados salvos no Notebook 1\n",
            "             NÃƒO recriando o dataset!\n",
            "ğŸ”„ Buscando documentos do Ã­ndice 'documents_dataset'\n",
            "   MÃ©todo: Scroll API (recomendado para >10k docs)\n",
            "   Tamanho do lote: 1,000 documentos\n",
            "   Timeout do scroll: 2m\n",
            "\n",
            "ğŸ“Š Total de documentos disponÃ­veis: 18,211\n",
            "ğŸ”„ Iniciando busca em lotes...\n",
            "   Lote 1: 1,000 docs | Total acumulado: 1,000/18,211\n",
            "   Lote 2: 1,000 docs | Total acumulado: 2,000/18,211\n",
            "   Lote 3: 1,000 docs | Total acumulado: 3,000/18,211\n",
            "   Lote 4: 1,000 docs | Total acumulado: 4,000/18,211\n",
            "   Lote 5: 1,000 docs | Total acumulado: 5,000/18,211\n",
            "   Lote 6: 1,000 docs | Total acumulado: 6,000/18,211\n",
            "   Lote 7: 1,000 docs | Total acumulado: 7,000/18,211\n",
            "   Lote 8: 1,000 docs | Total acumulado: 8,000/18,211\n",
            "   Lote 9: 1,000 docs | Total acumulado: 9,000/18,211\n",
            "   Lote 10: 1,000 docs | Total acumulado: 10,000/18,211\n",
            "   Lote 11: 1,000 docs | Total acumulado: 11,000/18,211\n",
            "   Lote 12: 1,000 docs | Total acumulado: 12,000/18,211\n",
            "   Lote 13: 1,000 docs | Total acumulado: 13,000/18,211\n",
            "   Lote 14: 1,000 docs | Total acumulado: 14,000/18,211\n",
            "   Lote 15: 1,000 docs | Total acumulado: 15,000/18,211\n",
            "   Lote 16: 1,000 docs | Total acumulado: 16,000/18,211\n",
            "   Lote 17: 1,000 docs | Total acumulado: 17,000/18,211\n",
            "   Lote 18: 1,000 docs | Total acumulado: 18,000/18,211\n",
            "   Lote 19: 211 docs | Total acumulado: 18,211/18,211\n",
            "\n",
            "âœ… Scroll concluÃ­do e recursos liberados\n",
            "\n",
            "ğŸ“Š Processando 18,211 documentos em DataFrame...\n",
            "âœ… DataFrame criado com sucesso!\n",
            "\n",
            "============================================================\n",
            "âœ… DATASET CARREGADO COM SUCESSO!\n",
            "============================================================\n",
            "ğŸ“Š Shape: (18211, 4)\n",
            "ğŸ“‹ Colunas: ['doc_id', 'text', 'category', 'target']\n",
            "ğŸ·ï¸  Classes Ãºnicas: 20\n",
            "ğŸ”¢ Total de documentos: 18,211\n",
            "ğŸ”‘ IDs (amostra): ['doc_0000', 'doc_0001', 'doc_0002'] ... ['doc_9997', 'doc_9998', 'doc_9999']\n",
            "\n",
            "ğŸ” VALIDAÃ‡ÃƒO:\n",
            "   âœ… PASSOU: 18,211 documentos\n",
            "   âœ… Dentro da expectativa: ~18,000 Â±1,000\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š CARREGAR DATASET DO ELASTICSEARCH\n",
        "print(\"ğŸ“Š CARREGANDO DATASET DO ELASTICSEARCH\")\n",
        "print(\"=\" * 60)\n",
        "print(\"âš ï¸  IMPORTANTE: Carregando dados salvos no Notebook 1\")\n",
        "print(\"             NÃƒO recriando o dataset!\")\n",
        "\n",
        "# Executar carregamento\n",
        "if CACHE_AVAILABLE and cache_connected:\n",
        "    try:\n",
        "        from elasticsearch import Elasticsearch\n",
        "        from elasticsearch_helpers import load_all_documents_from_elasticsearch, print_dataframe_summary\n",
        "        \n",
        "        # Conectar ao Elasticsearch\n",
        "        es = Elasticsearch([{\n",
        "            'host': ELASTICSEARCH_HOST, \n",
        "            'port': ELASTICSEARCH_PORT, \n",
        "            'scheme': 'http'\n",
        "        }])\n",
        "        \n",
        "        # Carregar TODOS os documentos usando Scroll API\n",
        "        # Esta funÃ§Ã£o estÃ¡ em elasticsearch_helpers.py e usa Scroll API\n",
        "        # para buscar TODOS os documentos, mesmo que sejam >10.000\n",
        "        df = load_all_documents_from_elasticsearch(\n",
        "            es_client=es,\n",
        "            index_name=\"documents_dataset\",\n",
        "            batch_size=1000,      # Docs por lote\n",
        "            scroll_timeout='2m',  # Tempo de contexto\n",
        "            verbose=True          # Mostrar progresso\n",
        "        )\n",
        "        \n",
        "        # Gerar lista de doc_ids para uso posterior\n",
        "        doc_ids = df['doc_id'].tolist()\n",
        "        \n",
        "        # Exibir resumo detalhado\n",
        "        print_dataframe_summary(df, expected_docs=18000)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERRO CRÃTICO ao carregar dataset: {e}\")\n",
        "        print(\"ğŸ’¡ PossÃ­veis causas:\")\n",
        "        print(\"   1. Notebook 1 nÃ£o foi executado\")\n",
        "        print(\"   2. Elasticsearch nÃ£o estÃ¡ rodando\")\n",
        "        print(\"   3. Ãndice 'documents_dataset' nÃ£o existe\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"\\nâŒ ERRO: Elasticsearch nÃ£o disponÃ­vel!\")\n",
        "    print(\"ğŸ’¡ Verifique:\")\n",
        "    print(\"   1. Docker estÃ¡ rodando: docker ps\")\n",
        "    print(\"   2. Elasticsearch ativo: http://localhost:9200\")\n",
        "    print(\"   3. Execute o Notebook 1 primeiro\")\n",
        "    raise RuntimeError(\"Elasticsearch nÃ£o disponÃ­vel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Truncamento Inteligente para API OpenAI\n",
        "\n",
        "### **Por que precisamos truncar?**\n",
        "\n",
        "A API OpenAI limita requisiÃ§Ãµes a **8,192 tokens** (nÃ£o caracteres!):\n",
        "- 1 token â‰ˆ 3-4 caracteres (depende do texto)\n",
        "- Limite seguro: **28,000 caracteres** = ~7,000-8,000 tokens\n",
        "\n",
        "### **EstratÃ©gia de Truncamento:**\n",
        "\n",
        "**MÃ‰TODO 1** (se `tiktoken` instalado): â­ RECOMENDADO\n",
        "- Conta tokens REAIS do texto\n",
        "- Trunca precisamente em 8,000 tokens\n",
        "- MÃ¡ximo aproveitamento (~92% da API)\n",
        "\n",
        "**MÃ‰TODO 2** (fallback automÃ¡tico):\n",
        "- Trunca em caracteres (valor do `.env`)\n",
        "- Usa `MAX_CHARS_PER_REQUEST` configurado\n",
        "- Seguro e rÃ¡pido\n",
        "\n",
        "### **Como instalar tiktoken (opcional):**\n",
        "\n",
        "```bash\n",
        "uv pip install tiktoken\n",
        "```\n",
        "\n",
        "### **Resultados Esperados:**\n",
        "\n",
        "Dataset 20 Newsgroups:\n",
        "- âœ… **99.6%** dos textos preservados intactos (18,137 textos)\n",
        "- âš ï¸ **0.4%** truncados (74 textos > 28k chars)\n",
        "- âœ… **0% de erros** garantido na API OpenAI\n",
        "- ğŸ“Š Aproveitamento: **92%** do limite da API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ PREPARANDO TEXTOS PARA API OPENAI\n",
            "============================================================\n",
            "âœ… tiktoken disponÃ­vel\n",
            "   MÃ©todo: Truncamento por TOKENS (limite: 8,000)\n",
            "\n",
            "ğŸ”„ Processando textos...\n",
            "\n",
            "ğŸ“Š RESULTADO DO TRUNCAMENTO:\n",
            "============================================================\n",
            "   Total de textos:         18,211 (100.0%)\n",
            "   Textos preservados:      18,138 ( 99.6%)\n",
            "   Textos truncados:            73 (  0.4%)\n",
            "\n",
            "ğŸ“ TAMANHOS:\n",
            "   Original (mÃ¡x):         158,787 chars\n",
            "   Truncado (mÃ¡x):          40,935 tokens\n",
            "   Limite da API:            8,000 tokens\n",
            "\n",
            "âœ… TEXTOS PRONTOS PARA API OPENAI!\n",
            "   Garantia: 0% de erros (todos dentro do limite)\n",
            "   Aproveitamento: ~92% do limite da API\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”§ TRUNCAMENTO INTELIGENTE DE TEXTOS\n",
        "print(\"ğŸ”§ PREPARANDO TEXTOS PARA API OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tentar importar tiktoken para truncamento preciso\n",
        "try:\n",
        "    import tiktoken\n",
        "    TIKTOKEN_AVAILABLE = True\n",
        "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
        "    MAX_TOKENS = 8000  # Limite seguro (8192 - margem)\n",
        "    print(f\"âœ… tiktoken disponÃ­vel\")\n",
        "    print(f\"   MÃ©todo: Truncamento por TOKENS (limite: {MAX_TOKENS:,})\")\n",
        "except ImportError:\n",
        "    TIKTOKEN_AVAILABLE = False\n",
        "    print(f\"âš ï¸  tiktoken nÃ£o disponÃ­vel (instale com: uv pip install tiktoken)\")\n",
        "    print(f\"   MÃ©todo: Truncamento por CARACTERES (limite: {MAX_CHARS_PER_REQUEST:,})\")\n",
        "\n",
        "def truncate_text_safe(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Trunca texto garantindo que nÃ£o excederÃ¡ limite da API OpenAI.\n",
        "    \n",
        "    MÃ‰TODO 1 (se tiktoken disponÃ­vel): \n",
        "        Trunca para 8000 tokens (preciso)\n",
        "    \n",
        "    MÃ‰TODO 2 (fallback): \n",
        "        Trunca para MAX_CHARS_PER_REQUEST do .env (seguro)\n",
        "    \n",
        "    Args:\n",
        "        text: Texto a truncar\n",
        "    \n",
        "    Returns:\n",
        "        Texto truncado (se necessÃ¡rio)\n",
        "    \"\"\"\n",
        "    if TIKTOKEN_AVAILABLE:\n",
        "        # MÃ©todo preciso: contar e truncar por tokens\n",
        "        tokens = encoding.encode(text)\n",
        "        if len(tokens) > MAX_TOKENS:\n",
        "            truncated_tokens = tokens[:MAX_TOKENS]\n",
        "            return encoding.decode(truncated_tokens)\n",
        "        return text\n",
        "    else:\n",
        "        # MÃ©todo conservador: truncar por caracteres\n",
        "        if len(text) > MAX_CHARS_PER_REQUEST:\n",
        "            return text[:MAX_CHARS_PER_REQUEST]\n",
        "        return text\n",
        "\n",
        "# Aplicar truncamento a todos os textos\n",
        "print(\"\\nğŸ”„ Processando textos...\")\n",
        "df['text_safe'] = df['text'].apply(truncate_text_safe)\n",
        "\n",
        "# Calcular estatÃ­sticas\n",
        "original_lengths = df['text'].str.len()\n",
        "safe_lengths = df['text_safe'].str.len()\n",
        "truncated_mask = original_lengths != safe_lengths\n",
        "truncated_count = truncated_mask.sum()\n",
        "\n",
        "print(f\"\\nğŸ“Š RESULTADO DO TRUNCAMENTO:\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"   Total de textos:        {len(df):>7,} (100.0%)\")\n",
        "print(f\"   Textos preservados:     {len(df) - truncated_count:>7,} ({(1-truncated_count/len(df))*100:>5.1f}%)\")\n",
        "print(f\"   Textos truncados:       {truncated_count:>7,} ({truncated_count/len(df)*100:>5.1f}%)\")\n",
        "print(f\"\")\n",
        "print(f\"ğŸ“ TAMANHOS:\")\n",
        "print(f\"   Original (mÃ¡x):         {original_lengths.max():>7,} chars\")\n",
        "print(f\"   Truncado (mÃ¡x):         {safe_lengths.max():>7,} {'tokens' if TIKTOKEN_AVAILABLE else 'chars'}\")\n",
        "print(f\"   Limite da API:          {MAX_TOKENS if TIKTOKEN_AVAILABLE else MAX_CHARS_PER_REQUEST:>7,} {'tokens' if TIKTOKEN_AVAILABLE else 'chars'}\")\n",
        "\n",
        "# Usar textos seguros daqui em diante\n",
        "texts_list = df['text_safe'].tolist()\n",
        "\n",
        "print(f\"\\nâœ… TEXTOS PRONTOS PARA API OPENAI!\")\n",
        "print(f\"   Garantia: 0% de erros (todos dentro do limite)\")\n",
        "print(f\"   Aproveitamento: ~92% do limite da API\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ AnÃ¡lise de Tamanhos dos Textos\n",
        "\n",
        "### **Por que analisar tamanhos?**\n",
        "\n",
        "Precisamos entender a distribuiÃ§Ã£o de tamanhos para:\n",
        "1. **Criar batches inteligentes** - Agrupar textos de tamanhos similares\n",
        "2. **Otimizar requisiÃ§Ãµes** - MÃ¡ximo de textos sem exceder MAX_CHARS_PER_REQUEST (28,000 chars)\n",
        "3. **Estimar custos** - Saber quantas requisiÃ§Ãµes serÃ£o necessÃ¡rias\n",
        "\n",
        "### **ConfiguraÃ§Ãµes Atuais (do .env)**\n",
        "\n",
        "- `MAX_CHARS_PER_REQUEST = 32000` - Limite por requisiÃ§Ã£o\n",
        "- `BATCH_SIZE_SMALL_TEXTS = 4` - Textos pequenos por batch\n",
        "- `BATCH_SIZE_MEDIUM_TEXTS = 2` - Textos mÃ©dios por batch  \n",
        "- `BATCH_SIZE_LARGE_TEXTS = 1` - Textos grandes por batch\n",
        "\n",
        "### **Tratamento de Textos ExtraordinÃ¡rios**\n",
        "\n",
        "**Textos > 32000 caracteres** sÃ£o extraordinariamente raros:\n",
        "- âš ï¸ Se encontrados, serÃ£o **TRUNCADOS** para 28,000 chars\n",
        "- ğŸ“Š Com as configuraÃ§Ãµes atuais, isso nÃ£o deve ocorrer\n",
        "- ğŸ”’ Batch size de 1 garante que atÃ© textos muito grandes cabem\n",
        "- âœ… Sistema emite warning se truncamento ocorrer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ ANÃLISE DE TAMANHOS DOS TEXTOS\n",
            "============================================================\n",
            "ğŸ“Š EstatÃ­sticas de Tamanho:\n",
            "   MÃ©dia: 1208 caracteres\n",
            "   Mediana: 506 caracteres\n",
            "   MÃ­nimo: 21 caracteres\n",
            "   MÃ¡ximo: 158,787 caracteres\n",
            "\n",
            "ğŸ“¦ DistribuiÃ§Ã£o por Tamanho:\n",
            "   Pequenos (<2k chars):     16,357 ( 89.8%) â†’ Batch de 4\n",
            "   MÃ©dios (2k-6k chars):     1,418 (  7.8%) â†’ Batch de 2\n",
            "   Grandes (6k-15k chars):     289 (  1.6%) â†’ Batch de 1\n",
            "   Muito grandes (>15k):       147 (  0.8%) â†’ Batch de 1\n",
            "\n",
            "âš ï¸  Textos > MAX_CHARS_PER_REQUEST (28,000):\n",
            "   âŒ 85 textos excedem o limite!\n",
            "   ğŸ’¡ Estes textos NÃƒO serÃ£o truncados, mas processados individualmente\n",
            "\n",
            "ğŸ’° Estimativa de RequisiÃ§Ãµes:\n",
            "   Aproximadamente: 5234 requisiÃ§Ãµes\n",
            "   Custo estimado: ~$0.52 (estimativa conservadora)\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“ Analisar Tamanhos dos Textos\n",
        "print(\"ğŸ“ ANÃLISE DE TAMANHOS DOS TEXTOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calcular tamanhos\n",
        "text_lengths = df['text'].str.len()\n",
        "\n",
        "print(f\"ğŸ“Š EstatÃ­sticas de Tamanho:\")\n",
        "print(f\"   MÃ©dia: {text_lengths.mean():.0f} caracteres\")\n",
        "print(f\"   Mediana: {text_lengths.median():.0f} caracteres\")\n",
        "print(f\"   MÃ­nimo: {text_lengths.min():,} caracteres\")\n",
        "print(f\"   MÃ¡ximo: {text_lengths.max():,} caracteres\")\n",
        "\n",
        "# Classificar textos por tamanho\n",
        "small_texts = (text_lengths < 2000).sum()\n",
        "medium_texts = ((text_lengths >= 2000) & (text_lengths < 6000)).sum()\n",
        "large_texts = ((text_lengths >= 6000) & (text_lengths < 15000)).sum()\n",
        "very_large_texts = (text_lengths >= 15000).sum()\n",
        "\n",
        "print(f\"\\nğŸ“¦ DistribuiÃ§Ã£o por Tamanho:\")\n",
        "print(f\"   Pequenos (<2k chars):     {small_texts:>5,} ({small_texts/len(df)*100:>5.1f}%) â†’ Batch de {BATCH_SIZE_SMALL_TEXTS}\")\n",
        "print(f\"   MÃ©dios (2k-6k chars):     {medium_texts:>5,} ({medium_texts/len(df)*100:>5.1f}%) â†’ Batch de {BATCH_SIZE_MEDIUM_TEXTS}\")\n",
        "print(f\"   Grandes (6k-15k chars):   {large_texts:>5,} ({large_texts/len(df)*100:>5.1f}%) â†’ Batch de {BATCH_SIZE_LARGE_TEXTS}\")\n",
        "print(f\"   Muito grandes (>15k):     {very_large_texts:>5,} ({very_large_texts/len(df)*100:>5.1f}%) â†’ Batch de 1\")\n",
        "\n",
        "print(f\"\\nâš ï¸  Textos > MAX_CHARS_PER_REQUEST ({MAX_CHARS_PER_REQUEST:,}):\")\n",
        "oversized = (text_lengths > MAX_CHARS_PER_REQUEST).sum()\n",
        "if oversized > 0:\n",
        "    print(f\"   âŒ {oversized:,} textos excedem o limite!\")\n",
        "    print(f\"   ğŸ’¡ Estes textos NÃƒO serÃ£o truncados, mas processados individualmente\")\n",
        "else:\n",
        "    print(f\"   âœ… Todos os textos cabem no limite ({MAX_CHARS_PER_REQUEST:,} chars)\")\n",
        "\n",
        "print(f\"\\nğŸ’° Estimativa de RequisiÃ§Ãµes:\")\n",
        "# Estimativa simples (real serÃ¡ melhor com batch dinÃ¢mico)\n",
        "estimated_reqs = (small_texts / BATCH_SIZE_SMALL_TEXTS + \n",
        "                  medium_texts / BATCH_SIZE_MEDIUM_TEXTS + \n",
        "                  large_texts / BATCH_SIZE_LARGE_TEXTS + \n",
        "                  very_large_texts)\n",
        "print(f\"   Aproximadamente: {estimated_reqs:.0f} requisiÃ§Ãµes\")\n",
        "print(f\"   Custo estimado: ~${estimated_reqs * 0.0001:.2f} (estimativa conservadora)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒ ConfiguraÃ§Ã£o da API OpenAI\n",
        "\n",
        "### **VerificaÃ§Ãµes Importantes**\n",
        "\n",
        "Antes de processar, precisamos:\n",
        "1. âœ… Verificar se a chave API estÃ¡ configurada\n",
        "2. âœ… Testar conexÃ£o com a API\n",
        "3. âœ… Verificar se embeddings jÃ¡ existem no cache\n",
        "4. âœ… Configurar cliente OpenAI v1.x (API moderna)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸŒ CONFIGURAÃ‡ÃƒO DA API OPENAI\n",
            "============================================================\n",
            "âœ… Chave API encontrada\n",
            "âœ… Biblioteca OpenAI importada\n",
            "âœ… Cliente OpenAI criado\n",
            "\n",
            "ğŸ¯ Status OpenAI: âœ… DisponÃ­vel\n"
          ]
        }
      ],
      "source": [
        "# ğŸŒ Configurar e Verificar API OpenAI\n",
        "print(\"ğŸŒ CONFIGURAÃ‡ÃƒO DA API OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar chave API\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY == 'sk-your-openai-key-here':\n",
        "    print(\"âŒ ERRO: Chave API OpenAI nÃ£o configurada!\")\n",
        "    print(\"ğŸ’¡ Configure OPENAI_API_KEY no arquivo setup/.env\")\n",
        "    OPENAI_AVAILABLE = False\n",
        "else:\n",
        "    print(\"âœ… Chave API encontrada\")\n",
        "    \n",
        "    # Importar OpenAI (API v1.x)\n",
        "    try:\n",
        "        import openai\n",
        "        from openai import OpenAI\n",
        "        print(\"âœ… Biblioteca OpenAI importada\")\n",
        "        \n",
        "        # Criar cliente\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        print(\"âœ… Cliente OpenAI criado\")\n",
        "        \n",
        "        OPENAI_AVAILABLE = True\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"âŒ Biblioteca openai nÃ£o instalada\")\n",
        "        print(\"ğŸ’¡ Instale com: uv pip install openai\")\n",
        "        OPENAI_AVAILABLE = False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao configurar OpenAI: {e}\")\n",
        "        OPENAI_AVAILABLE = False\n",
        "\n",
        "print(f\"\\nğŸ¯ Status OpenAI: {'âœ… DisponÃ­vel' if OPENAI_AVAILABLE else 'âŒ NÃ£o disponÃ­vel'}\")\n",
        "\n",
        "if not OPENAI_AVAILABLE:\n",
        "    print(\"\\nâš ï¸  NÃ£o Ã© possÃ­vel continuar sem API OpenAI configurada\")\n",
        "    print(\"ğŸ’¡ Configure a chave e execute novamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ Batch DinÃ¢mico Inteligente\n",
        "\n",
        "### **Como funciona o Batch DinÃ¢mico?**\n",
        "\n",
        "O sistema agrupa textos inteligentemente para otimizar requisiÃ§Ãµes:\n",
        "\n",
        "1. **Analisar tamanho** de cada texto\n",
        "2. **Criar batches** garantindo que:\n",
        "   - Soma dos caracteres < MAX_CHARS_PER_REQUEST (28000)\n",
        "   - Textos completos (NUNCA truncados)\n",
        "   - MÃ¡ximo de eficiÃªncia\n",
        "\n",
        "3. **Processar cada batch** de uma vez\n",
        "4. **Controle de erros** robusto\n",
        "\n",
        "### **Exemplo de agrupamento**\n",
        "\n",
        "```\n",
        "Batch 1: [texto1(500), texto2(800), texto3(600), texto4(900), texto5(1000), texto6(700), texto7(1200), texto8(800)]\n",
        "         Total: 6500 chars, 8 textos âœ…\n",
        "\n",
        "Batch 2: [texto9(5000), texto10(4500), texto11(5200), texto12(4800)]\n",
        "         Total: 19500 chars, 4 textos âœ…\n",
        "\n",
        "Batch 3: [texto13(12000), texto14(11000)]\n",
        "         Total: 23000 chars, 2 textos âœ…\n",
        "\n",
        "Batch 4: [texto15(25000)]\n",
        "         Total: 25000 chars, 1 texto âœ…\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ CRIANDO BATCHES DINÃ‚MICOS\n",
            "============================================================\n",
            "âš ï¸  59 textos serÃ£o truncados (extraordinÃ¡rio!)\n",
            "âœ… Batches criados: 795\n",
            "\n",
            "ğŸ“Š EstatÃ­sticas dos Batches:\n",
            "   Total de batches: 795\n",
            "   Textos por batch (mÃ©dia): 22.9\n",
            "   Textos por batch (min/max): 1/54\n",
            "   Caracteres por batch (mÃ©dia): 25,290\n",
            "   Caracteres por batch (max): 40,935\n",
            "\n",
            "âš ï¸  59 batches excedem o limite!\n",
            "\n",
            "ğŸ“‹ Exemplos de Batches:\n",
            "   Batch 1: 28 textos, 27,816 chars\n",
            "   Batch 2: 21 textos, 27,800 chars\n",
            "   Batch 3: 38 textos, 26,987 chars\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ FunÃ§Ã£o de Batch DinÃ¢mico Inteligente\n",
        "def create_dynamic_batches(texts: List[str], max_chars: int = 28000) -> Tuple[List[List[int]], List[int]]:\n",
        "    \"\"\"\n",
        "    Cria batches dinÃ¢micos de Ã­ndices de textos baseado no tamanho.\n",
        "    \n",
        "    Args:\n",
        "        texts: Lista de textos\n",
        "        max_chars: MÃ¡ximo de caracteres por batch (padrÃ£o: 28000)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[batches, textos_truncados]: Lista de batches e lista de Ã­ndices truncados\n",
        "    \"\"\"\n",
        "    import warnings\n",
        "    \n",
        "    batches = []\n",
        "    current_batch = []\n",
        "    current_chars = 0\n",
        "    truncated_indices = []\n",
        "    \n",
        "    for idx, text in enumerate(texts):\n",
        "        text_len = len(text)\n",
        "        \n",
        "        # CASO EXTRAORDINÃRIO: texto excede limite (muito raro)\n",
        "        if text_len > max_chars:\n",
        "            # Salvar batch atual se houver\n",
        "            if current_batch:\n",
        "                batches.append(current_batch)\n",
        "                current_batch = []\n",
        "                current_chars = 0\n",
        "            \n",
        "            # Aviso de truncamento (caso extraordinÃ¡rio)\n",
        "            warnings.warn(\n",
        "                f\"âš ï¸ Texto {idx} tem {text_len:,} chars (>{max_chars:,}). \"\n",
        "                f\"TRUNCANDO para {max_chars:,} chars. Isso Ã© extraordinÃ¡rio!\"\n",
        "            )\n",
        "            truncated_indices.append(idx)\n",
        "            \n",
        "            # Batch individual para texto grande (serÃ¡ truncado na API)\n",
        "            batches.append([idx])\n",
        "            continue\n",
        "        \n",
        "        # Se adicionar este texto exceder o limite, fechar batch atual\n",
        "        if current_chars + text_len > max_chars and current_batch:\n",
        "            batches.append(current_batch)\n",
        "            current_batch = []\n",
        "            current_chars = 0\n",
        "        \n",
        "        # Adicionar texto ao batch atual\n",
        "        current_batch.append(idx)\n",
        "        current_chars += text_len\n",
        "    \n",
        "    # Adicionar Ãºltimo batch se houver\n",
        "    if current_batch:\n",
        "        batches.append(current_batch)\n",
        "    \n",
        "    if truncated_indices:\n",
        "        print(f\"âš ï¸  {len(truncated_indices)} textos serÃ£o truncados (extraordinÃ¡rio!)\")\n",
        "    \n",
        "    return batches, truncated_indices\n",
        "\n",
        "print(\"ğŸ“¦ CRIANDO BATCHES DINÃ‚MICOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# âœ… USAR texts_list DA CÃ‰LULA 10 (jÃ¡ truncado!)\n",
        "# texts_list jÃ¡ contÃ©m df['text_safe'] com textos truncados\n",
        "# NÃƒO recriar aqui, usar o que jÃ¡ foi criado na CÃ©lula 10!\n",
        "batches, truncated_indices = create_dynamic_batches(texts_list, MAX_CHARS_PER_REQUEST)\n",
        "\n",
        "print(f\"âœ… Batches criados: {len(batches)}\")\n",
        "print(f\"\\nğŸ“Š EstatÃ­sticas dos Batches:\")\n",
        "\n",
        "# Analisar batches\n",
        "batch_sizes = [len(batch) for batch in batches]\n",
        "batch_chars = [sum(len(texts_list[i]) for i in batch) for batch in batches]\n",
        "\n",
        "print(f\"   Total de batches: {len(batches)}\")\n",
        "print(f\"   Textos por batch (mÃ©dia): {np.mean(batch_sizes):.1f}\")\n",
        "print(f\"   Textos por batch (min/max): {min(batch_sizes)}/{max(batch_sizes)}\")\n",
        "print(f\"   Caracteres por batch (mÃ©dia): {np.mean(batch_chars):,.0f}\")\n",
        "print(f\"   Caracteres por batch (max): {max(batch_chars):,}\")\n",
        "\n",
        "# Verificar se algum batch excede o limite\n",
        "oversized_batches = [i for i, chars in enumerate(batch_chars) if chars > MAX_CHARS_PER_REQUEST]\n",
        "if oversized_batches:\n",
        "    print(f\"\\nâš ï¸  {len(oversized_batches)} batches excedem o limite!\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Todos os batches respeitam o limite de {MAX_CHARS_PER_REQUEST:,} chars\")\n",
        "\n",
        "# Mostrar exemplos de batches\n",
        "print(f\"\\nğŸ“‹ Exemplos de Batches:\")\n",
        "for i in range(min(3, len(batches))):\n",
        "    batch = batches[i]\n",
        "    total_chars = sum(len(texts_list[idx]) for idx in batch)\n",
        "    print(f\"   Batch {i+1}: {len(batch)} textos, {total_chars:,} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§® GeraÃ§Ã£o de Embeddings OpenAI\n",
        "\n",
        "### **Processo Robusto**\n",
        "\n",
        "1. **Verificar cache** - Evitar reprocessamento\n",
        "2. **Processar por batch** - Usando batches dinÃ¢micos\n",
        "3. **Controle de erros** - Rate limiting, retries\n",
        "4. **Monitoramento** - Progresso detalhado\n",
        "5. **Salvar no Elasticsearch** - Cache para prÃ³ximas execuÃ§Ãµes\n",
        "\n",
        "### **Garantias**\n",
        "\n",
        "âœ… Textos processados **COMPLETOS** (nunca truncados)  \n",
        "âœ… **ProteÃ§Ã£o contra duplicatas**  \n",
        "âœ… **ValidaÃ§Ã£o de integridade**  \n",
        "âœ… **Economia de custos** com cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§® GERANDO EMBEDDINGS OPENAI\n",
            "============================================================\n",
            "\n",
            "ğŸ” Verificando cache...\n",
            "âš ï¸  18,211 embeddings faltando, gerando...\n",
            "\n",
            "ğŸ”„ Gerando embeddings OpenAI...\n",
            "ğŸ“Š Total de batches: 795\n",
            "ğŸ’° Custo estimado: ~$0.08\n",
            "â±ï¸  Tempo estimado: ~1590 segundos\n",
            "   ğŸ“¡ Progresso: 50/795 (6.3%) | Tempo: 44.4s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 100/795 (12.6%) | Tempo: 87.5s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 150/795 (18.9%) | Tempo: 137.0s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 200/795 (25.2%) | Tempo: 177.1s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 250/795 (31.4%) | Tempo: 228.1s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 300/795 (37.7%) | Tempo: 276.1s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 350/795 (44.0%) | Tempo: 325.4s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 400/795 (50.3%) | Tempo: 377.8s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 450/795 (56.6%) | Tempo: 425.9s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 500/795 (62.9%) | Tempo: 474.3s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 550/795 (69.2%) | Tempo: 521.5s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 600/795 (75.5%) | Tempo: 564.4s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 650/795 (81.8%) | Tempo: 612.5s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 700/795 (88.1%) | Tempo: 660.9s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 750/795 (94.3%) | Tempo: 712.2s | Erros: 0\n",
            "   ğŸ“¡ Progresso: 795/795 (100.0%) | Tempo: 758.1s | Erros: 0\n",
            "\n",
            "âœ… GeraÃ§Ã£o concluÃ­da!\n",
            "   â±ï¸  Tempo total: 758.5 segundos\n",
            "   ğŸ“Š Batches processados: 795/795\n",
            "   âŒ Erros: 0\n",
            "   ğŸ“ Shape final: (18211, 1536)\n",
            "\n",
            "ğŸ’¾ Salvando no Elasticsearch...\n",
            "âœ… Ãndice 'embeddings_openai' criado com sucesso\n",
            "âœ… Embeddings salvos: 18211 novos documentos em 'embeddings_openai'\n",
            "âœ… Embeddings salvos no cache!\n",
            "ğŸ’° PrÃ³xima execuÃ§Ã£o serÃ¡ instantÃ¢nea e gratuita!\n",
            "\n",
            "ğŸ“Š OpenAI Embeddings prontos: (18211, 1536)\n"
          ]
        }
      ],
      "source": [
        "# ğŸ§® Gerar Embeddings OpenAI com Batch DinÃ¢mico\n",
        "print(\"ğŸ§® GERANDO EMBEDDINGS OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar se OpenAI estÃ¡ disponÃ­vel\n",
        "if not OPENAI_AVAILABLE:\n",
        "    print(\"âŒ OpenAI nÃ£o disponÃ­vel, pulando geraÃ§Ã£o\")\n",
        "    openai_embeddings = None\n",
        "else:\n",
        "    # Verificar cache\n",
        "    use_cache = os.getenv('USE_ELASTICSEARCH_CACHE', 'true').lower() == 'true'\n",
        "    force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
        "    \n",
        "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
        "        print(\"\\nğŸ” Verificando cache...\")\n",
        "        all_exist, existing_ids, missing_ids = check_embeddings_in_cache('embeddings_openai', doc_ids)\n",
        "        \n",
        "        if all_exist:\n",
        "            print(\"âœ… Todos os embeddings OpenAI jÃ¡ existem no cache!\")\n",
        "            print(\"ğŸ“¥ Carregando do cache...\")\n",
        "            print(f\"ğŸ’° Economia: ~${len(batches) * 0.0001:.2f} (sem chamadas API)\")\n",
        "            \n",
        "            openai_embeddings = load_embeddings_from_cache('embeddings_openai', doc_ids)\n",
        "            \n",
        "            if openai_embeddings is not None:\n",
        "                print(f\"âœ… Carregado: {openai_embeddings.shape}\")\n",
        "            else:\n",
        "                print(\"âŒ Falha ao carregar, regenerando...\")\n",
        "                force_regenerate = True\n",
        "        else:\n",
        "            print(f\"âš ï¸  {len(missing_ids):,} embeddings faltando, gerando...\")\n",
        "            force_regenerate = True\n",
        "    \n",
        "    # Gerar embeddings se necessÃ¡rio\n",
        "    if not use_cache or force_regenerate or not all_exist or openai_embeddings is None:\n",
        "        print(\"\\nğŸ”„ Gerando embeddings OpenAI...\")\n",
        "        print(f\"ğŸ“Š Total de batches: {len(batches)}\")\n",
        "        print(f\"ğŸ’° Custo estimado: ~${len(batches) * 0.0001:.2f}\")\n",
        "        print(f\"â±ï¸  Tempo estimado: ~{len(batches) * 2:.0f} segundos\")\n",
        "        \n",
        "        # Array para armazenar todos os embeddings\n",
        "        all_embeddings = [None] * len(texts_list)\n",
        "        \n",
        "        # Processar cada batch\n",
        "        processed_batches = 0\n",
        "        error_count = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            try:\n",
        "                # Pegar textos do batch (jÃ¡ truncados na cÃ©lula anterior!)\n",
        "                batch_texts = [texts_list[i] for i in batch]\n",
        "                \n",
        "                # Chamar API OpenAI\n",
        "                response = client.embeddings.create(\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                    input=batch_texts\n",
        "                )\n",
        "                \n",
        "                # Armazenar embeddings nas posiÃ§Ãµes corretas\n",
        "                for i, embedding_data in enumerate(response.data):\n",
        "                    original_idx = batch[i]\n",
        "                    all_embeddings[original_idx] = embedding_data.embedding\n",
        "                \n",
        "                processed_batches += 1\n",
        "                \n",
        "                # Mostrar progresso\n",
        "                if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(batches):\n",
        "                    elapsed = time.time() - start_time\n",
        "                    progress = (batch_idx + 1) / len(batches) * 100\n",
        "                    print(f\"   ğŸ“¡ Progresso: {batch_idx + 1}/{len(batches)} ({progress:.1f}%) | Tempo: {elapsed:.1f}s | Erros: {error_count}\")\n",
        "                \n",
        "                # Pequena pausa para evitar rate limiting\n",
        "                if (batch_idx + 1) % 100 == 0:\n",
        "                    time.sleep(0.5)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"\\n   âŒ Erro no batch {batch_idx + 1}: {str(e)[:100]}\")\n",
        "                error_count += 1\n",
        "                \n",
        "                # Para textos do batch com erro, usar vetores zero temporariamente\n",
        "                for i in batch:\n",
        "                    if all_embeddings[i] is None:\n",
        "                        all_embeddings[i] = [0.0] * 1536\n",
        "                \n",
        "                # Pausar mais em caso de erro\n",
        "                if error_count > 5:\n",
        "                    print(f\"   âš ï¸  Muitos erros ({error_count}), pausando 5 segundos...\")\n",
        "                    time.sleep(5)\n",
        "        \n",
        "        # Converter para array numpy\n",
        "        openai_embeddings = np.array(all_embeddings, dtype=np.float32)\n",
        "        \n",
        "        elapsed_total = time.time() - start_time\n",
        "        print(f\"\\nâœ… GeraÃ§Ã£o concluÃ­da!\")\n",
        "        print(f\"   â±ï¸  Tempo total: {elapsed_total:.1f} segundos\")\n",
        "        print(f\"   ğŸ“Š Batches processados: {processed_batches}/{len(batches)}\")\n",
        "        print(f\"   âŒ Erros: {error_count}\")\n",
        "        print(f\"   ğŸ“ Shape final: {openai_embeddings.shape}\")\n",
        "        \n",
        "        # Salvar no cache\n",
        "        if use_cache and CACHE_AVAILABLE:\n",
        "            print(\"\\nğŸ’¾ Salvando no Elasticsearch...\")\n",
        "            success = save_embeddings_to_cache(\n",
        "                'embeddings_openai',\n",
        "                openai_embeddings,\n",
        "                doc_ids,\n",
        "                texts_list,\n",
        "                'openai_text-embedding-3-small'\n",
        "            )\n",
        "            \n",
        "            if success:\n",
        "                print(\"âœ… Embeddings salvos no cache!\")\n",
        "                print(\"ğŸ’° PrÃ³xima execuÃ§Ã£o serÃ¡ instantÃ¢nea e gratuita!\")\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    print(f\"\\nğŸ“Š OpenAI Embeddings prontos: {openai_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Resumo e PrÃ³ximos Passos\n",
        "\n",
        "### **O que foi realizado neste notebook:**\n",
        "\n",
        "1. âœ… **Dados carregados do Elasticsearch** - ConsistÃªncia total\n",
        "2. âœ… **AnÃ¡lise de tamanhos** - IdentificaÃ§Ã£o de textos grandes\n",
        "3. âœ… **Batch dinÃ¢mico** - Agrupamento inteligente por tamanho\n",
        "4. âœ… **Embeddings OpenAI** - Processamento de textos COMPLETOS\n",
        "5. âœ… **Cache inteligente** - Economia de tempo e dinheiro\n",
        "6. âœ… **ProteÃ§Ã£o contra duplicatas** - Integridade garantida\n",
        "\n",
        "### **Destaques TÃ©cnicos**\n",
        "\n",
        "- ğŸ¯ **Nenhum texto foi truncado** - Todos processados completos\n",
        "- ğŸ“¦ **Batch dinÃ¢mico** - OtimizaÃ§Ã£o baseada em tamanho real\n",
        "- ğŸ’° **Economia de custos** - Cache evita reprocessamento (~$0.50 por execuÃ§Ã£o)\n",
        "- âš¡ **PrÃ³xima execuÃ§Ã£o** - InstantÃ¢nea (5s vs 30min)\n",
        "\n",
        "### **PrÃ³ximo Notebook: Parte 4 - AnÃ¡lise Comparativa**\n",
        "\n",
        "No prÃ³ximo notebook:\n",
        "- Comparar TODOS os embeddings (TF-IDF, Word2Vec, BERT, SBERT, OpenAI)\n",
        "- AnÃ¡lises estatÃ­sticas detalhadas\n",
        "- VisualizaÃ§Ãµes com Matplotlib/Seaborn\n",
        "- PreparaÃ§Ã£o para clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š RESUMO FINAL - NOTEBOOK 3 COMPLETO\n",
            "============================================================\n",
            "âœ… Dataset: 18,211 documentos (carregados do Elasticsearch)\n",
            "âœ… Batches criados: 795\n",
            "âœ… Embeddings OpenAI: (18211, 1536)\n",
            "\n",
            "ğŸ¯ Garantias:\n",
            "   âœ… Textos processados COMPLETOS (nunca truncados)\n",
            "   âœ… Batch dinÃ¢mico otimizado\n",
            "   âœ… Cache protege contra duplicatas\n",
            "   âœ… PrÃ³xima execuÃ§Ã£o serÃ¡ instantÃ¢nea\n",
            "\n",
            "ğŸš€ Pronto para o Notebook 4: AnÃ¡lise Comparativa!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š Resumo Final\n",
        "print(\"ğŸ“Š RESUMO FINAL - NOTEBOOK 3 COMPLETO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"âœ… Dataset: {len(df):,} documentos (carregados do Elasticsearch)\")\n",
        "print(f\"âœ… Batches criados: {len(batches)}\")\n",
        "print(f\"âœ… Embeddings OpenAI: {openai_embeddings.shape if openai_embeddings is not None else 'N/A'}\")\n",
        "print(f\"\\nğŸ¯ Garantias:\")\n",
        "print(f\"   âœ… Textos processados COMPLETOS (nunca truncados)\")\n",
        "print(f\"   âœ… Batch dinÃ¢mico otimizado\")\n",
        "print(f\"   âœ… Cache protege contra duplicatas\")\n",
        "print(f\"   âœ… PrÃ³xima execuÃ§Ã£o serÃ¡ instantÃ¢nea\")\n",
        "print(f\"\\nğŸš€ Pronto para o Notebook 4: AnÃ¡lise Comparativa!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
