#!/usr/bin/env python3
"""
Exemplo de Uso - Embeddings AvanÃ§ados e Clustering SemÃ¢ntico
============================================================

Este script demonstra como usar as funcionalidades principais do projeto
de forma programÃ¡tica, sem necessidade do Jupyter Notebook.

Autor: Sistema de Aulas NLP
Data: 2025
"""

import os
import sys
import warnings
import numpy as np
import pandas as pd
from pathlib import Path

# Adicionar o diretÃ³rio raiz ao path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Configurar warnings
warnings.filterwarnings('ignore')

def main():
    """FunÃ§Ã£o principal de exemplo"""
    print("ğŸš€ EXEMPLO DE USO - EMBEDDINGS AVANÃ‡ADOS E CLUSTERING")
    print("=" * 60)
    
    # Verificar se o ambiente estÃ¡ configurado
    if not check_environment():
        print("âŒ Ambiente nÃ£o configurado corretamente")
        print("ğŸ’¡ Execute: make install && make test")
        return
    
    # Exemplo 1: AnÃ¡lise bÃ¡sica com TF-IDF
    print("\nğŸ“Š EXEMPLO 1: AnÃ¡lise BÃ¡sica com TF-IDF")
    print("-" * 40)
    example_tfidf_analysis()
    
    # Exemplo 2: Uso do sistema de cache
    print("\nğŸ—„ï¸ EXEMPLO 2: Sistema de Cache Elasticsearch")
    print("-" * 40)
    example_cache_usage()
    
    # Exemplo 3: Clustering bÃ¡sico
    print("\nğŸ” EXEMPLO 3: Clustering BÃ¡sico")
    print("-" * 40)
    example_clustering()
    
    print("\nâœ… Exemplos concluÃ­dos com sucesso!")
    print("ğŸ’¡ Para anÃ¡lise completa, use o Jupyter Notebook: SeÃ§Ã£o5.1_Embeddings.ipynb")

def check_environment():
    """Verifica se o ambiente estÃ¡ configurado"""
    try:
        import numpy as np
        import pandas as pd
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        print("âœ… DependÃªncias bÃ¡sicas disponÃ­veis")
        return True
    except ImportError as e:
        print(f"âŒ DependÃªncia faltando: {e}")
        return False

def example_tfidf_analysis():
    """Exemplo de anÃ¡lise com TF-IDF"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.datasets import fetch_20newsgroups
    
    # Carregar dados de exemplo
    print("ğŸ“¥ Carregando dados de exemplo...")
    newsgroups = fetch_20newsgroups(
        subset='train', 
        categories=['sci.med', 'sci.crypt'],
        remove=('headers', 'footers', 'quotes')
    )
    
    # Limitar a 100 documentos para exemplo rÃ¡pido
    texts = newsgroups.data[:100]
    categories = newsgroups.target[:100]
    
    print(f"ğŸ“Š Documentos carregados: {len(texts)}")
    print(f"ğŸ“Š Categorias: {len(set(categories))}")
    
    # Gerar embeddings TF-IDF
    print("ğŸ”„ Gerando embeddings TF-IDF...")
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words='english',
        min_df=2,
        max_df=0.8
    )
    
    embeddings = vectorizer.fit_transform(texts)
    print(f"ğŸ“ DimensÃµes dos embeddings: {embeddings.shape}")
    print(f"ğŸ“Š Densidade: {(embeddings != 0).mean():.3f}")
    
    # AnÃ¡lise bÃ¡sica
    feature_names = vectorizer.get_feature_names_out()
    mean_tfidf = embeddings.mean(axis=0).A1
    top_features = np.argsort(mean_tfidf)[-10:][::-1]
    
    print("ğŸ” Top 10 features mais importantes:")
    for i, idx in enumerate(top_features):
        print(f"   {i+1:2d}. {feature_names[idx]:<15}: {mean_tfidf[idx]:.4f}")

def example_cache_usage():
    """Exemplo de uso do sistema de cache"""
    try:
        from elasticsearch_manager import (
            init_elasticsearch_cache, 
            get_cache_status,
            check_embeddings_in_cache
        )
        
        # Inicializar cache
        print("ğŸ”Œ Conectando ao Elasticsearch...")
        cache_connected = init_elasticsearch_cache()
        
        if cache_connected:
            print("âœ… Elasticsearch conectado com sucesso")
            
            # Verificar status
            status = get_cache_status()
            print(f"ğŸ“Š Ãndices encontrados: {status.get('indices_count', 0)}")
            print(f"ğŸ“‹ Total de documentos: {status.get('total_docs', 0):,}")
            print(f"ğŸ’¾ EspaÃ§o usado: {status.get('total_size_mb', 0):.1f} MB")
            
            # Exemplo de verificaÃ§Ã£o de cache
            doc_ids = ['doc_0001', 'doc_0002', 'doc_0003']
            exists, existing, missing = check_embeddings_in_cache('embeddings_tfidf', doc_ids)
            
            print(f"ğŸ” VerificaÃ§Ã£o de cache:")
            print(f"   Existem: {exists}")
            print(f"   Encontrados: {len(existing)}")
            print(f"   Faltando: {len(missing)}")
            
        else:
            print("âš ï¸  Elasticsearch nÃ£o disponÃ­vel")
            print("ğŸ’¡ Execute: docker-compose up -d")
            
    except ImportError:
        print("âš ï¸  MÃ³dulo elasticsearch_manager nÃ£o encontrado")
        print("ğŸ’¡ Certifique-se de que estÃ¡ no diretÃ³rio raiz do projeto")

def example_clustering():
    """Exemplo de clustering bÃ¡sico"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    from sklearn.datasets import fetch_20newsgroups
    
    # Carregar dados
    print("ğŸ“¥ Carregando dados para clustering...")
    newsgroups = fetch_20newsgroups(
        subset='train',
        categories=['sci.med', 'sci.crypt', 'rec.sport.baseball'],
        remove=('headers', 'footers', 'quotes')
    )
    
    # Limitar a 200 documentos
    texts = newsgroups.data[:200]
    true_labels = newsgroups.target[:200]
    
    # Gerar embeddings
    print("ğŸ”„ Gerando embeddings...")
    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')
    embeddings = vectorizer.fit_transform(texts)
    
    # Clustering
    print("ğŸ” Aplicando clustering K-Means...")
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(embeddings.toarray())
    
    # AvaliaÃ§Ã£o
    silhouette = silhouette_score(embeddings, cluster_labels)
    print(f"ğŸ“Š Silhouette Score: {silhouette:.3f}")
    print(f"ğŸ“Š Clusters encontrados: {len(set(cluster_labels))}")
    
    # AnÃ¡lise dos clusters
    print("ğŸ“ˆ AnÃ¡lise dos clusters:")
    for i in range(3):
        cluster_mask = cluster_labels == i
        cluster_size = cluster_mask.sum()
        print(f"   Cluster {i}: {cluster_size} documentos")
        
        # Mostrar algumas palavras caracterÃ­sticas
        cluster_embeddings = embeddings[cluster_mask]
        mean_embeddings = cluster_embeddings.mean(axis=0).A1
        top_words_idx = np.argsort(mean_embeddings)[-5:][::-1]
        feature_names = vectorizer.get_feature_names_out()
        
        top_words = [feature_names[idx] for idx in top_words_idx]
        print(f"      Palavras caracterÃ­sticas: {', '.join(top_words)}")

if __name__ == "__main__":
    main()
