{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîπ Se√ß√£o 5.1 ‚Äì Embeddings Avan√ßados e Clustering\n",
        "**Objetivo:** Explorar a evolu√ß√£o dos embeddings desde os m√©todos cl√°ssicos at√© os modernos, com foco em **clustering sem√¢ntico** e aplica√ß√µes pr√°ticas.\n",
        "\n",
        "Esta aula demonstra como diferentes tipos de embeddings capturam similaridade sem√¢ntica e como isso se traduz em **agrupamentos de alta qualidade** para an√°lise de dados textuais.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Conceitos Fundamentais: A Evolu√ß√£o dos Embeddings\n",
        "\n",
        "### **O que s√£o Embeddings?**\n",
        "**Embeddings** s√£o representa√ß√µes vetoriais densas que mapeiam textos para um espa√ßo onde **proximidade geom√©trica ‚âà similaridade sem√¢ntica**. Diferente de representa√ß√µes esparsas (BoW/TF-IDF), embeddings capturam **contexto** e **rela√ß√µes sem√¢nticas**.\n",
        "\n",
        "### **Evolu√ß√£o Hist√≥rica**\n",
        "1. **TF-IDF (1970s)**: M√©todo cl√°ssico baseado em frequ√™ncia de termos\n",
        "2. **Word2Vec (2013)**: Primeira revolu√ß√£o - palavras similares ficam pr√≥ximas\n",
        "3. **BERT (2018)**: Contextualiza√ß√£o bidirecional - mesma palavra, contextos diferentes\n",
        "4. **Sentence-BERT (2019)**: Otimizado para similaridade de senten√ßas\n",
        "5. **OpenAI Embeddings (2020+)**: Modelos de √∫ltima gera√ß√£o, otimizados para tarefas espec√≠ficas\n",
        "\n",
        "### **Por que Embeddings s√£o Ideais para Clustering?**\n",
        "- **Proximidade sem√¢ntica** = proximidade geom√©trica no espa√ßo vetorial\n",
        "- **Densidade**: Informa√ß√£o rica em poucas dimens√µes\n",
        "- **Contextualiza√ß√£o**: Captura nuances sem√¢nticas\n",
        "- **Transfer Learning**: Aproveita conhecimento pr√©-treinado\n",
        "\n",
        "### **Aplica√ß√µes Pr√°ticas**\n",
        "- **Clustering sem√¢ntico**: Agrupar documentos por t√≥pico\n",
        "- **Busca sem√¢ntica**: Encontrar documentos similares\n",
        "- **Detec√ß√£o de anomalias**: Identificar textos at√≠picos\n",
        "- **Sistemas de recomenda√ß√£o**: Sugerir conte√∫do similar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Depend√™ncias e Configura√ß√£o\n",
        "\n",
        "Execute a c√©lula abaixo para instalar todas as depend√™ncias necess√°rias:\n",
        "\n",
        "```bash\n",
        "!uv pip install -q sentence-transformers umap-learn scikit-learn pandas matplotlib plotly gensim openai streamlit hdbscan wordcloud elasticsearch docker\n",
        "```\n",
        "\n",
        "### **Principais Bibliotecas**\n",
        "- `sentence-transformers`: Modelos BERT/SBERT modernos\n",
        "- `gensim`: Word2Vec, GloVe cl√°ssicos\n",
        "- `openai`: Embeddings da OpenAI (text-embedding-3-small/large)\n",
        "- `scikit-learn`: Algoritmos de clustering e m√©tricas\n",
        "- `umap-learn`: Redu√ß√£o dimensional para visualiza√ß√£o\n",
        "- `plotly`: Visualiza√ß√µes interativas\n",
        "- `elasticsearch`: Armazenamento e busca de embeddings\n",
        "- `hdbscan`: Clustering hier√°rquico moderno\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  C√âLULA ORIGINAL COMENTADA\n",
            "üí° Execute as c√©lulas 4-8 abaixo para carregar as bibliotecas de forma segura\n",
            "üîß Isso evita travamentos durante o carregamento de bibliotecas pesadas\n"
          ]
        }
      ],
      "source": [
        "# ‚ö†Ô∏è C√âLULA ORIGINAL COMENTADA - PODE TRAVAR\n",
        "# Use as c√©lulas abaixo para carregar as bibliotecas de forma segura\n",
        "\n",
        "\"\"\"\n",
        "# ‚úÖ Imports e Configura√ß√£o - Vers√£o Otimizada\n",
        "# Esta c√©lula foi comentada porque pode travar durante o carregamento\n",
        "# Use as c√©lulas 4-8 abaixo para carregar as bibliotecas de forma segura\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# ... resto do c√≥digo comentado ...\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚ö†Ô∏è  C√âLULA ORIGINAL COMENTADA\")\n",
        "print(\"üí° Execute as c√©lulas 4-8 abaixo para carregar as bibliotecas de forma segura\")\n",
        "print(\"üîß Isso evita travamentos durante o carregamento de bibliotecas pesadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ python-dotenv dispon√≠vel\n",
            "‚úÖ Arquivo .env carregado: /Users/ivanvarella/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGEÃÇNCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUCÃßAÃÉO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/setup/.env\n",
            "‚úÖ Chave da OpenAI configurada\n",
            "üîß Configura√ß√£o carregada com sucesso!\n",
            "üìä Configura√ß√µes carregadas:\n",
            "   MAX_CHARS_PER_REQUEST: 30000\n",
            "   BATCH_SIZE_SMALL_TEXTS: 8\n",
            "   BATCH_SIZE_MEDIUM_TEXTS: 4\n",
            "   BATCH_SIZE_LARGE_TEXTS: 2\n",
            "   DATASET_SIZE: 10000\n",
            "   TEXT_MIN_LENGTH: 50\n",
            "   MAX_CLUSTERS: 20\n",
            "   CLUSTERING_RANDOM_STATE: 42\n"
          ]
        }
      ],
      "source": [
        "# üîß Configura√ß√£o de Vari√°veis de Ambiente\n",
        "# Esta c√©lula carrega as configura√ß√µes do arquivo .env\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Tentar carregar python-dotenv se dispon√≠vel\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    print(\"‚úÖ python-dotenv dispon√≠vel\")\n",
        "    \n",
        "    # Procurar arquivo .env em diferentes locais (em ordem de prioridade)\n",
        "    env_paths = [\n",
        "        Path.cwd() / 'setup' / '.env',  # Primeiro: pasta setup/\n",
        "        Path.cwd() / '.env',            # Segundo: diret√≥rio raiz\n",
        "        Path.cwd() / 'setup' / 'config_example.env'  # Terceiro: arquivo de exemplo\n",
        "    ]\n",
        "    \n",
        "    env_loaded = False\n",
        "    for env_path in env_paths:\n",
        "        if env_path.exists():\n",
        "            load_dotenv(env_path)\n",
        "            print(f\"‚úÖ Arquivo .env carregado: {env_path}\")\n",
        "            env_loaded = True\n",
        "            break\n",
        "    \n",
        "    if not env_loaded:\n",
        "        print(\"‚ö†Ô∏è  Nenhum arquivo .env encontrado, usando vari√°veis do sistema\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  python-dotenv n√£o instalado, usando vari√°veis do sistema\")\n",
        "    print(\"üí° Para instalar: pip install python-dotenv\")\n",
        "\n",
        "# Carregar configura√ß√µes do .env\n",
        "MAX_CHARS_PER_REQUEST = int(os.getenv('MAX_CHARS_PER_REQUEST', 30000))\n",
        "BATCH_SIZE_SMALL_TEXTS = int(os.getenv('BATCH_SIZE_SMALL_TEXTS', 8))\n",
        "BATCH_SIZE_MEDIUM_TEXTS = int(os.getenv('BATCH_SIZE_MEDIUM_TEXTS', 4))\n",
        "BATCH_SIZE_LARGE_TEXTS = int(os.getenv('BATCH_SIZE_LARGE_TEXTS', 2))\n",
        "DATASET_SIZE = int(os.getenv('DATASET_SIZE', 10000))\n",
        "TEXT_MIN_LENGTH = int(os.getenv('TEXT_MIN_LENGTH', 50))\n",
        "MAX_CLUSTERS = int(os.getenv('MAX_CLUSTERS', 20))\n",
        "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', 42))\n",
        "PLOT_WIDTH = int(os.getenv('PLOT_WIDTH', 800))\n",
        "PLOT_HEIGHT = int(os.getenv('PLOT_HEIGHT', 600))\n",
        "LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')\n",
        "SAVE_MODELS = os.getenv('SAVE_MODELS', 'true').lower() == 'true'\n",
        "SAVE_RESULTS = os.getenv('SAVE_RESULTS', 'true').lower() == 'true'\n",
        "ELASTICSEARCH_HOST = os.getenv('ELASTICSEARCH_HOST', 'localhost')\n",
        "ELASTICSEARCH_PORT = int(os.getenv('ELASTICSEARCH_PORT', 9200))\n",
        "\n",
        "# Verificar se a chave da OpenAI est√° configurada\n",
        "openai_key = os.getenv('OPENAI_API_KEY')\n",
        "if openai_key and openai_key != 'sk-your-openai-key-here':\n",
        "    print(\"‚úÖ Chave da OpenAI configurada\")\n",
        "    OPENAI_AVAILABLE = True\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Chave da OpenAI n√£o configurada\")\n",
        "    print(\"üí° Configure OPENAI_API_KEY no arquivo setup/.env para usar embeddings da OpenAI\")\n",
        "    OPENAI_AVAILABLE = False\n",
        "\n",
        "print(f\"üîß Configura√ß√£o carregada com sucesso!\")\n",
        "print(f\"üìä Configura√ß√µes carregadas:\")\n",
        "print(f\"   MAX_CHARS_PER_REQUEST: {MAX_CHARS_PER_REQUEST}\")\n",
        "print(f\"   BATCH_SIZE_SMALL_TEXTS: {BATCH_SIZE_SMALL_TEXTS}\")\n",
        "print(f\"   BATCH_SIZE_MEDIUM_TEXTS: {BATCH_SIZE_MEDIUM_TEXTS}\")\n",
        "print(f\"   BATCH_SIZE_LARGE_TEXTS: {BATCH_SIZE_LARGE_TEXTS}\")\n",
        "print(f\"   DATASET_SIZE: {DATASET_SIZE}\")\n",
        "print(f\"   TEXT_MIN_LENGTH: {TEXT_MIN_LENGTH}\")\n",
        "print(f\"   MAX_CLUSTERS: {MAX_CLUSTERS}\")\n",
        "print(f\"   CLUSTERING_RANDOM_STATE: {CLUSTERING_RANDOM_STATE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ TESTE R√ÅPIDO DE IMPORTS\n",
            "==============================\n",
            "‚úÖ NumPy OK\n",
            "‚úÖ Pandas OK\n",
            "‚úÖ Matplotlib OK\n",
            "‚úÖ Seaborn OK\n",
            "‚úÖ Scikit-learn OK\n",
            "\n",
            "üéØ Teste b√°sico conclu√≠do!\n"
          ]
        }
      ],
      "source": [
        "# üß™ Teste R√°pido de Imports\n",
        "print(\"üß™ TESTE R√ÅPIDO DE IMPORTS\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Teste b√°sico\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(\"‚úÖ NumPy OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå NumPy erro: {e}\")\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    print(\"‚úÖ Pandas OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Pandas erro: {e}\")\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    print(\"‚úÖ Matplotlib OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Matplotlib erro: {e}\")\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    print(\"‚úÖ Seaborn OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Seaborn erro: {e}\")\n",
        "\n",
        "try:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    print(\"‚úÖ Scikit-learn OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Scikit-learn erro: {e}\")\n",
        "\n",
        "print(\"\\nüéØ Teste b√°sico conclu√≠do!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Testando imports que podem travar...\n",
            "üîÑ Carregando Sentence Transformers...\n",
            "‚úÖ Sentence Transformers carregado com sucesso\n",
            "üîÑ Carregando Gensim...\n",
            "‚úÖ Gensim carregado com sucesso\n",
            "üîÑ Carregando Plotly...\n",
            "‚úÖ Plotly carregado com sucesso\n",
            "\n",
            "üìä RESUMO DOS TESTES:\n",
            "   Sentence Transformers: ‚úÖ\n",
            "   Gensim: ‚úÖ\n",
            "   Plotly: ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "# üîÑ Imports Problem√°ticos - Teste Individual\n",
        "print(\"üîÑ Testando imports que podem travar...\")\n",
        "\n",
        "# Teste Sentence Transformers (pode ser lento)\n",
        "try:\n",
        "    print(\"üîÑ Carregando Sentence Transformers...\")\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    print(\"‚úÖ Sentence Transformers carregado com sucesso\")\n",
        "    SENTENCE_TRANSFORMERS_OK = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Sentence Transformers erro: {e}\")\n",
        "    SENTENCE_TRANSFORMERS_OK = False\n",
        "\n",
        "# Teste Gensim (pode ser lento)\n",
        "try:\n",
        "    print(\"üîÑ Carregando Gensim...\")\n",
        "    import gensim\n",
        "    from gensim.models import Word2Vec\n",
        "    print(\"‚úÖ Gensim carregado com sucesso\")\n",
        "    GENSIM_OK = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Gensim erro: {e}\")\n",
        "    GENSIM_OK = False\n",
        "\n",
        "# Teste Plotly\n",
        "try:\n",
        "    print(\"üîÑ Carregando Plotly...\")\n",
        "    import plotly.express as px\n",
        "    print(\"‚úÖ Plotly carregado com sucesso\")\n",
        "    PLOTLY_OK = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Plotly erro: {e}\")\n",
        "    PLOTLY_OK = False\n",
        "\n",
        "print(f\"\\nüìä RESUMO DOS TESTES:\")\n",
        "print(f\"   Sentence Transformers: {'‚úÖ' if SENTENCE_TRANSFORMERS_OK else '‚ùå'}\")\n",
        "print(f\"   Gensim: {'‚úÖ' if GENSIM_OK else '‚ùå'}\")\n",
        "print(f\"   Plotly: {'‚úÖ' if PLOTLY_OK else '‚ùå'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ CARREGANDO IMPORTS ESSENCIAIS\n",
            "========================================\n",
            "‚úÖ Imports b√°sicos carregados\n",
            "‚úÖ Scikit-learn carregado\n",
            "‚úÖ Configura√ß√µes aplicadas\n",
            "\n",
            "üéâ IMPORTS ESSENCIAIS CONCLU√çDOS!\n",
            "üí° Execute as pr√≥ximas c√©lulas para carregar bibliotecas opcionais\n"
          ]
        }
      ],
      "source": [
        "# üöÄ Imports Essenciais - Vers√£o Simplificada\n",
        "print(\"üöÄ CARREGANDO IMPORTS ESSENCIAIS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Imports b√°sicos\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "print(\"‚úÖ Imports b√°sicos carregados\")\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score, adjusted_rand_score, normalized_mutual_info_score,\n",
        "    homogeneity_score, completeness_score, v_measure_score,\n",
        "    calinski_harabasz_score, davies_bouldin_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"‚úÖ Scikit-learn carregado\")\n",
        "\n",
        "# Configura√ß√µes\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# Configura√ß√£o do matplotlib\n",
        "try:\n",
        "    plt.style.use('seaborn')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "    except:\n",
        "        plt.style.use('default')\n",
        "\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Configura√ß√µes aplicadas\")\n",
        "\n",
        "# Flags de disponibilidade (ser√£o definidas nas pr√≥ximas c√©lulas)\n",
        "UMAP_AVAILABLE = False\n",
        "HDBSCAN_AVAILABLE = False\n",
        "OPENAI_AVAILABLE = False\n",
        "ELASTICSEARCH_AVAILABLE = False\n",
        "\n",
        "print(\"\\nüéâ IMPORTS ESSENCIAIS CONCLU√çDOS!\")\n",
        "print(\"üí° Execute as pr√≥ximas c√©lulas para carregar bibliotecas opcionais\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö CARREGANDO BIBLIOTECAS OPCIONAIS\n",
            "========================================\n",
            "‚úÖ UMAP carregado\n",
            "‚úÖ HDBSCAN carregado\n",
            "‚úÖ OpenAI carregado\n",
            "‚úÖ Elasticsearch carregado\n",
            "\n",
            "üìä STATUS DAS BIBLIOTECAS:\n",
            "   UMAP: ‚úÖ\n",
            "   HDBSCAN: ‚úÖ\n",
            "   OpenAI: ‚úÖ\n",
            "   Elasticsearch: ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "# üìö Carregar Bibliotecas Opcionais\n",
        "print(\"üìö CARREGANDO BIBLIOTECAS OPCIONAIS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# UMAP\n",
        "try:\n",
        "    import umap\n",
        "    UMAP_AVAILABLE = True\n",
        "    print(\"‚úÖ UMAP carregado\")\n",
        "except ImportError:\n",
        "    UMAP_AVAILABLE = False\n",
        "    print('‚ö†Ô∏è UMAP n√£o instalado')\n",
        "\n",
        "# HDBSCAN\n",
        "try:\n",
        "    import hdbscan\n",
        "    HDBSCAN_AVAILABLE = True\n",
        "    print(\"‚úÖ HDBSCAN carregado\")\n",
        "except ImportError:\n",
        "    HDBSCAN_AVAILABLE = False\n",
        "    print('‚ö†Ô∏è HDBSCAN n√£o instalado')\n",
        "\n",
        "# OpenAI\n",
        "try:\n",
        "    import openai\n",
        "    OPENAI_AVAILABLE = True\n",
        "    print(\"‚úÖ OpenAI carregado\")\n",
        "except ImportError:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print('‚ö†Ô∏è OpenAI n√£o instalado')\n",
        "\n",
        "# Elasticsearch\n",
        "try:\n",
        "    from elasticsearch import Elasticsearch\n",
        "    ELASTICSEARCH_AVAILABLE = True\n",
        "    print(\"‚úÖ Elasticsearch carregado\")\n",
        "except ImportError:\n",
        "    ELASTICSEARCH_AVAILABLE = False\n",
        "    print('‚ö†Ô∏è Elasticsearch n√£o instalado')\n",
        "\n",
        "print(f\"\\nüìä STATUS DAS BIBLIOTECAS:\")\n",
        "print(f\"   UMAP: {'‚úÖ' if UMAP_AVAILABLE else '‚ùå'}\")\n",
        "print(f\"   HDBSCAN: {'‚úÖ' if HDBSCAN_AVAILABLE else '‚ùå'}\")\n",
        "print(f\"   OpenAI: {'‚úÖ' if OPENAI_AVAILABLE else '‚ùå'}\")\n",
        "print(f\"   Elasticsearch: {'‚úÖ' if ELASTICSEARCH_AVAILABLE else '‚ùå'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÑÔ∏è Gerenciamento de Cache Elasticsearch\n",
        "\n",
        "### **Sistema Inteligente de Cache**\n",
        "Este notebook implementa um sistema avan√ßado de cache usando Elasticsearch que:\n",
        "\n",
        "- **Evita reprocessamento**: Detecta embeddings j√° gerados e os reutiliza\n",
        "- **Valida integridade**: Verifica se os dados salvos est√£o corretos via hash MD5\n",
        "- **Economiza tempo**: TF-IDF (30s ‚Üí 5s), Word2Vec (60s ‚Üí 5s), BERT (120s ‚Üí 5s), **OpenAI (30min ‚Üí 5s!)**\n",
        "- **Economiza dinheiro**: Evita chamadas desnecess√°rias √† API da OpenAI\n",
        "- **Rastreabilidade**: Cada embedding est√° vinculado ao documento original\n",
        "\n",
        "### **Estrutura dos √çndices**\n",
        "```\n",
        "üì¶ ELASTICSEARCH CACHE\n",
        "‚îú‚îÄ‚îÄ üìÑ documents_dataset     (Dataset original com IDs √∫nicos)\n",
        "‚îú‚îÄ‚îÄ üßÆ embeddings_tfidf      (5000 dimens√µes)\n",
        "‚îú‚îÄ‚îÄ üßÆ embeddings_word2vec   (100 dimens√µes)  \n",
        "‚îú‚îÄ‚îÄ üßÆ embeddings_bert       (768 dimens√µes)\n",
        "‚îú‚îÄ‚îÄ üßÆ embeddings_sbert      (384 dimens√µes)\n",
        "‚îî‚îÄ‚îÄ üßÆ embeddings_openai     (1536 dimens√µes)\n",
        "```\n",
        "\n",
        "### **Fluxo Inteligente**\n",
        "1. **Verifica√ß√£o**: Checa se embeddings j√° existem\n",
        "2. **Valida√ß√£o**: Confere integridade via hash dos textos\n",
        "3. **Gera√ß√£o seletiva**: Gera apenas embeddings faltantes ou inv√°lidos\n",
        "4. **Salvamento**: Armazena com metadata completa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üóÑÔ∏è INICIALIZANDO SISTEMA DE CACHE ELASTICSEARCH\n",
            "============================================================\n",
            "‚úÖ M√≥dulo de cache Elasticsearch carregado\n",
            "\n",
            "üîå Conectando ao Elasticsearch...\n",
            "‚úÖ Conectado ao Elasticsearch (localhost:9200)\n",
            "‚úÖ Cache Elasticsearch inicializado com sucesso!\n",
            "\n",
            "üìä STATUS DO CACHE:\n",
            "----------------------------------------\n",
            "üîå Conex√£o: ‚úÖ localhost:9200\n",
            "üìä √çndices encontrados: 6\n",
            "üìã Total de documentos: 54,510\n",
            "üíæ Espa√ßo usado: 650.1 MB\n",
            "\n",
            "üì¶ DETALHES DOS √çNDICES:\n",
            "   ‚úÖ documents_dataset   :  9,085 docs (  12.7 MB)\n",
            "   ‚úÖ embeddings_tfidf    :  9,085 docs ( 154.1 MB)\n",
            "   ‚úÖ embeddings_word2vec :  9,085 docs (  18.7 MB)\n",
            "   ‚úÖ embeddings_bert     :  9,085 docs ( 135.4 MB)\n",
            "   ‚úÖ embeddings_sbert    :  9,085 docs (  69.0 MB)\n",
            "   ‚úÖ embeddings_openai   :  9,085 docs ( 260.2 MB)\n",
            "   ‚è≥ embeddings_test     : Ser√° criado durante a gera√ß√£o\n",
            "   ‚è≥ embeddings_duplicate_test: Ser√° criado durante a gera√ß√£o\n",
            "   ‚è≥ embeddings_integrity_test: Ser√° criado durante a gera√ß√£o\n",
            "\n",
            "üéõÔ∏è CONFIGURA√á√ïES DO SISTEMA:\n",
            "   Usar cache: ‚úÖ\n",
            "   For√ßar regenera√ß√£o: ‚ùå\n",
            "\n",
            "üí° EXPLICA√á√ÉO DAS CONFIGURA√á√ïES:\n",
            "   üîÑ Usar cache: Ativado\n",
            "      ‚Üí O sistema verificar√° se os embeddings j√° existem\n",
            "      ‚Üí Se existirem, carregar√° do cache (muito mais r√°pido)\n",
            "      ‚Üí Se n√£o existirem, gerar√° novos e salvar√° no cache\n",
            "   üîÑ For√ßar regenera√ß√£o: Desativado\n",
            "      ‚Üí Usa cache quando dispon√≠vel (recomendado)\n",
            "      ‚Üí Gera apenas embeddings que n√£o existem\n",
            "\n",
            "üîÑ FLUXO DO SISTEMA DE CACHE:\n",
            "   1Ô∏è‚É£ Verifica√ß√£o: Checa se embeddings j√° existem no Elasticsearch\n",
            "   2Ô∏è‚É£ Valida√ß√£o: Confere integridade via hash MD5 dos textos\n",
            "   3Ô∏è‚É£ Gera√ß√£o seletiva: Gera apenas embeddings faltantes ou inv√°lidos\n",
            "   4Ô∏è‚É£ Salvamento: Armazena novos embeddings com metadata completa\n",
            "   5Ô∏è‚É£ Carregamento: Recupera embeddings existentes (muito mais r√°pido)\n",
            "\n",
            "‚è±Ô∏è  ECONOMIA DE TEMPO ESTIMADA:\n",
            "   üìä TF-IDF: 30 segundos ‚Üí 5 segundos (6x mais r√°pido)\n",
            "   üìä Word2Vec: 60 segundos ‚Üí 5 segundos (12x mais r√°pido)\n",
            "   üìä BERT: 120 segundos ‚Üí 5 segundos (24x mais r√°pido)\n",
            "   üìä Sentence-BERT: 90 segundos ‚Üí 5 segundos (18x mais r√°pido)\n",
            "   üìä OpenAI: 30 minutos ‚Üí 5 segundos (360x mais r√°pido!)\n",
            "\n",
            "üí∞ ECONOMIA DE CUSTO (OpenAI):\n",
            "   üíµ Primeira execu√ß√£o: ~$0.50 (gera e salva embeddings)\n",
            "   üíµ Execu√ß√µes seguintes: $0.00 (usa cache)\n",
            "   üíµ Economia total: ~$4.50 em 10 execu√ß√µes\n",
            "\n",
            "üéØ STATUS FINAL: ‚úÖ Cache ativo\n",
            "üöÄ Pronto para gerar embeddings com cache inteligente!\n",
            "üí° Os √≠ndices vazios ser√£o preenchidos conforme os embeddings forem gerados\n"
          ]
        }
      ],
      "source": [
        "# üóÑÔ∏è Inicializa√ß√£o do Sistema de Cache Elasticsearch\n",
        "print(\"üóÑÔ∏è INICIALIZANDO SISTEMA DE CACHE ELASTICSEARCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Importar m√≥dulo de gerenciamento de cache\n",
        "try:\n",
        "    from elasticsearch_manager import (\n",
        "        init_elasticsearch_cache, get_cache_status, save_dataset_to_cache,\n",
        "        save_embeddings_to_cache, load_embeddings_from_cache, \n",
        "        check_embeddings_in_cache, clear_elasticsearch_cache\n",
        "    )\n",
        "    print(\"‚úÖ M√≥dulo de cache Elasticsearch carregado\")\n",
        "    CACHE_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erro ao carregar m√≥dulo de cache: {e}\")\n",
        "    print(\"üí° Certifique-se de que o arquivo elasticsearch_manager.py est√° no diret√≥rio\")\n",
        "    CACHE_AVAILABLE = False\n",
        "\n",
        "# Inicializar conex√£o com Elasticsearch\n",
        "if CACHE_AVAILABLE:\n",
        "    print(\"\\nüîå Conectando ao Elasticsearch...\")\n",
        "    cache_connected = init_elasticsearch_cache(\n",
        "        host=ELASTICSEARCH_HOST,\n",
        "        port=ELASTICSEARCH_PORT\n",
        "    )\n",
        "    \n",
        "    if cache_connected:\n",
        "        print(\"‚úÖ Cache Elasticsearch inicializado com sucesso!\")\n",
        "        \n",
        "        # Verificar status do cache\n",
        "        print(\"\\nüìä STATUS DO CACHE:\")\n",
        "        print(\"-\" * 40)\n",
        "        status = get_cache_status()\n",
        "        \n",
        "        if status.get(\"connected\", False):\n",
        "            print(f\"üîå Conex√£o: ‚úÖ {status.get('host', 'N/A')}\")\n",
        "            print(f\"üìä √çndices encontrados: {len([k for k, v in status.get('indices', {}).items() if v.get('exists', False)])}\")\n",
        "            print(f\"üìã Total de documentos: {status.get('total_docs', 0):,}\")\n",
        "            print(f\"üíæ Espa√ßo usado: {status.get('total_size_mb', 0):.1f} MB\")\n",
        "            \n",
        "            # Mostrar detalhes por √≠ndice\n",
        "            print(f\"\\nüì¶ DETALHES DOS √çNDICES:\")\n",
        "            for index_name, info in status.get('indices', {}).items():\n",
        "                if info.get('exists', False):\n",
        "                    print(f\"   ‚úÖ {index_name:<20}: {info.get('doc_count', 0):>6,} docs ({info.get('size_mb', 0):>6.1f} MB)\")\n",
        "                else:\n",
        "                    print(f\"   ‚è≥ {index_name:<20}: Ser√° criado durante a gera√ß√£o\")\n",
        "            \n",
        "            # Verificar se deve usar cache\n",
        "            use_cache = os.getenv('USE_ELASTICSEARCH_CACHE', 'true').lower() == 'true'\n",
        "            force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
        "            \n",
        "            print(f\"\\nüéõÔ∏è CONFIGURA√á√ïES DO SISTEMA:\")\n",
        "            print(f\"   Usar cache: {'‚úÖ' if use_cache else '‚ùå'}\")\n",
        "            print(f\"   For√ßar regenera√ß√£o: {'‚úÖ' if force_regenerate else '‚ùå'}\")\n",
        "            \n",
        "            print(f\"\\nüí° EXPLICA√á√ÉO DAS CONFIGURA√á√ïES:\")\n",
        "            print(f\"   üîÑ Usar cache: {'Ativado' if use_cache else 'Desativado'}\")\n",
        "            if use_cache:\n",
        "                print(f\"      ‚Üí O sistema verificar√° se os embeddings j√° existem\")\n",
        "                print(f\"      ‚Üí Se existirem, carregar√° do cache (muito mais r√°pido)\")\n",
        "                print(f\"      ‚Üí Se n√£o existirem, gerar√° novos e salvar√° no cache\")\n",
        "            else:\n",
        "                print(f\"      ‚Üí Todos os embeddings ser√£o gerados do zero\")\n",
        "                print(f\"      ‚Üí Nenhum dado ser√° salvo no cache\")\n",
        "            \n",
        "            print(f\"   üîÑ For√ßar regenera√ß√£o: {'Ativado' if force_regenerate else 'Desativado'}\")\n",
        "            if force_regenerate:\n",
        "                print(f\"      ‚Üí Todos os embeddings ser√£o regenerados, mesmo se existirem\")\n",
        "                print(f\"      ‚Üí √ötil para atualizar embeddings com novos modelos\")\n",
        "                print(f\"      ‚Üí Ignora completamente o cache existente\")\n",
        "            else:\n",
        "                print(f\"      ‚Üí Usa cache quando dispon√≠vel (recomendado)\")\n",
        "                print(f\"      ‚Üí Gera apenas embeddings que n√£o existem\")\n",
        "            \n",
        "            if force_regenerate:\n",
        "                print(f\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Modo de regenera√ß√£o for√ßada ativado\")\n",
        "                print(f\"   Todos os embeddings ser√£o regenerados, ignorando cache\")\n",
        "            \n",
        "            print(f\"\\nüîÑ FLUXO DO SISTEMA DE CACHE:\")\n",
        "            print(f\"   1Ô∏è‚É£ Verifica√ß√£o: Checa se embeddings j√° existem no Elasticsearch\")\n",
        "            print(f\"   2Ô∏è‚É£ Valida√ß√£o: Confere integridade via hash MD5 dos textos\")\n",
        "            print(f\"   3Ô∏è‚É£ Gera√ß√£o seletiva: Gera apenas embeddings faltantes ou inv√°lidos\")\n",
        "            print(f\"   4Ô∏è‚É£ Salvamento: Armazena novos embeddings com metadata completa\")\n",
        "            print(f\"   5Ô∏è‚É£ Carregamento: Recupera embeddings existentes (muito mais r√°pido)\")\n",
        "            \n",
        "            print(f\"\\n‚è±Ô∏è  ECONOMIA DE TEMPO ESTIMADA:\")\n",
        "            print(f\"   üìä TF-IDF: 30 segundos ‚Üí 5 segundos (6x mais r√°pido)\")\n",
        "            print(f\"   üìä Word2Vec: 60 segundos ‚Üí 5 segundos (12x mais r√°pido)\")\n",
        "            print(f\"   üìä BERT: 120 segundos ‚Üí 5 segundos (24x mais r√°pido)\")\n",
        "            print(f\"   üìä Sentence-BERT: 90 segundos ‚Üí 5 segundos (18x mais r√°pido)\")\n",
        "            print(f\"   üìä OpenAI: 30 minutos ‚Üí 5 segundos (360x mais r√°pido!)\")\n",
        "            \n",
        "            print(f\"\\nüí∞ ECONOMIA DE CUSTO (OpenAI):\")\n",
        "            print(f\"   üíµ Primeira execu√ß√£o: ~$0.50 (gera e salva embeddings)\")\n",
        "            print(f\"   üíµ Execu√ß√µes seguintes: $0.00 (usa cache)\")\n",
        "            print(f\"   üíµ Economia total: ~$4.50 em 10 execu√ß√µes\")\n",
        "            \n",
        "        else:\n",
        "            print(f\"‚ùå Erro no cache: {status.get('error', 'Desconhecido')}\")\n",
        "    else:\n",
        "        print(\"‚ùå Falha ao conectar com Elasticsearch\")\n",
        "        print(\"üí° Verifique se o Docker est√° rodando: docker-compose up -d\")\n",
        "        CACHE_AVAILABLE = False\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Sistema de cache n√£o dispon√≠vel\")\n",
        "    print(\"üí° O notebook funcionar√° sem cache (mais lento)\")\n",
        "\n",
        "print(f\"\\nüéØ STATUS FINAL: {'‚úÖ Cache ativo' if CACHE_AVAILABLE and cache_connected else '‚ùå Cache inativo'}\")\n",
        "if CACHE_AVAILABLE and cache_connected:\n",
        "    print(f\"üöÄ Pronto para gerar embeddings com cache inteligente!\")\n",
        "    print(f\"üí° Os √≠ndices vazios ser√£o preenchidos conforme os embeddings forem gerados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üêå CARREGANDO BIBLIOTECAS PESADAS\n",
            "========================================\n",
            "‚è≥ Isso pode demorar alguns segundos...\n",
            "üîÑ Carregando Gensim...\n",
            "‚úÖ Gensim carregado com sucesso\n",
            "üîÑ Carregando Sentence Transformers...\n",
            "‚úÖ Sentence Transformers carregado com sucesso\n",
            "üîÑ Carregando Plotly...\n",
            "‚úÖ Plotly carregado com sucesso\n",
            "\n",
            "üìä RESUMO DAS BIBLIOTECAS PESADAS:\n",
            "   Gensim: ‚úÖ\n",
            "   Sentence Transformers: ‚úÖ\n",
            "   Plotly: ‚úÖ\n",
            "\n",
            "üéâ TODAS AS BIBLIOTECAS CARREGADAS COM SUCESSO!\n"
          ]
        }
      ],
      "source": [
        "# üêå Carregar Bibliotecas Pesadas (Pode Demorar)\n",
        "print(\"üêå CARREGANDO BIBLIOTECAS PESADAS\")\n",
        "print(\"=\" * 40)\n",
        "print(\"‚è≥ Isso pode demorar alguns segundos...\")\n",
        "\n",
        "# Gensim (pode ser lento)\n",
        "try:\n",
        "    print(\"üîÑ Carregando Gensim...\")\n",
        "    import gensim\n",
        "    from gensim.models import Word2Vec, KeyedVectors\n",
        "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "    print(\"‚úÖ Gensim carregado com sucesso\")\n",
        "    GENSIM_OK = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao carregar Gensim: {e}\")\n",
        "    GENSIM_OK = False\n",
        "\n",
        "# Sentence Transformers (pode ser lento)\n",
        "try:\n",
        "    print(\"üîÑ Carregando Sentence Transformers...\")\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    print(\"‚úÖ Sentence Transformers carregado com sucesso\")\n",
        "    SENTENCE_TRANSFORMERS_OK = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao carregar Sentence Transformers: {e}\")\n",
        "    SENTENCE_TRANSFORMERS_OK = False\n",
        "\n",
        "# Plotly (pode ser lento)\n",
        "try:\n",
        "    print(\"üîÑ Carregando Plotly...\")\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    import plotly.figure_factory as ff\n",
        "    print(\"‚úÖ Plotly carregado com sucesso\")\n",
        "    PLOTLY_OK = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao carregar Plotly: {e}\")\n",
        "    PLOTLY_OK = False\n",
        "\n",
        "print(f\"\\nüìä RESUMO DAS BIBLIOTECAS PESADAS:\")\n",
        "print(f\"   Gensim: {'‚úÖ' if GENSIM_OK else '‚ùå'}\")\n",
        "print(f\"   Sentence Transformers: {'‚úÖ' if SENTENCE_TRANSFORMERS_OK else '‚ùå'}\")\n",
        "print(f\"   Plotly: {'‚úÖ' if PLOTLY_OK else '‚ùå'}\")\n",
        "\n",
        "if all([GENSIM_OK, SENTENCE_TRANSFORMERS_OK, PLOTLY_OK]):\n",
        "    print(\"\\nüéâ TODAS AS BIBLIOTECAS CARREGADAS COM SUCESSO!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Algumas bibliotecas falharam, mas o notebook pode continuar funcionando\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Prepara√ß√£o dos Dados: 20 Newsgroups\n",
        "\n",
        "### **Por que 20 Newsgroups?**\n",
        "O dataset **20 Newsgroups** √© ideal para estudos de clustering porque:\n",
        "- **Classes bem definidas**: 20 categorias distintas de not√≠cias\n",
        "- **Tamanho gerenci√°vel**: ~18.000 documentos\n",
        "- **Qualidade**: Textos limpos e bem estruturados\n",
        "- **Diversidade**: T√≥picos variados (tecnologia, pol√≠tica, esportes, etc.)\n",
        "- **Ground truth**: Classes conhecidas para valida√ß√£o\n",
        "\n",
        "### **Sele√ß√£o das 10 Classes Mais Interessantes**\n",
        "Vamos selecionar as classes mais distintas e balanceadas para nosso estudo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Carregando 20 Newsgroups...\n",
            "‚úÖ Dataset carregado: 9085 documentos\n",
            "üìä Classes: 10\n",
            "üìà Distribui√ß√£o por classe:\n",
            "   soc.religion.christian: 969 documentos\n",
            "   sci.med: 948 documentos\n",
            "   sci.crypt: 943 documentos\n",
            "   misc.forsale: 933 documentos\n",
            "   comp.graphics: 930 documentos\n",
            "   rec.sport.baseball: 916 documentos\n",
            "   comp.sys.mac.hardware: 910 documentos\n",
            "   rec.autos: 905 documentos\n",
            "   talk.politics.guns: 871 documentos\n",
            "   alt.atheism: 760 documentos\n",
            "\n",
            "üìù Exemplos de textos por classe:\n",
            "================================================================================\n",
            "\n",
            "üîπ comp.graphics:\n",
            "   I'm still looking for Fractint drivers or a new release which supports the\n",
            " 24bit color mode of the Diamond Speedstar 24X.  There are some 2, 4 and 26\n",
            " million colros drivers, but none work with the 2...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üîπ sci.med:\n",
            "   It would be nice to think that individuals can somehow 'beat the system'\n",
            "and like a space explorer, boldly go where no man has gone before and\n",
            "return with a prize cure. Unfortunately, too often the pr...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üîπ alt.atheism:\n",
            "   ##I strongly suggest that you look up a book called THE BIBLE, THE QURAN, AND\n",
            "##SCIENCE by Maurice Baucaille, a French surgeon.  It is not comprehensive,\n",
            "##but, it is well researched.  I imagine your ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# üìä Carregamento e Sele√ß√£o dos Dados\n",
        "def load_20newsgroups_subset():\n",
        "    \"\"\"\n",
        "    Carrega um subconjunto balanceado do 20 Newsgroups com 10 classes selecionadas.\n",
        "    Retorna textos, labels e metadados.\n",
        "    \"\"\"\n",
        "    # Classes selecionadas para o estudo (diversas e interessantes)\n",
        "    selected_categories = [\n",
        "        'alt.atheism',\n",
        "        'comp.graphics', \n",
        "        'comp.sys.mac.hardware',\n",
        "        'misc.forsale',\n",
        "        'rec.autos',\n",
        "        'rec.sport.baseball',\n",
        "        'sci.crypt',\n",
        "        'sci.med',\n",
        "        'soc.religion.christian',\n",
        "        'talk.politics.guns'\n",
        "    ]\n",
        "    \n",
        "    print(\"üîÑ Carregando 20 Newsgroups...\")\n",
        "    \n",
        "    # Carregar dados\n",
        "    newsgroups = fetch_20newsgroups(\n",
        "        subset='all',\n",
        "        categories=selected_categories,\n",
        "        remove=('headers', 'footers', 'quotes'),\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'text': newsgroups.data,\n",
        "        'category': [newsgroups.target_names[i] for i in newsgroups.target],\n",
        "        'target': newsgroups.target\n",
        "    })\n",
        "    \n",
        "    # Limpeza b√°sica usando configura√ß√µes do .env\n",
        "    df['text'] = df['text'].str.strip()\n",
        "    df = df[df['text'].str.len() > TEXT_MIN_LENGTH]  # Usar configura√ß√£o do .env\n",
        "    \n",
        "    print(f\"‚úÖ Dataset carregado: {len(df)} documentos\")\n",
        "    print(f\"üìä Classes: {df['category'].nunique()}\")\n",
        "    print(f\"üìà Distribui√ß√£o por classe:\")\n",
        "    \n",
        "    # Mostrar distribui√ß√£o\n",
        "    class_counts = df['category'].value_counts()\n",
        "    for category, count in class_counts.items():\n",
        "        print(f\"   {category}: {count} documentos\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Carregar dados\n",
        "df = load_20newsgroups_subset()\n",
        "\n",
        "# Mostrar exemplos\n",
        "print(f\"\\nüìù Exemplos de textos por classe:\")\n",
        "print(\"=\" * 80)\n",
        "for category in df['category'].unique()[:3]:  # Mostrar apenas 3 classes\n",
        "    sample_text = df[df['category'] == category]['text'].iloc[0]\n",
        "    print(f\"\\nüîπ {category}:\")\n",
        "    print(f\"   {sample_text[:200]}...\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Salvamento do Dataset no Elasticsearch\n",
        "\n",
        "### **Por que Salvar o Dataset?**\n",
        "- **IDs √∫nicos**: O dataset 20 Newsgroups n√£o possui identificadores nativos\n",
        "- **Rastreabilidade**: Cada embedding ser√° vinculado ao documento original\n",
        "- **Valida√ß√£o**: Hash MD5 garante integridade dos dados\n",
        "- **Efici√™ncia**: Busca r√°pida de documentos por ID\n",
        "\n",
        "### **Estrutura no Elasticsearch**\n",
        "```\n",
        "üìÑ documents_dataset\n",
        "‚îú‚îÄ‚îÄ doc_id (string) - ID √∫nico gerado (doc_0001, doc_0002, ...)\n",
        "‚îú‚îÄ‚îÄ text (text) - Conte√∫do do documento\n",
        "‚îú‚îÄ‚îÄ category (keyword) - Categoria do documento\n",
        "‚îú‚îÄ‚îÄ target (integer) - √çndice num√©rico da categoria\n",
        "‚îú‚îÄ‚îÄ text_hash (keyword) - Hash MD5 para valida√ß√£o\n",
        "‚îî‚îÄ‚îÄ created_at (date) - Timestamp de cria√ß√£o\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ SALVANDO DATASET NO ELASTICSEARCH\n",
            "==================================================\n",
            "üîÑ Salvando dataset com IDs √∫nicos...\n",
            "‚úÖ √çndice 'documents_dataset' j√° existe\n",
            "‚úÖ Dataset salvo: 9085 documentos em 'documents_dataset'\n",
            "‚úÖ Dataset salvo com sucesso no Elasticsearch!\n",
            "üìä 9085 documentos salvos com IDs √∫nicos\n",
            "üîó Cada documento agora possui um ID √∫nico (doc_0001, doc_0002, ...)\n",
            "üîç Hash MD5 gerado para valida√ß√£o de integridade\n",
            "üìã IDs gerados: 9085 documentos\n",
            "üî¢ Exemplos: ['doc_0000', 'doc_0001', 'doc_0002'] ... ['doc_9082', 'doc_9083', 'doc_9084']\n",
            "\n",
            "üìä STATUS ATUALIZADO DO CACHE:\n",
            "   üìÑ documents_dataset: 9,085 docs (21.5 MB)\n",
            "\n",
            "üéØ Dataset preparado: 9085 documentos com IDs √∫nicos\n",
            "üîó Pr√≥ximo passo: Gera√ß√£o de embeddings com rastreabilidade completa\n"
          ]
        }
      ],
      "source": [
        "# üíæ Salvamento do Dataset no Elasticsearch com IDs √önicos\n",
        "print(\"üíæ SALVANDO DATASET NO ELASTICSEARCH\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Verificar se o cache est√° dispon√≠vel\n",
        "if CACHE_AVAILABLE and 'cache_connected' in locals() and cache_connected:\n",
        "    print(\"üîÑ Salvando dataset com IDs √∫nicos...\")\n",
        "    \n",
        "    # Salvar dataset no Elasticsearch\n",
        "    success = save_dataset_to_cache(df)\n",
        "    \n",
        "    if success:\n",
        "        print(\"‚úÖ Dataset salvo com sucesso no Elasticsearch!\")\n",
        "        print(f\"üìä {len(df)} documentos salvos com IDs √∫nicos\")\n",
        "        print(\"üîó Cada documento agora possui um ID √∫nico (doc_0001, doc_0002, ...)\")\n",
        "        print(\"üîç Hash MD5 gerado para valida√ß√£o de integridade\")\n",
        "        \n",
        "        # Gerar lista de IDs para uso posterior\n",
        "        doc_ids = [f\"doc_{i:04d}\" for i in range(len(df))]\n",
        "        print(f\"üìã IDs gerados: {len(doc_ids)} documentos\")\n",
        "        print(f\"üî¢ Exemplos: {doc_ids[:3]} ... {doc_ids[-3:]}\")\n",
        "        \n",
        "        # Verificar status do cache ap√≥s salvamento\n",
        "        print(f\"\\nüìä STATUS ATUALIZADO DO CACHE:\")\n",
        "        status = get_cache_status()\n",
        "        if status.get(\"connected\", False):\n",
        "            dataset_info = status.get('indices', {}).get('documents_dataset', {})\n",
        "            if dataset_info.get('exists', False):\n",
        "                print(f\"   üìÑ documents_dataset: {dataset_info.get('doc_count', 0):,} docs ({dataset_info.get('size_mb', 0):.1f} MB)\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Falha ao salvar dataset no Elasticsearch\")\n",
        "        print(\"üí° O notebook continuar√° sem cache (mais lento)\")\n",
        "        CACHE_AVAILABLE = False\n",
        "        doc_ids = [f\"doc_{i:04d}\" for i in range(len(df))]  # IDs locais como fallback\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cache n√£o dispon√≠vel, usando IDs locais\")\n",
        "    print(\"üí° O notebook funcionar√° sem cache (mais lento)\")\n",
        "    doc_ids = [f\"doc_{i:04d}\" for i in range(len(df))]  # IDs locais como fallback\n",
        "\n",
        "print(f\"\\nüéØ Dataset preparado: {len(df)} documentos com IDs √∫nicos\")\n",
        "print(f\"üîó Pr√≥ximo passo: Gera√ß√£o de embeddings com rastreabilidade completa\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç AN√ÅLISE DETALHADA DO DATASET\n",
            "============================================================\n",
            "üìä ESTAT√çSTICAS DO DATASET 20 NEWSGROUPS\n",
            "============================================================\n",
            "üìã Total de documentos: 9,085\n",
            "üè∑Ô∏è  N√∫mero de classes: 10\n",
            "üìè Tamanho m√©dio dos textos: 1130 caracteres\n",
            "üìè Tamanho mediano dos textos: 517 caracteres\n",
            "üìè Tamanho m√≠nimo: 51 caracteres\n",
            "üìè Tamanho m√°ximo: 70844 caracteres\n",
            "‚ö†Ô∏è  Textos > 6000 chars: 194 (2.1%)\n",
            "‚ö†Ô∏è  Textos > 8000 chars: 132 (1.5%)\n",
            "\n",
            "üìà DISTRIBUI√á√ÉO POR CLASSE:\n",
            "   soc.religion.christian        :  969 docs ( 10.7%)\n",
            "   sci.med                       :  948 docs ( 10.4%)\n",
            "   sci.crypt                     :  943 docs ( 10.4%)\n",
            "   misc.forsale                  :  933 docs ( 10.3%)\n",
            "   comp.graphics                 :  930 docs ( 10.2%)\n",
            "   rec.sport.baseball            :  916 docs ( 10.1%)\n",
            "   comp.sys.mac.hardware         :  910 docs ( 10.0%)\n",
            "   rec.autos                     :  905 docs ( 10.0%)\n",
            "   talk.politics.guns            :  871 docs (  9.6%)\n",
            "   alt.atheism                   :  760 docs (  8.4%)\n",
            "\n",
            "‚öñÔ∏è  AN√ÅLISE DE BALANCEAMENTO:\n",
            "   üìä C√°lculo: min(classes) / max(classes) = 760 / 969\n",
            "   üìä Resultado: 0.784\n",
            "   üìä Interpreta√ß√£o:\n",
            "      ‚Ä¢ 1.0 = perfeitamente balanceado\n",
            "      ‚Ä¢ 0.7+ = bem balanceado\n",
            "      ‚Ä¢ 0.4-0.7 = moderadamente balanceado\n",
            "      ‚Ä¢ <0.4 = desbalanceado\n",
            "   üìä Para clustering: Balanceamento ajuda na qualidade dos clusters\n",
            "   ‚úÖ Dataset bem balanceado\n"
          ]
        }
      ],
      "source": [
        "# üìä Estat√≠sticas Detalhadas do Dataset\n",
        "def print_dataset_statistics(df):\n",
        "    \"\"\"Imprime estat√≠sticas detalhadas do dataset\"\"\"\n",
        "    print(\"üìä ESTAT√çSTICAS DO DATASET 20 NEWSGROUPS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìã Total de documentos: {len(df):,}\")\n",
        "    print(f\"üè∑Ô∏è  N√∫mero de classes: {df['category'].nunique()}\")\n",
        "    print(f\"üìè Tamanho m√©dio dos textos: {df['text'].str.len().mean():.0f} caracteres\")\n",
        "    print(f\"üìè Tamanho mediano dos textos: {df['text'].str.len().median():.0f} caracteres\")\n",
        "    print(f\"üìè Tamanho m√≠nimo: {df['text'].str.len().min()} caracteres\")\n",
        "    print(f\"üìè Tamanho m√°ximo: {df['text'].str.len().max()} caracteres\")\n",
        "    print(f\"‚ö†Ô∏è  Textos > 6000 chars: {(df['text'].str.len() > 6000).sum()} ({(df['text'].str.len() > 6000).mean()*100:.1f}%)\")\n",
        "    print(f\"‚ö†Ô∏è  Textos > 8000 chars: {(df['text'].str.len() > 8000).sum()} ({(df['text'].str.len() > 8000).mean()*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nüìà DISTRIBUI√á√ÉO POR CLASSE:\")\n",
        "    class_counts = df['category'].value_counts()\n",
        "    for category, count in class_counts.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"   {category:<30}: {count:>4} docs ({percentage:>5.1f}%)\")\n",
        "    \n",
        "    # An√°lise de balanceamento\n",
        "    balance_ratio = class_counts.min() / class_counts.max()\n",
        "    print(f\"\\n‚öñÔ∏è  AN√ÅLISE DE BALANCEAMENTO:\")\n",
        "    print(f\"   üìä C√°lculo: min(classes) / max(classes) = {class_counts.min()} / {class_counts.max()}\")\n",
        "    print(f\"   üìä Resultado: {balance_ratio:.3f}\")\n",
        "    print(f\"   üìä Interpreta√ß√£o:\")\n",
        "    print(f\"      ‚Ä¢ 1.0 = perfeitamente balanceado\")\n",
        "    print(f\"      ‚Ä¢ 0.7+ = bem balanceado\")\n",
        "    print(f\"      ‚Ä¢ 0.4-0.7 = moderadamente balanceado\")\n",
        "    print(f\"      ‚Ä¢ <0.4 = desbalanceado\")\n",
        "    print(f\"   üìä Para clustering: Balanceamento ajuda na qualidade dos clusters\")\n",
        "    \n",
        "    if balance_ratio > 0.7:\n",
        "        print(\"   ‚úÖ Dataset bem balanceado\")\n",
        "    elif balance_ratio > 0.4:\n",
        "        print(\"   ‚ö†Ô∏è  Dataset moderadamente balanceado\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Dataset desbalanceado\")\n",
        "    \n",
        "    return class_counts\n",
        "\n",
        "# Executar an√°lise do dataset\n",
        "print(\"üîç AN√ÅLISE DETALHADA DO DATASET\")\n",
        "print(\"=\" * 60)\n",
        "class_counts = print_dataset_statistics(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä An√°lise Explorat√≥ria dos Dados\n",
        "\n",
        "### **Estat√≠sticas do Dataset**\n",
        "Vamos analisar as caracter√≠sticas dos nossos dados para entender melhor o desafio de clustering:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä AN√ÅLISE EXPLORAT√ìRIA DO DATASET\n",
            "==================================================\n",
            "üìã Total de documentos: 9085\n",
            "üè∑Ô∏è  N√∫mero de classes: 10\n",
            "üìè Tamanho m√©dio dos textos: 1130 caracteres\n",
            "üìè Tamanho mediano dos textos: 517 caracteres\n",
            "üìè Tamanho m√≠nimo: 51 caracteres\n",
            "üìè Tamanho m√°ximo: 70844 caracteres\n",
            "\n",
            "üìä DISTRIBUI√á√ÉO POR CLASSE:\n",
            "   soc.religion.christian        :  969 docs ( 10.7%)\n",
            "   sci.med                       :  948 docs ( 10.4%)\n",
            "   sci.crypt                     :  943 docs ( 10.4%)\n",
            "   misc.forsale                  :  933 docs ( 10.3%)\n",
            "   comp.graphics                 :  930 docs ( 10.2%)\n",
            "   rec.sport.baseball            :  916 docs ( 10.1%)\n",
            "   comp.sys.mac.hardware         :  910 docs ( 10.0%)\n",
            "   rec.autos                     :  905 docs ( 10.0%)\n",
            "   talk.politics.guns            :  871 docs (  9.6%)\n",
            "   alt.atheism                   :  760 docs (  8.4%)\n",
            "\n",
            "‚öñÔ∏è  Balanceamento: 0.784 (1.0 = perfeitamente balanceado)\n",
            "   ‚úÖ Dataset bem balanceado\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# üìà An√°lise Explorat√≥ria Detalhada\n",
        "def analyze_dataset(df):\n",
        "    \"\"\"An√°lise completa do dataset para clustering\"\"\"\n",
        "    \n",
        "    print(\"üìä AN√ÅLISE EXPLORAT√ìRIA DO DATASET\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Estat√≠sticas b√°sicas\n",
        "    print(f\"üìã Total de documentos: {len(df)}\")\n",
        "    print(f\"üè∑Ô∏è  N√∫mero de classes: {df['category'].nunique()}\")\n",
        "    print(f\"üìè Tamanho m√©dio dos textos: {df['text'].str.len().mean():.0f} caracteres\")\n",
        "    print(f\"üìè Tamanho mediano dos textos: {df['text'].str.len().median():.0f} caracteres\")\n",
        "    print(f\"üìè Tamanho m√≠nimo: {df['text'].str.len().min()} caracteres\")\n",
        "    print(f\"üìè Tamanho m√°ximo: {df['text'].str.len().max()} caracteres\")\n",
        "    \n",
        "    # Distribui√ß√£o por classe\n",
        "    print(f\"\\nüìä DISTRIBUI√á√ÉO POR CLASSE:\")\n",
        "    class_counts = df['category'].value_counts()\n",
        "    for category, count in class_counts.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"   {category:<30}: {count:>4} docs ({percentage:>5.1f}%)\")\n",
        "    \n",
        "    # An√°lise de balanceamento\n",
        "    balance_ratio = class_counts.min() / class_counts.max()\n",
        "    print(f\"\\n‚öñÔ∏è  Balanceamento: {balance_ratio:.3f} (1.0 = perfeitamente balanceado)\")\n",
        "    \n",
        "    if balance_ratio > 0.7:\n",
        "        print(\"   ‚úÖ Dataset bem balanceado\")\n",
        "    elif balance_ratio > 0.4:\n",
        "        print(\"   ‚ö†Ô∏è  Dataset moderadamente balanceado\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Dataset desbalanceado\")\n",
        "    \n",
        "    return class_counts\n",
        "\n",
        "# Executar an√°lise\n",
        "class_counts = analyze_dataset(df)\n",
        "\n",
        "# Visualiza√ß√£o da distribui√ß√£o\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "class_counts.plot(kind='bar', color='skyblue', alpha=0.7)\n",
        "plt.title('Distribui√ß√£o de Documentos por Classe')\n",
        "plt.xlabel('Classe')\n",
        "plt.ylabel('N√∫mero de Documentos')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df['text'].str.len().hist(bins=50, color='lightcoral', alpha=0.7)\n",
        "plt.title('Distribui√ß√£o do Tamanho dos Textos')\n",
        "plt.xlabel('N√∫mero de Caracteres')\n",
        "plt.ylabel('Frequ√™ncia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Embeddings Cl√°ssicos: TF-IDF e Word2Vec\n",
        "\n",
        "### **TF-IDF (Term Frequency-Inverse Document Frequency) - O Cl√°ssico**\n",
        "**TF-IDF** √© um dos m√©todos mais fundamentais para representa√ß√£o de textos:\n",
        "- **TF (Term Frequency)**: Frequ√™ncia da palavra no documento\n",
        "- **IDF (Inverse Document Frequency)**: Raridade da palavra no corpus\n",
        "- **F√≥rmula**: TF-IDF = TF √ó log(N/DF), onde N = total de documentos, DF = documentos contendo a palavra\n",
        "- **Caracter√≠sticas**: Matriz esparsa, interpret√°vel, baseline s√≥lido\n",
        "\n",
        "### **Word2Vec (2013) - A Primeira Revolu√ß√£o**\n",
        "**Word2Vec** foi o primeiro modelo a capturar efetivamente similaridade sem√¢ntica atrav√©s de:\n",
        "- **Skip-gram**: Prediz palavras vizinhas dado uma palavra central\n",
        "- **CBOW**: Prediz palavra central dado contexto\n",
        "- **Janela deslizante**: Considera palavras em uma janela de contexto\n",
        "- **Resultado**: Palavras similares ficam pr√≥ximas no espa√ßo vetorial\n",
        "\n",
        "### **Por que Usar Embeddings Cl√°ssicos?**\n",
        "- **Simplicidade**: F√°ceis de entender e implementar\n",
        "- **Efici√™ncia**: R√°pidos para treinar e usar\n",
        "- **Baseline**: Excelente ponto de partida para compara√ß√£o\n",
        "- **Interpretabilidade**: Podemos visualizar palavras similares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Resumo dos Embeddings Cl√°ssicos\n",
        "\n",
        "### **O que Aprendemos?**\n",
        "Os embeddings cl√°ssicos (TF-IDF e Word2Vec) s√£o fundamentais para entender representa√ß√µes de texto:\n",
        "\n",
        "- **TF-IDF**: Baseado em frequ√™ncia, simples e interpret√°vel\n",
        "- **Word2Vec**: Captura contexto sem√¢ntico, palavras similares ficam pr√≥ximas\n",
        "- **Efici√™ncia**: R√°pidos para treinar e usar\n",
        "- **Baseline**: Excelente ponto de partida para compara√ß√£o\n",
        "- **Interpretabilidade**: Podemos visualizar palavras similares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Embeddings Cl√°ssicos: TF-IDF e Word2Vec\n",
        "\n",
        "### **TF-IDF (1970s) - Frequ√™ncia e Import√¢ncia**\n",
        "**TF-IDF** √© um dos m√©todos mais antigos e ainda amplamente usado:\n",
        "- **TF (Term Frequency)**: Frequ√™ncia do termo no documento\n",
        "- **IDF (Inverse Document Frequency)**: Raridade do termo no corpus\n",
        "- **C√°lculo**: `TF(t,d) √ó IDF(t,D) = tf(t,d) √ó log(N/df(t))`\n",
        "- **Resultado**: Matriz esparsa onde cada documento √© um vetor de frequ√™ncias ponderadas\n",
        "- **Dimens√µes**: Baseado no tamanho do vocabul√°rio (muito maior que Word2Vec)\n",
        "\n",
        "### **Word2Vec (2013) - A Primeira Revolu√ß√£o**\n",
        "**Word2Vec** foi o primeiro modelo a capturar efetivamente similaridade sem√¢ntica atrav√©s de:\n",
        "- **Skip-gram**: Prediz palavras vizinhas dado uma palavra central\n",
        "- **CBOW**: Prediz palavra central dado contexto\n",
        "- **Janela deslizante**: Considera palavras em uma janela de contexto\n",
        "- **Resultado**: Palavras similares ficam pr√≥ximas no espa√ßo vetorial\n",
        "- **Dimens√µes**: Configur√°vel (geralmente 100-300)\n",
        "\n",
        "### **Por que Usar Embeddings Cl√°ssicos?**\n",
        "- **Simplicidade**: F√°ceis de entender e implementar\n",
        "- **Efici√™ncia**: R√°pidos para treinar e usar\n",
        "- **Baseline**: Excelente ponto de partida para compara√ß√£o\n",
        "- **Interpretabilidade**: Podemos visualizar palavras similares\n",
        "- **Sem depend√™ncias**: N√£o precisam de modelos pr√©-treinados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèõÔ∏è GERANDO EMBEDDINGS CL√ÅSSICOS\n",
            "==================================================\n",
            "üíª Processamento local (sem internet necess√°ria)\n",
            "\n",
            "üìä Gerando embeddings TF-IDF...\n",
            "üîÑ Gerando embeddings TF-IDF localmente...\n",
            "   üíª Processamento local (sem internet necess√°ria)\n",
            "   üìä TF-IDF: Term Frequency √ó Inverse Document Frequency\n",
            "‚úÖ Embeddings TF-IDF gerados localmente: (9085, 4096)\n",
            "   üìä Vocabul√°rio: 4096 palavras\n",
            "   üìä Dimens√µes: 4096 features\n",
            "   üìä Densidade: 0.011 (1.0 = denso, 0.0 = esparso)\n",
            "\n",
            "üìä Gerando embeddings Word2Vec...\n",
            "   üìä Usando amostra de 5000 documentos para treinamento\n",
            "\n",
            "   ‚ÑπÔ∏è  O QUE √â O TREINAMENTO DO WORD2VEC?\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "   Word2Vec √© um modelo que APRENDE representa√ß√µes de palavras\n",
            "   analisando como elas aparecem juntas nos textos.\n",
            "   \n",
            "   üîÑ PROCESSO DE TREINAMENTO:\n",
            "   1. L√™ todos os 5000 documentos palavra por palavra\n",
            "   2. Para cada palavra, analisa as palavras vizinhas (janela de contexto)\n",
            "   3. Aprende: palavras que aparecem em contextos similares = significados similares\n",
            "   4. Gera um vetor num√©rico (100 n√∫meros) para cada palavra √∫nica\n",
            "   \n",
            "   üìä EXEMPLO PR√ÅTICO:\n",
            "   Se 'computer', 'laptop' e 'desktop' aparecem frequentemente com\n",
            "   as mesmas palavras vizinhas ('software', 'hardware', 'program'),\n",
            "   o modelo aprende que s√£o conceitos relacionados e seus vetores\n",
            "   ficam pr√≥ximos no espa√ßo vetorial.\n",
            "   \n",
            "   ‚ö†Ô∏è  POR QUE APENAS 5000 DOCUMENTOS?\n",
            "   O treinamento do Word2Vec √© computacionalmente custoso. Usar\n",
            "   5000 documentos (amostra representativa) garante:\n",
            "   - ‚úÖ Treinamento mais r√°pido (~10 segundos vs ~30 segundos)\n",
            "   - ‚úÖ Vocabul√°rio suficiente para o dom√≠nio\n",
            "   - ‚úÖ Qualidade mantida para fins did√°ticos\n",
            "üîÑ Treinando Word2Vec...\n",
            "‚úÖ Word2Vec treinado: 14066 palavras √∫nicas\n",
            "\n",
            "   üìö VOCABUL√ÅRIO APRENDIDO: 14066 palavras\n",
            "   Cada palavra agora possui um vetor de 100 dimens√µes que\n",
            "   representa seu significado sem√¢ntico baseado no contexto.\n",
            "   \n",
            "   üîÑ PR√ìXIMO PASSO: Gerar embeddings de documentos\n",
            "   Agora que temos vetores para palavras individuais, podemos\n",
            "   criar embeddings de documentos inteiros calculando a M√âDIA\n",
            "   dos vetores de todas as palavras do documento.\n",
            "\n",
            "üîÑ Gerando embeddings Word2Vec para todos os documentos...\n",
            "   üí° COMO FUNCIONA:\n",
            "   Para cada documento (dos 9085 totais):\n",
            "   1. Separa o texto em palavras\n",
            "   2. Para cada palavra, busca seu vetor no vocabul√°rio treinado\n",
            "   3. Calcula a M√âDIA de todos os vetores de palavras\n",
            "   4. Resultado: 1 vetor de 100 dimens√µes por documento\n",
            "‚úÖ Embeddings Word2Vec gerados: (9085, 100)\n",
            "   üìä 9085 documentos √ó 100 dimens√µes = 9085 vetores\n",
            "\n",
            "üîç Exemplos de palavras similares (Word2Vec):\n",
            "   computer: ['graphics', 'vision', 'project']\n",
            "   car: ['dealer', 'tires', 'car.']\n",
            "   science: ['psychology', 'fiction', 'science,']\n",
            "   religion: ['religion,', 'christianity', 'evolutionary']\n",
            "\n",
            "üìã RESUMO DOS EMBEDDINGS CL√ÅSSICOS:\n",
            "   TF-IDF: (9085, 4096)\n",
            "   Word2Vec: (9085, 100)\n"
          ]
        }
      ],
      "source": [
        "# üèõÔ∏è Implementa√ß√£o de Embeddings Cl√°ssicos com Cache Inteligente\n",
        "def generate_tfidf_embeddings(texts, max_features=4096):\n",
        "    \"\"\"Gera embeddings usando TF-IDF com explica√ß√µes detalhadas\"\"\"\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    \n",
        "    print(\"üîÑ Gerando embeddings TF-IDF localmente...\")\n",
        "    print(\"   üíª Processamento local (sem internet necess√°ria)\")\n",
        "    print(\"   üìä TF-IDF: Term Frequency √ó Inverse Document Frequency\")\n",
        "    \n",
        "    # Configurar TF-IDF\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2),  # Unigramas e bigramas\n",
        "        min_df=2,  # Palavra deve aparecer em pelo menos 2 documentos\n",
        "        max_df=0.95  # Palavra deve aparecer em no m√°ximo 95% dos documentos\n",
        "    )\n",
        "    \n",
        "    # Treinar e transformar\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    embeddings = tfidf_matrix.toarray()\n",
        "    \n",
        "    print(f\"‚úÖ Embeddings TF-IDF gerados localmente: {embeddings.shape}\")\n",
        "    print(f\"   üìä Vocabul√°rio: {len(vectorizer.vocabulary_)} palavras\")\n",
        "    print(f\"   üìä Dimens√µes: {embeddings.shape[1]} features\")\n",
        "    print(f\"   üìä Densidade: {(embeddings != 0).mean():.3f} (1.0 = denso, 0.0 = esparso)\")\n",
        "    \n",
        "    return embeddings, vectorizer\n",
        "\n",
        "def preprocess_text_for_word2vec(texts):\n",
        "    \"\"\"Preprocessa textos para treinamento do Word2Vec\"\"\"\n",
        "    processed_texts = []\n",
        "    for text in texts:\n",
        "        # Tokeniza√ß√£o simples\n",
        "        words = text.lower().split()\n",
        "        # Remover palavras muito curtas e n√∫meros\n",
        "        words = [word for word in words if len(word) > 2 and not word.isdigit()]\n",
        "        processed_texts.append(words)\n",
        "    return processed_texts\n",
        "\n",
        "def train_word2vec(texts, vector_size=100, window=5, min_count=5, workers=4):\n",
        "    \"\"\"Treina modelo Word2Vec personalizado com explica√ß√µes did√°ticas\"\"\"\n",
        "    print(\"üîÑ Treinando Word2Vec...\")\n",
        "    \n",
        "    # Preprocessar textos\n",
        "    processed_texts = preprocess_text_for_word2vec(texts)\n",
        "    \n",
        "    # Treinar modelo\n",
        "    model = Word2Vec(\n",
        "        sentences=processed_texts,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        workers=workers,\n",
        "        sg=1,  # Skip-gram\n",
        "        epochs=10\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Word2Vec treinado: {len(model.wv)} palavras √∫nicas\")\n",
        "    return model\n",
        "\n",
        "def get_document_embeddings_word2vec(model, texts):\n",
        "    \"\"\"Gera embeddings de documentos usando Word2Vec\"\"\"\n",
        "    embeddings = []\n",
        "    processed_texts = preprocess_text_for_word2vec(texts)\n",
        "    \n",
        "    for words in processed_texts:\n",
        "        # M√©dia dos vetores das palavras (m√©todo simples)\n",
        "        word_vectors = []\n",
        "        for word in words:\n",
        "            if word in model.wv:\n",
        "                word_vectors.append(model.wv[word])\n",
        "        \n",
        "        if word_vectors:\n",
        "            doc_embedding = np.mean(word_vectors, axis=0)\n",
        "            # Normalizar para evitar vetores zero\n",
        "            if np.linalg.norm(doc_embedding) == 0:\n",
        "                doc_embedding = np.random.normal(0, 0.01, model.vector_size)\n",
        "        else:\n",
        "            # Se nenhuma palavra for encontrada, usar vetor pequeno aleat√≥rio\n",
        "            doc_embedding = np.random.normal(0, 0.01, model.vector_size)\n",
        "        \n",
        "        embeddings.append(doc_embedding)\n",
        "    \n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Gerar embeddings cl√°ssicos\n",
        "print(\"üèõÔ∏è GERANDO EMBEDDINGS CL√ÅSSICOS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"üíª Processamento local (sem internet necess√°ria)\")\n",
        "\n",
        "# TF-IDF (mais r√°pido)\n",
        "print(\"\\nüìä Gerando embeddings TF-IDF...\")\n",
        "tfidf_embeddings, tfidf_vectorizer = generate_tfidf_embeddings(df['text'].tolist())\n",
        "\n",
        "# Word2Vec (mais lento, usar amostra)\n",
        "print(\"\\nüìä Gerando embeddings Word2Vec...\")\n",
        "sample_size = min(5000, len(df))\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "print(f\"   üìä Usando amostra de {sample_size} documentos para treinamento\")\n",
        "\n",
        "print(f\"\\n   ‚ÑπÔ∏è  O QUE √â O TREINAMENTO DO WORD2VEC?\")\n",
        "print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "print(f\"   Word2Vec √© um modelo que APRENDE representa√ß√µes de palavras\")\n",
        "print(f\"   analisando como elas aparecem juntas nos textos.\")\n",
        "print(f\"   \")\n",
        "print(f\"   üîÑ PROCESSO DE TREINAMENTO:\")\n",
        "print(f\"   1. L√™ todos os {sample_size} documentos palavra por palavra\")\n",
        "print(f\"   2. Para cada palavra, analisa as palavras vizinhas (janela de contexto)\")\n",
        "print(f\"   3. Aprende: palavras que aparecem em contextos similares = significados similares\")\n",
        "print(f\"   4. Gera um vetor num√©rico (100 n√∫meros) para cada palavra √∫nica\")\n",
        "print(f\"   \")\n",
        "print(f\"   üìä EXEMPLO PR√ÅTICO:\")\n",
        "print(f\"   Se 'computer', 'laptop' e 'desktop' aparecem frequentemente com\")\n",
        "print(f\"   as mesmas palavras vizinhas ('software', 'hardware', 'program'),\")\n",
        "print(f\"   o modelo aprende que s√£o conceitos relacionados e seus vetores\")\n",
        "print(f\"   ficam pr√≥ximos no espa√ßo vetorial.\")\n",
        "print(f\"   \")\n",
        "print(f\"   ‚ö†Ô∏è  POR QUE APENAS {sample_size} DOCUMENTOS?\")\n",
        "print(f\"   O treinamento do Word2Vec √© computacionalmente custoso. Usar\")\n",
        "print(f\"   {sample_size} documentos (amostra representativa) garante:\")\n",
        "print(f\"   - ‚úÖ Treinamento mais r√°pido (~10 segundos vs ~30 segundos)\")\n",
        "print(f\"   - ‚úÖ Vocabul√°rio suficiente para o dom√≠nio\")\n",
        "print(f\"   - ‚úÖ Qualidade mantida para fins did√°ticos\")\n",
        "\n",
        "# Treinar Word2Vec\n",
        "word2vec_model = train_word2vec(df_sample['text'].tolist())\n",
        "\n",
        "print(f\"\\n   üìö VOCABUL√ÅRIO APRENDIDO: {len(word2vec_model.wv)} palavras\")\n",
        "print(f\"   Cada palavra agora possui um vetor de 100 dimens√µes que\")\n",
        "print(f\"   representa seu significado sem√¢ntico baseado no contexto.\")\n",
        "print(f\"   \")\n",
        "print(f\"   üîÑ PR√ìXIMO PASSO: Gerar embeddings de documentos\")\n",
        "print(f\"   Agora que temos vetores para palavras individuais, podemos\")\n",
        "print(f\"   criar embeddings de documentos inteiros calculando a M√âDIA\")\n",
        "print(f\"   dos vetores de todas as palavras do documento.\")\n",
        "\n",
        "# Gerar embeddings para todos os documentos\n",
        "print(\"\\nüîÑ Gerando embeddings Word2Vec para todos os documentos...\")\n",
        "print(f\"   üí° COMO FUNCIONA:\")\n",
        "print(f\"   Para cada documento (dos {len(df)} totais):\")\n",
        "print(f\"   1. Separa o texto em palavras\")\n",
        "print(f\"   2. Para cada palavra, busca seu vetor no vocabul√°rio treinado\")\n",
        "print(f\"   3. Calcula a M√âDIA de todos os vetores de palavras\")\n",
        "print(f\"   4. Resultado: 1 vetor de 100 dimens√µes por documento\")\n",
        "\n",
        "word2vec_embeddings = get_document_embeddings_word2vec(word2vec_model, df['text'].tolist())\n",
        "\n",
        "print(f\"‚úÖ Embeddings Word2Vec gerados: {word2vec_embeddings.shape}\")\n",
        "print(f\"   üìä {len(df)} documentos √ó 100 dimens√µes = {len(df)} vetores\")\n",
        "\n",
        "# Mostrar palavras similares (exemplo)\n",
        "print(f\"\\nüîç Exemplos de palavras similares (Word2Vec):\")\n",
        "example_words = ['computer', 'car', 'science', 'religion']\n",
        "for word in example_words:\n",
        "    if word in word2vec_model.wv:\n",
        "        similar = word2vec_model.wv.most_similar(word, topn=3)\n",
        "        print(f\"   {word}: {[w for w, _ in similar]}\")\n",
        "    else:\n",
        "        print(f\"   {word}: palavra n√£o encontrada no vocabul√°rio\")\n",
        "\n",
        "print(f\"\\nüìã RESUMO DOS EMBEDDINGS CL√ÅSSICOS:\")\n",
        "print(f\"   TF-IDF: {tfidf_embeddings.shape}\")\n",
        "print(f\"   Word2Vec: {word2vec_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Salvamento dos Embeddings Cl√°ssicos no Cache\n",
        "\n",
        "### **Sistema de Cache Inteligente**\n",
        "- **Verifica√ß√£o autom√°tica**: Detecta se embeddings j√° existem\n",
        "- **Valida√ß√£o de integridade**: Confere hash MD5 dos textos\n",
        "- **Gera√ß√£o seletiva**: Cria apenas embeddings faltantes ou inv√°lidos\n",
        "- **Rastreabilidade**: Vincula cada embedding ao documento original\n",
        "\n",
        "### **Benef√≠cios do Cache**\n",
        "- **TF-IDF**: 30s ‚Üí 5s (6x mais r√°pido)\n",
        "- **Word2Vec**: 60s ‚Üí 5s (12x mais r√°pido)\n",
        "- **Economia de tempo**: Evita reprocessamento desnecess√°rio\n",
        "- **Consist√™ncia**: Garante que os mesmos textos gerem os mesmos embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ SALVANDO EMBEDDINGS CL√ÅSSICOS NO CACHE\n",
            "============================================================\n",
            "üîç Verificando cache para embeddings cl√°ssicos...\n",
            "\n",
            "üìä Verificando cache TF-IDF...\n",
            "‚úÖ Todos os embeddings TF-IDF j√° existem no cache\n",
            "üì• Carregando TF-IDF do cache...\n",
            "‚úÖ Embeddings carregados: (9085, 4096) de 'embeddings_tfidf'\n",
            "\n",
            "üìä Verificando cache Word2Vec...\n",
            "‚úÖ Todos os embeddings Word2Vec j√° existem no cache\n",
            "üì• Carregando Word2Vec do cache...\n",
            "‚úÖ Embeddings carregados: (9085, 100) de 'embeddings_word2vec'\n",
            "\n",
            "‚úÖ Embeddings cl√°ssicos salvos no cache com sucesso!\n",
            "üìä TF-IDF: (9085, 4096)\n",
            "üìä Word2Vec: (9085, 100)\n",
            "\n",
            "üéØ Pr√≥ximo passo: Gera√ß√£o de embeddings modernos (BERT, Sentence-BERT, OpenAI)\n"
          ]
        }
      ],
      "source": [
        "# üíæ Salvamento dos Embeddings Cl√°ssicos no Cache Elasticsearch\n",
        "print(\"üíæ SALVANDO EMBEDDINGS CL√ÅSSICOS NO CACHE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar se deve usar cache\n",
        "use_cache = CACHE_AVAILABLE and 'cache_connected' in locals() and cache_connected\n",
        "force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
        "\n",
        "if use_cache and not force_regenerate:\n",
        "    print(\"üîç Verificando cache para embeddings cl√°ssicos...\")\n",
        "    \n",
        "    # Verificar TF-IDF\n",
        "    print(\"\\nüìä Verificando cache TF-IDF...\")\n",
        "    all_exist_tfidf, existing_tfidf, missing_tfidf = check_embeddings_in_cache('embeddings_tfidf', doc_ids)\n",
        "    \n",
        "    if all_exist_tfidf:\n",
        "        print(\"‚úÖ Todos os embeddings TF-IDF j√° existem no cache\")\n",
        "        print(\"üì• Carregando TF-IDF do cache...\")\n",
        "        tfidf_embeddings = load_embeddings_from_cache('embeddings_tfidf', doc_ids)\n",
        "        if tfidf_embeddings is None:\n",
        "            print(\"‚ùå Falha ao carregar TF-IDF do cache, regenerando...\")\n",
        "            tfidf_embeddings, tfidf_vectorizer = generate_tfidf_embeddings(df['text'].tolist())\n",
        "            save_embeddings_to_cache('embeddings_tfidf', tfidf_embeddings, doc_ids, df['text'].tolist(), 'tfidf')\n",
        "    else:\n",
        "        print(f\"üîÑ {len(missing_tfidf)} embeddings TF-IDF faltando, salvando no cache...\")\n",
        "        save_embeddings_to_cache('embeddings_tfidf', tfidf_embeddings, doc_ids, df['text'].tolist(), 'tfidf')\n",
        "    \n",
        "    # Verificar Word2Vec\n",
        "    print(\"\\nüìä Verificando cache Word2Vec...\")\n",
        "    all_exist_word2vec, existing_word2vec, missing_word2vec = check_embeddings_in_cache('embeddings_word2vec', doc_ids)\n",
        "    \n",
        "    if all_exist_word2vec:\n",
        "        print(\"‚úÖ Todos os embeddings Word2Vec j√° existem no cache\")\n",
        "        print(\"üì• Carregando Word2Vec do cache...\")\n",
        "        word2vec_embeddings = load_embeddings_from_cache('embeddings_word2vec', doc_ids)\n",
        "        if word2vec_embeddings is None:\n",
        "            print(\"‚ùå Falha ao carregar Word2Vec do cache, regenerando...\")\n",
        "            # Regenerar Word2Vec se necess√°rio\n",
        "            sample_size = min(5000, len(df))\n",
        "            df_sample = df.sample(n=sample_size, random_state=42)\n",
        "            word2vec_model = train_word2vec(df_sample['text'].tolist())\n",
        "            word2vec_embeddings = get_document_embeddings_word2vec(word2vec_model, df['text'].tolist())\n",
        "            save_embeddings_to_cache('embeddings_word2vec', word2vec_embeddings, doc_ids, df['text'].tolist(), 'word2vec')\n",
        "    else:\n",
        "        print(f\"üîÑ {len(missing_word2vec)} embeddings Word2Vec faltando, salvando no cache...\")\n",
        "        save_embeddings_to_cache('embeddings_word2vec', word2vec_embeddings, doc_ids, df['text'].tolist(), 'word2vec')\n",
        "    \n",
        "    print(f\"\\n‚úÖ Embeddings cl√°ssicos salvos no cache com sucesso!\")\n",
        "    print(f\"üìä TF-IDF: {tfidf_embeddings.shape}\")\n",
        "    print(f\"üìä Word2Vec: {word2vec_embeddings.shape}\")\n",
        "    \n",
        "elif force_regenerate:\n",
        "    print(\"‚ö†Ô∏è  Modo de regenera√ß√£o for√ßada ativado\")\n",
        "    print(\"üîÑ Salvando embeddings cl√°ssicos no cache (ignorando existentes)...\")\n",
        "    \n",
        "    # Salvar TF-IDF\n",
        "    print(\"\\nüìä Salvando TF-IDF no cache...\")\n",
        "    save_embeddings_to_cache('embeddings_tfidf', tfidf_embeddings, doc_ids, df['text'].tolist(), 'tfidf')\n",
        "    \n",
        "    # Salvar Word2Vec\n",
        "    print(\"\\nüìä Salvando Word2Vec no cache...\")\n",
        "    save_embeddings_to_cache('embeddings_word2vec', word2vec_embeddings, doc_ids, df['text'].tolist(), 'word2vec')\n",
        "    \n",
        "    print(f\"\\n‚úÖ Embeddings cl√°ssicos salvos no cache (regenera√ß√£o for√ßada)!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cache n√£o dispon√≠vel, embeddings n√£o ser√£o salvos\")\n",
        "    print(\"üí° O notebook continuar√° sem cache (mais lento)\")\n",
        "\n",
        "print(f\"\\nüéØ Pr√≥ximo passo: Gera√ß√£o de embeddings modernos (BERT, Sentence-BERT, OpenAI)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Embeddings Modernos: BERT, Sentence-BERT e OpenAI\n",
        "\n",
        "### **BERT (2018) - Contextualiza√ß√£o Bidirecional**\n",
        "**BERT** revolucionou o NLP com:\n",
        "- **Bidirecional**: L√™ texto em ambas as dire√ß√µes\n",
        "- **Attention Mechanism**: Foca em palavras relevantes\n",
        "- **Pre-training**: Aprende representa√ß√µes gerais\n",
        "- **Fine-tuning**: Adapta para tarefas espec√≠ficas\n",
        "- **Dimens√µes**: 768 (bert-base-uncased)\n",
        "\n",
        "### **Sentence-BERT (2019) - Otimizado para Similaridade**\n",
        "**Sentence-BERT** foi especificamente otimizado para:\n",
        "- **Similaridade de senten√ßas**: Embeddings compar√°veis\n",
        "- **Clustering**: Ideal para agrupamento sem√¢ntico\n",
        "- **Busca sem√¢ntica**: Encontrar documentos similares\n",
        "- **Dimens√µes**: 384 (all-MiniLM-L6-v2)\n",
        "\n",
        "### **OpenAI Embeddings (2020+) - √öltima Gera√ß√£o**\n",
        "**OpenAI Embeddings** oferecem:\n",
        "- **text-embedding-3-small**: 1536 dim, r√°pido e eficiente\n",
        "- **text-embedding-3-large**: 3072 dim, m√°xima qualidade\n",
        "- **Otimiza√ß√£o**: Treinados especificamente para similaridade\n",
        "- **Qualidade superior**: Melhores resultados em benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ GERANDO EMBEDDINGS MODERNOS LOCAIS\n",
            "==================================================\n",
            "üíª Processamento local (sem internet necess√°ria)\n",
            "\n",
            "üìä Gerando embeddings Sentence-BERT...\n",
            "‚úÖ Todos os embeddings Sentence-BERT j√° existem no cache\n",
            "üì• Carregando Sentence-BERT do cache...\n",
            "‚úÖ Embeddings carregados: (9085, 384) de 'embeddings_sbert'\n",
            "\n",
            "üìä Gerando embeddings BERT...\n",
            "‚úÖ Todos os embeddings BERT j√° existem no cache\n",
            "üì• Carregando BERT do cache...\n",
            "‚úÖ Embeddings carregados: (9085, 768) de 'embeddings_bert'\n",
            "\n",
            "üìã RESUMO DOS EMBEDDINGS MODERNOS LOCAIS:\n",
            "   BERT: (9085, 768)\n",
            "   Sentence-BERT: (9085, 384)\n",
            "\n",
            "üíæ Embeddings modernos salvos no cache Elasticsearch!\n",
            "‚è±Ô∏è  Pr√≥xima execu√ß√£o ser√° muito mais r√°pida (5s vs 2min)\n"
          ]
        }
      ],
      "source": [
        "# üöÄ Gera√ß√£o de Embeddings Modernos Locais com Cache Inteligente\n",
        "def generate_bert_embeddings(texts, model_name='bert-base-uncased'):\n",
        "    \"\"\"Gera embeddings usando BERT (processamento local)\"\"\"\n",
        "    print(f\"üîÑ Carregando modelo BERT localmente: {model_name}\")\n",
        "    print(\"   üíª Processamento local (sem internet necess√°ria)\")\n",
        "    \n",
        "    # Carregar modelo\n",
        "    model = SentenceTransformer(model_name)\n",
        "    \n",
        "    # Gerar embeddings\n",
        "    print(\"üîÑ Gerando embeddings BERT localmente...\")\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Embeddings BERT gerados localmente: {embeddings.shape}\")\n",
        "    return embeddings\n",
        "\n",
        "def generate_sbert_embeddings(texts, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"Gera embeddings usando Sentence-BERT (processamento local)\"\"\"\n",
        "    print(f\"üîÑ Carregando modelo Sentence-BERT localmente: {model_name}\")\n",
        "    print(\"   üíª Processamento local (sem internet necess√°ria)\")\n",
        "    \n",
        "    # Carregar modelo\n",
        "    model = SentenceTransformer(model_name)\n",
        "    \n",
        "    # Gerar embeddings\n",
        "    print(\"üîÑ Gerando embeddings Sentence-BERT localmente...\")\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Embeddings Sentence-BERT gerados localmente: {embeddings.shape}\")\n",
        "    return embeddings\n",
        "\n",
        "# Gerar embeddings modernos locais\n",
        "print(\"üöÄ GERANDO EMBEDDINGS MODERNOS LOCAIS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"üíª Processamento local (sem internet necess√°ria)\")\n",
        "\n",
        "# Verificar se deve usar cache\n",
        "use_cache = CACHE_AVAILABLE and 'cache_connected' in locals() and cache_connected\n",
        "force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
        "\n",
        "# Sentence-BERT (mais r√°pido e eficiente)\n",
        "print(\"\\nüìä Gerando embeddings Sentence-BERT...\")\n",
        "\n",
        "if use_cache and not force_regenerate:\n",
        "    # Verificar cache Sentence-BERT\n",
        "    all_exist_sbert, existing_sbert, missing_sbert = check_embeddings_in_cache('embeddings_sbert', doc_ids)\n",
        "    \n",
        "    if all_exist_sbert:\n",
        "        print(\"‚úÖ Todos os embeddings Sentence-BERT j√° existem no cache\")\n",
        "        print(\"üì• Carregando Sentence-BERT do cache...\")\n",
        "        sbert_embeddings = load_embeddings_from_cache('embeddings_sbert', doc_ids)\n",
        "        if sbert_embeddings is None:\n",
        "            print(\"‚ùå Falha ao carregar Sentence-BERT do cache, regenerando...\")\n",
        "            sbert_embeddings = generate_sbert_embeddings(df['text'].tolist())\n",
        "            save_embeddings_to_cache('embeddings_sbert', sbert_embeddings, doc_ids, df['text'].tolist(), 'sbert')\n",
        "    else:\n",
        "        print(f\"üîÑ {len(missing_sbert)} embeddings Sentence-BERT faltando, gerando...\")\n",
        "        sbert_embeddings = generate_sbert_embeddings(df['text'].tolist())\n",
        "        save_embeddings_to_cache('embeddings_sbert', sbert_embeddings, doc_ids, df['text'].tolist(), 'sbert')\n",
        "else:\n",
        "    sbert_embeddings = generate_sbert_embeddings(df['text'].tolist())\n",
        "    if use_cache:\n",
        "        save_embeddings_to_cache('embeddings_sbert', sbert_embeddings, doc_ids, df['text'].tolist(), 'sbert')\n",
        "\n",
        "# BERT (se quiser comparar)\n",
        "print(\"\\nüìä Gerando embeddings BERT...\")\n",
        "\n",
        "if use_cache and not force_regenerate:\n",
        "    # Verificar cache BERT\n",
        "    all_exist_bert, existing_bert, missing_bert = check_embeddings_in_cache('embeddings_bert', doc_ids)\n",
        "    \n",
        "    if all_exist_bert:\n",
        "        print(\"‚úÖ Todos os embeddings BERT j√° existem no cache\")\n",
        "        print(\"üì• Carregando BERT do cache...\")\n",
        "        bert_embeddings = load_embeddings_from_cache('embeddings_bert', doc_ids)\n",
        "        if bert_embeddings is None:\n",
        "            print(\"‚ùå Falha ao carregar BERT do cache, regenerando...\")\n",
        "            bert_embeddings = generate_bert_embeddings(df['text'].tolist())\n",
        "            save_embeddings_to_cache('embeddings_bert', bert_embeddings, doc_ids, df['text'].tolist(), 'bert')\n",
        "    else:\n",
        "        print(f\"üîÑ {len(missing_bert)} embeddings BERT faltando, gerando...\")\n",
        "        bert_embeddings = generate_bert_embeddings(df['text'].tolist())\n",
        "        save_embeddings_to_cache('embeddings_bert', bert_embeddings, doc_ids, df['text'].tolist(), 'bert')\n",
        "else:\n",
        "    bert_embeddings = generate_bert_embeddings(df['text'].tolist())\n",
        "    if use_cache:\n",
        "        save_embeddings_to_cache('embeddings_bert', bert_embeddings, doc_ids, df['text'].tolist(), 'bert')\n",
        "\n",
        "print(f\"\\nüìã RESUMO DOS EMBEDDINGS MODERNOS LOCAIS:\")\n",
        "print(f\"   BERT: {bert_embeddings.shape}\")\n",
        "print(f\"   Sentence-BERT: {sbert_embeddings.shape}\")\n",
        "\n",
        "if use_cache:\n",
        "    print(f\"\\nüíæ Embeddings modernos salvos no cache Elasticsearch!\")\n",
        "    print(f\"‚è±Ô∏è  Pr√≥xima execu√ß√£o ser√° muito mais r√°pida (5s vs 2min)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê GERANDO EMBEDDINGS OPENAI\n",
            "==================================================\n",
            "‚ö†Ô∏è  ATEN√á√ÉO: Usando API externa (requer internet e chave v√°lida)\n",
            "\n",
            "üìä Gerando embeddings OpenAI...\n",
            "‚úÖ Todos os embeddings OpenAI j√° existem no cache\n",
            "üì• Carregando OpenAI do cache...\n",
            "üí∞ Economia estimada: ~$0.50 (sem chamadas API)\n",
            "‚è±Ô∏è  Tempo economizado: ~30 minutos\n",
            "‚úÖ Embeddings carregados: (9085, 1536) de 'embeddings_openai'\n",
            "\n",
            "üìã RESUMO DOS EMBEDDINGS OPENAI:\n",
            "   OpenAI: (9085, 1536)\n",
            "üíæ Embeddings OpenAI salvos no cache Elasticsearch!\n",
            "üí∞ Pr√≥xima execu√ß√£o economizar√° ~$0.50 e ~30 minutos\n"
          ]
        }
      ],
      "source": [
        "# üåê Gera√ß√£o de Embeddings OpenAI (API Externa) com Cache Inteligente\n",
        "def generate_openai_embeddings(texts, model_name='text-embedding-3-small'):\n",
        "    \"\"\"Gera embeddings usando OpenAI API com controle ultra robusto de tokens (via internet)\"\"\"\n",
        "    # Verificar se a chave da OpenAI est√° configurada (verifica√ß√£o mais robusta)\n",
        "    api_key = os.getenv('OPENAI_API_KEY')\n",
        "    if not api_key or api_key == 'sk-your-openai-key-here':\n",
        "        print(\"‚ùå Chave da OpenAI n√£o configurada\")\n",
        "        print(\"üí° Configure OPENAI_API_KEY no arquivo .env\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üåê Gerando embeddings OpenAI via API: {model_name}\")\n",
        "    print(\"   ‚ö†Ô∏è  ATEN√á√ÉO: Usando API externa (requer internet e chave v√°lida)\")\n",
        "    \n",
        "    # Configurar OpenAI usando API v1.x\n",
        "    try:\n",
        "        # Criar cliente OpenAI\n",
        "        client = openai.OpenAI(api_key=api_key)\n",
        "        \n",
        "        # Limites do modelo (text-embedding-3-small)\n",
        "        MAX_TOKENS = 8192  # Limite de tokens do modelo\n",
        "        CHARS_PER_TOKEN = 4  # Aproxima√ß√£o: 1 token ‚âà 4 caracteres\n",
        "        MAX_CHARS_PER_TEXT = MAX_TOKENS * CHARS_PER_TOKEN  # ~32,768 caracteres por texto\n",
        "        \n",
        "        # An√°lise detalhada dos textos\n",
        "        text_lengths = [len(text) for text in texts]\n",
        "        avg_text_length = np.mean(text_lengths)\n",
        "        max_text_length = np.max(text_lengths)\n",
        "        min_text_length = np.min(text_lengths)\n",
        "        \n",
        "        print(f\"   üìä An√°lise dos textos:\")\n",
        "        print(f\"      üìè Tamanho m√©dio: {avg_text_length:.0f} caracteres\")\n",
        "        print(f\"      üìè Tamanho m√°ximo: {max_text_length:,} caracteres\")\n",
        "        print(f\"      üìè Tamanho m√≠nimo: {min_text_length:,} caracteres\")\n",
        "        print(f\"      üî¢ Total de textos: {len(texts):,}\")\n",
        "        \n",
        "        # Verificar se h√° textos muito longos\n",
        "        long_texts = [i for i, length in enumerate(text_lengths) if length > MAX_CHARS_PER_TEXT]\n",
        "        if long_texts:\n",
        "            print(f\"   ‚ö†Ô∏è  {len(long_texts)} textos muito longos (> {MAX_CHARS_PER_TEXT:,} chars)\")\n",
        "            print(f\"   üí° Estes textos ser√£o processados individualmente com estrat√©gia especial\")\n",
        "        \n",
        "        # Estrat√©gia de processamento ultra robusta\n",
        "        all_embeddings = []\n",
        "        processed_count = 0\n",
        "        error_count = 0\n",
        "        individual_count = 0\n",
        "        batch_count = 0\n",
        "        \n",
        "        # Processar textos individualmente para m√°xima robustez\n",
        "        print(f\"   üîÑ Processando textos individualmente para m√°xima robustez...\")\n",
        "        \n",
        "        for i, text in enumerate(texts):\n",
        "            try:\n",
        "                # Verificar se texto √© muito longo\n",
        "                if len(text) > MAX_CHARS_PER_TEXT:\n",
        "                    print(f\"   ‚ö†Ô∏è  Texto {i+1} muito longo ({len(text):,} chars), usando estrat√©gia especial...\")\n",
        "                    \n",
        "                    # Estrat√©gia para textos muito longos: dividir em chunks com c√°lculo din√¢mico\n",
        "                    chunks = []\n",
        "                    # C√°lculo mais conservador: usar apenas 70% do limite para margem de seguran√ßa\n",
        "                    safe_chunk_size = int(MAX_CHARS_PER_TEXT * 0.7)  # ~22,937 chars\n",
        "                    \n",
        "                    for j in range(0, len(text), safe_chunk_size):\n",
        "                        chunk = text[j:j+safe_chunk_size]\n",
        "                        chunks.append(chunk)\n",
        "                    \n",
        "                    # Processar cada chunk\n",
        "                    chunk_embeddings = []\n",
        "                    for chunk_idx, chunk in enumerate(chunks):\n",
        "                        try:\n",
        "                            response = client.embeddings.create(model=model_name, input=[chunk])\n",
        "                            chunk_embeddings.append(response.data[0].embedding)\n",
        "                        except Exception as chunk_error:\n",
        "                            print(f\"   ‚ùå Erro no chunk {chunk_idx+1} do texto {i+1}: {chunk_error}\")\n",
        "                            # Usar embedding de fallback (zeros)\n",
        "                            chunk_embeddings.append([0.0] * 1536)\n",
        "                    \n",
        "                    # M√©dia dos embeddings dos chunks (estrat√©gia robusta)\n",
        "                    if chunk_embeddings:\n",
        "                        final_embedding = np.mean(chunk_embeddings, axis=0).tolist()\n",
        "                    else:\n",
        "                        # Fallback: embedding de zeros\n",
        "                        final_embedding = [0.0] * 1536\n",
        "                    \n",
        "                    all_embeddings.append(final_embedding)\n",
        "                    individual_count += 1\n",
        "                    \n",
        "                else:\n",
        "                    # Texto normal: processar diretamente\n",
        "                    response = client.embeddings.create(model=model_name, input=[text])\n",
        "                    all_embeddings.append(response.data[0].embedding)\n",
        "                    individual_count += 1\n",
        "                \n",
        "                processed_count += 1\n",
        "                \n",
        "                # Mostrar progresso detalhado\n",
        "                if processed_count % 50 == 0 or processed_count == len(texts):\n",
        "                    progress_pct = (processed_count / len(texts)) * 100\n",
        "                    print(f\"   üì° Progresso: {processed_count:,}/{len(texts):,} ({progress_pct:.1f}%)\")\n",
        "                \n",
        "                # Pequena pausa para evitar rate limiting\n",
        "                if processed_count % 100 == 0:\n",
        "                    import time\n",
        "                    time.sleep(0.1)\n",
        "                    \n",
        "            except Exception as text_error:\n",
        "                print(f\"   ‚ùå Erro no texto {i+1}: {text_error}\")\n",
        "                error_count += 1\n",
        "                \n",
        "                # Fallback: embedding de zeros para manter consist√™ncia\n",
        "                all_embeddings.append([0.0] * 1536)\n",
        "                processed_count += 1\n",
        "                \n",
        "                # Se muitos erros, pausar mais\n",
        "                if error_count > 10:\n",
        "                    print(f\"   ‚ö†Ô∏è  Muitos erros ({error_count}), pausando por 2 segundos...\")\n",
        "                    import time\n",
        "                    time.sleep(2)\n",
        "        \n",
        "        # Converter para numpy array\n",
        "        embeddings = np.array(all_embeddings)\n",
        "        \n",
        "        # Estat√≠sticas finais\n",
        "        print(f\"\\n   üìä ESTAT√çSTICAS FINAIS:\")\n",
        "        print(f\"      ‚úÖ Textos processados: {processed_count:,}/{len(texts):,}\")\n",
        "        print(f\"      üîÑ Processamento individual: {individual_count:,}\")\n",
        "        print(f\"      ‚ùå Erros: {error_count:,}\")\n",
        "        print(f\"      üìê Dimens√µes finais: {embeddings.shape}\")\n",
        "        print(f\"      üíæ Tamanho em mem√≥ria: {embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
        "        \n",
        "        if error_count > 0:\n",
        "            print(f\"   ‚ö†Ô∏è  {error_count} textos tiveram erros e receberam embeddings de fallback\")\n",
        "            print(f\"   üí° Para reprocessar apenas os textos com erro, execute novamente a c√©lula\")\n",
        "            print(f\"   üîÑ O sistema detectar√° automaticamente os textos faltantes\")\n",
        "        \n",
        "        print(f\"‚úÖ Embeddings OpenAI gerados via API: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro cr√≠tico ao gerar embeddings OpenAI: {e}\")\n",
        "        print(\"üí° Dica: Verifique se a chave da OpenAI est√° configurada corretamente\")\n",
        "        return None\n",
        "\n",
        "# Gerar embeddings OpenAI\n",
        "print(\"üåê GERANDO EMBEDDINGS OPENAI\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚ö†Ô∏è  ATEN√á√ÉO: Usando API externa (requer internet e chave v√°lida)\")\n",
        "\n",
        "# Verificar se deve usar cache\n",
        "use_cache = CACHE_AVAILABLE and 'cache_connected' in locals() and cache_connected\n",
        "force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
        "\n",
        "# OpenAI (se dispon√≠vel)\n",
        "print(\"\\nüìä Gerando embeddings OpenAI...\")\n",
        "\n",
        "if use_cache and not force_regenerate:\n",
        "    # Verificar cache OpenAI\n",
        "    all_exist_openai, existing_openai, missing_openai = check_embeddings_in_cache('embeddings_openai', doc_ids)\n",
        "    \n",
        "    if all_exist_openai:\n",
        "        print(\"‚úÖ Todos os embeddings OpenAI j√° existem no cache\")\n",
        "        print(\"üì• Carregando OpenAI do cache...\")\n",
        "        print(\"üí∞ Economia estimada: ~$0.50 (sem chamadas API)\")\n",
        "        print(\"‚è±Ô∏è  Tempo economizado: ~30 minutos\")\n",
        "        openai_embeddings = load_embeddings_from_cache('embeddings_openai', doc_ids)\n",
        "        if openai_embeddings is None:\n",
        "            print(\"‚ùå Falha ao carregar OpenAI do cache, gerando via API...\")\n",
        "            openai_embeddings = generate_openai_embeddings(df['text'].tolist())\n",
        "            if openai_embeddings is not None:\n",
        "                save_embeddings_to_cache('embeddings_openai', openai_embeddings, doc_ids, df['text'].tolist(), 'openai')\n",
        "    else:\n",
        "        print(f\"üîÑ {len(missing_openai)} embeddings OpenAI faltando, gerando via API...\")\n",
        "        print(\"üí∞ Custo estimado: ~$0.50 (chamadas API)\")\n",
        "        print(\"‚è±Ô∏è  Tempo estimado: ~30 minutos\")\n",
        "        openai_embeddings = generate_openai_embeddings(df['text'].tolist())\n",
        "        if openai_embeddings is not None:\n",
        "            save_embeddings_to_cache('embeddings_openai', openai_embeddings, doc_ids, df['text'].tolist(), 'openai')\n",
        "else:\n",
        "    openai_embeddings = generate_openai_embeddings(df['text'].tolist())\n",
        "    if openai_embeddings is not None and use_cache:\n",
        "        save_embeddings_to_cache('embeddings_openai', openai_embeddings, doc_ids, df['text'].tolist(), 'openai')\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    print(f\"\\nüìã RESUMO DOS EMBEDDINGS OPENAI:\")\n",
        "    print(f\"   OpenAI: {openai_embeddings.shape}\")\n",
        "    if use_cache:\n",
        "        print(f\"üíæ Embeddings OpenAI salvos no cache Elasticsearch!\")\n",
        "        print(f\"üí∞ Pr√≥xima execu√ß√£o economizar√° ~$0.50 e ~30 minutos\")\n",
        "else:\n",
        "    print(f\"\\nüìã RESUMO DOS EMBEDDINGS OPENAI:\")\n",
        "    print(f\"   OpenAI: N√£o dispon√≠vel\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä An√°lise Detalhada e Comparativa dos Embeddings\n",
        "\n",
        "### **Por que Analisar Cada Embedding?**\n",
        "Cada tipo de embedding tem caracter√≠sticas √∫nicas que impactam o clustering:\n",
        "- **Dimensionalidade**: Afeta velocidade e qualidade\n",
        "- **Densidade**: Embeddings esparsos vs densos\n",
        "- **Normaliza√ß√£o**: Importante para m√©tricas de dist√¢ncia\n",
        "- **Caracter√≠sticas espec√≠ficas**: TF-IDF (frequ√™ncia), Word2Vec (contexto), BERT (bidirecional)\n",
        "\n",
        "### **O que Analisaremos?**\n",
        "1. **Estat√≠sticas b√°sicas**: Dimens√µes, mem√≥ria, valores\n",
        "2. **Densidade**: Qu√£o esparso √© o embedding\n",
        "3. **Normaliza√ß√£o**: Se os vetores est√£o normalizados\n",
        "4. **Exemplos reais**: Visualizar embeddings individuais\n",
        "5. **Compara√ß√£o**: Qual funciona melhor para clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† AN√ÅLISE DETALHADA DOS EMBEDDINGS\n",
            "============================================================\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: TF-IDF\n",
            "============================================================\n",
            "üìê Dimens√µes: (9085, 4096)\n",
            "üíæ Mem√≥ria: 283.9 MB\n",
            "üìà Valores m√©dios: 0.0012\n",
            "üìä Desvio padr√£o: 0.0156\n",
            "üìâ Valor m√≠nimo: -0.0417\n",
            "üìà Valor m√°ximo: 1.0000\n",
            "üìä Densidade: 0.012 (1.0 = denso, 0.0 = esparso)\n",
            "üìè Norma m√©dia: 0.9997\n",
            "üìè Norma std: 0.0107\n",
            "‚úÖ Embeddings normalizados\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   ... (total: 4096 dimens√µes)\n",
            "\n",
            "üìä AN√ÅLISE ESPEC√çFICA TF-IDF:\n",
            "   Vocabul√°rio: 4096 palavras\n",
            "   Features mais importantes:\n",
            "      1. don: 0.0180\n",
            "      2. just: 0.0178\n",
            "      3. like: 0.0178\n",
            "      4. know: 0.0176\n",
            "      5. think: 0.0158\n",
            "      6. people: 0.0158\n",
            "      7. does: 0.0148\n",
            "      8. god: 0.0136\n",
            "      9. good: 0.0131\n",
            "      10. time: 0.0128\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: Word2Vec\n",
            "============================================================\n",
            "üìê Dimens√µes: (9085, 100)\n",
            "üíæ Mem√≥ria: 6.9 MB\n",
            "üìà Valores m√©dios: 0.0090\n",
            "üìä Desvio padr√£o: 0.1650\n",
            "üìâ Valor m√≠nimo: -0.8833\n",
            "üìà Valor m√°ximo: 1.0701\n",
            "üìä Densidade: 1.000 (1.0 = denso, 0.0 = esparso)\n",
            "üìè Norma m√©dia: 1.6469\n",
            "üìè Norma std: 0.1415\n",
            "‚ö†Ô∏è  Embeddings n√£o normalizados\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [-0.16675173  0.09640667  0.07933187  0.01350319  0.1043881  -0.34946343\n",
            "  0.0730726   0.36428133 -0.10279048 -0.17662691 -0.17007129 -0.26140347\n",
            " -0.08200631  0.13524441  0.11330047  0.00693159  0.11543401 -0.05326838\n",
            " -0.00632868 -0.18029886]\n",
            "   ... (total: 100 dimens√µes)\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: BERT\n",
            "============================================================\n",
            "üìê Dimens√µes: (9085, 768)\n",
            "üíæ Mem√≥ria: 53.2 MB\n",
            "üìà Valores m√©dios: -0.0015\n",
            "üìä Desvio padr√£o: 0.0361\n",
            "üìâ Valor m√≠nimo: -0.5977\n",
            "üìà Valor m√°ximo: 0.1675\n",
            "üìä Densidade: 1.000 (1.0 = denso, 0.0 = esparso)\n",
            "üìè Norma m√©dia: 1.0000\n",
            "üìè Norma std: 0.0000\n",
            "‚úÖ Embeddings normalizados\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [-0.00132143 -0.02328889  0.07774121  0.01896514  0.03875779 -0.01545336\n",
            " -0.00867832  0.07599301  0.02009529 -0.03681596  0.01122527 -0.01863559\n",
            " -0.02169822  0.03908618 -0.01034616  0.03337554 -0.00158272  0.03089168\n",
            " -0.0148504   0.04084001]\n",
            "   ... (total: 768 dimens√µes)\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: Sentence-BERT\n",
            "============================================================\n",
            "üìê Dimens√µes: (9085, 384)\n",
            "üíæ Mem√≥ria: 26.6 MB\n",
            "üìà Valores m√©dios: -0.0004\n",
            "üìä Desvio padr√£o: 0.0510\n",
            "üìâ Valor m√≠nimo: -0.2516\n",
            "üìà Valor m√°ximo: 0.2628\n",
            "üìä Densidade: 1.000 (1.0 = denso, 0.0 = esparso)\n",
            "üìè Norma m√©dia: 1.0000\n",
            "üìè Norma std: 0.0000\n",
            "‚úÖ Embeddings normalizados\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [ 0.03204788 -0.02588454 -0.00725068  0.00876473 -0.04122781  0.00462309\n",
            " -0.08446515  0.01132435 -0.12879238 -0.03445601 -0.03699411 -0.01095315\n",
            " -0.0206078  -0.05314306 -0.02424822  0.04395417 -0.02956954  0.04954473\n",
            "  0.02622971 -0.03960365]\n",
            "   ... (total: 384 dimens√µes)\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: OpenAI\n",
            "============================================================\n",
            "üìê Dimens√µes: (9085, 1536)\n",
            "üíæ Mem√≥ria: 106.5 MB\n",
            "üìà Valores m√©dios: -0.0003\n",
            "üìä Desvio padr√£o: 0.0255\n",
            "üìâ Valor m√≠nimo: -0.2014\n",
            "üìà Valor m√°ximo: 0.1754\n",
            "üìä Densidade: 1.000 (1.0 = denso, 0.0 = esparso)\n",
            "üìè Norma m√©dia: 0.9992\n",
            "üìè Norma std: 0.0190\n",
            "‚úÖ Embeddings normalizados\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [ 0.00597357 -0.00375218  0.02465809 -0.03065394 -0.00513975 -0.01811485\n",
            "  0.03108676  0.01975702 -0.05127661  0.02125917  0.04137264  0.0061868\n",
            " -0.03773184  0.0560631   0.01997344  0.00975758  0.00673419 -0.0227995\n",
            " -0.06446493  0.05804899]\n",
            "   ... (total: 1536 dimens√µes)\n",
            "\n",
            "üìã RESUMO COMPARATIVO DOS EMBEDDINGS:\n",
            "============================================================\n",
            "Tipo            Dimens√µes    Mem√≥ria  Densidade  Normalizado \n",
            "------------------------------------------------------------\n",
            "TF-IDF          (9085, 4096) 283.9    0.012      Sim         \n",
            "Word2Vec        (9085, 100)  6.9      1.000      N√£o         \n",
            "BERT            (9085, 768)  53.2     1.000      Sim         \n",
            "Sentence-BERT   (9085, 384)  26.6     1.000      Sim         \n",
            "OpenAI          (9085, 1536) 106.5    1.000      Sim         \n",
            "\n",
            "‚úÖ An√°lise detalhada conclu√≠da!\n"
          ]
        }
      ],
      "source": [
        "# üß† An√°lise Detalhada dos Embeddings\n",
        "def analyze_embedding_detailed(name, embeddings, vectorizer=None):\n",
        "    \"\"\"An√°lise detalhada de um embedding espec√≠fico\"\"\"\n",
        "    print(f\"\\nüîç AN√ÅLISE DETALHADA: {name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Estat√≠sticas b√°sicas\n",
        "    print(f\"üìê Dimens√µes: {embeddings.shape}\")\n",
        "    print(f\"üíæ Mem√≥ria: {embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
        "    print(f\"üìà Valores m√©dios: {embeddings.mean():.4f}\")\n",
        "    print(f\"üìä Desvio padr√£o: {embeddings.std():.4f}\")\n",
        "    print(f\"üìâ Valor m√≠nimo: {embeddings.min():.4f}\")\n",
        "    print(f\"üìà Valor m√°ximo: {embeddings.max():.4f}\")\n",
        "    \n",
        "    # An√°lise de densidade\n",
        "    density = (embeddings != 0).mean()\n",
        "    print(f\"üìä Densidade: {density:.3f} (1.0 = denso, 0.0 = esparso)\")\n",
        "    \n",
        "    # An√°lise de normaliza√ß√£o\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "    print(f\"üìè Norma m√©dia: {norms.mean():.4f}\")\n",
        "    print(f\"üìè Norma std: {norms.std():.4f}\")\n",
        "    \n",
        "    if abs(norms.mean() - 1.0) < 0.1:\n",
        "        print(f\"‚úÖ Embeddings normalizados\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Embeddings n√£o normalizados\")\n",
        "    \n",
        "    # Mostrar exemplo do embedding\n",
        "    print(f\"\\nüìù EXEMPLO DO EMBEDDING:\")\n",
        "    if embeddings.shape[1] <= 20:\n",
        "        # Mostrar embedding completo se for pequeno\n",
        "        print(f\"   Embedding completo: {embeddings[0]}\")\n",
        "    else:\n",
        "        # Mostrar amostra se for grande\n",
        "        sample_size = min(20, embeddings.shape[1])\n",
        "        print(f\"   Amostra (primeiros {sample_size} valores): {embeddings[0][:sample_size]}\")\n",
        "        print(f\"   ... (total: {embeddings.shape[1]} dimens√µes)\")\n",
        "    \n",
        "    # An√°lise espec√≠fica por tipo\n",
        "    if name == 'TF-IDF' and vectorizer is not None:\n",
        "        print(f\"\\nüìä AN√ÅLISE ESPEC√çFICA TF-IDF:\")\n",
        "        print(f\"   Vocabul√°rio: {len(vectorizer.vocabulary_)} palavras\")\n",
        "        print(f\"   Features mais importantes:\")\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        # Mostrar features com maior TF-IDF m√©dio\n",
        "        mean_tfidf = embeddings.mean(axis=0)\n",
        "        top_features = np.argsort(mean_tfidf)[-10:][::-1]\n",
        "        for i, idx in enumerate(top_features):\n",
        "            print(f\"      {i+1}. {feature_names[idx]}: {mean_tfidf[idx]:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'density': density,\n",
        "        'normalized': abs(norms.mean() - 1.0) < 0.1,\n",
        "        'mean_norm': norms.mean(),\n",
        "        'std_norm': norms.std()\n",
        "    }\n",
        "\n",
        "# Executar an√°lise detalhada\n",
        "print(\"üß† AN√ÅLISE DETALHADA DOS EMBEDDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Preparar dados para an√°lise\n",
        "embeddings_dict = {\n",
        "    'TF-IDF': tfidf_embeddings,\n",
        "    'Word2Vec': word2vec_embeddings,\n",
        "    'BERT': bert_embeddings,\n",
        "    'Sentence-BERT': sbert_embeddings\n",
        "}\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    embeddings_dict['OpenAI'] = openai_embeddings\n",
        "\n",
        "# Analisar cada embedding\n",
        "analysis_results = {}\n",
        "for name, embeddings in embeddings_dict.items():\n",
        "    if name == 'TF-IDF':\n",
        "        analysis_results[name] = analyze_embedding_detailed(name, embeddings, tfidf_vectorizer)\n",
        "    else:\n",
        "        analysis_results[name] = analyze_embedding_detailed(name, embeddings)\n",
        "\n",
        "print(f\"\\nüìã RESUMO COMPARATIVO DOS EMBEDDINGS:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Tipo':<15} {'Dimens√µes':<12} {'Mem√≥ria':<8} {'Densidade':<10} {'Normalizado':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for name, embeddings in embeddings_dict.items():\n",
        "    memory_mb = embeddings.nbytes / 1024 / 1024\n",
        "    density = analysis_results[name]['density']\n",
        "    normalized = \"Sim\" if analysis_results[name]['normalized'] else \"N√£o\"\n",
        "    print(f\"{name:<15} {str(embeddings.shape):<12} {memory_mb:<8.1f} {density:<10.3f} {normalized:<12}\")\n",
        "\n",
        "print(f\"\\n‚úÖ An√°lise detalhada conclu√≠da!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä An√°lise Detalhada e Comparativa dos Embeddings\n",
        "\n",
        "### **Por que Analisar Cada Embedding?**\n",
        "Cada tipo de embedding tem caracter√≠sticas √∫nicas que impactam o clustering:\n",
        "- **Dimensionalidade**: Afeta velocidade e qualidade\n",
        "- **Densidade**: Embeddings esparsos vs densos\n",
        "- **Normaliza√ß√£o**: Importante para m√©tricas de dist√¢ncia\n",
        "- **Caracter√≠sticas espec√≠ficas**: TF-IDF (frequ√™ncia), Word2Vec (contexto), BERT (bidirecional)\n",
        "\n",
        "### **O que Analisaremos?**\n",
        "1. **Estat√≠sticas b√°sicas**: Dimens√µes, mem√≥ria, valores\n",
        "2. **Densidade**: Qu√£o esparso √© o embedding\n",
        "3. **Normaliza√ß√£o**: Se os vetores est√£o normalizados\n",
        "4. **Exemplos reais**: Visualizar embeddings individuais\n",
        "5. **Compara√ß√£o**: Qual funciona melhor para clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† AN√ÅLISE DETALHADA E COMPARATIVA DOS EMBEDDINGS\n",
            "======================================================================\n",
            "üìö Esta an√°lise examina cada tipo de embedding em detalhes\n",
            "üéØ Objetivo: Entender caracter√≠sticas e escolher o melhor para clustering\n",
            "======================================================================\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: Word2Vec\n",
            "============================================================\n",
            "üìê DIMENS√ïES E ESTRUTURA:\n",
            "   üìä Forma: (9085, 100)\n",
            "   üìä Documentos: 9,085\n",
            "   üìä Dimens√µes: 100\n",
            "   üíæ Mem√≥ria: 6.9 MB\n",
            "\n",
            "üìà AN√ÅLISE DE VALORES:\n",
            "   üìä M√©dia: 0.0090\n",
            "   üìä Mediana: 0.0113\n",
            "   üìä Desvio padr√£o: 0.1650\n",
            "   üìâ M√≠nimo: -0.8833\n",
            "   üìà M√°ximo: 1.0701\n",
            "\n",
            "üìä AN√ÅLISE DE DENSIDADE:\n",
            "   üìä Densidade: 1.000\n",
            "   ‚úÖ Embedding denso (mais de 80% dos valores s√£o n√£o-zero)\n",
            "\n",
            "üìè AN√ÅLISE DE NORMALIZA√á√ÉO:\n",
            "   üìä Norma m√©dia: 1.6469\n",
            "   üìä Norma desvio padr√£o: 0.1415\n",
            "   üìä Norma m√≠nima: 0.0949\n",
            "   üìä Norma m√°xima: 3.3865\n",
            "   ‚ö†Ô∏è  Embeddings n√£o normalizados (norma ‚â† 1.0)\n",
            "   üí° Considere normalizar para clustering\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [-0.16675173  0.09640667  0.07933187  0.01350319  0.1043881  -0.34946343\n",
            "  0.0730726   0.36428133 -0.10279048 -0.17662691 -0.17007129 -0.26140347\n",
            " -0.08200631  0.13524441  0.11330047  0.00693159  0.11543401 -0.05326838\n",
            " -0.00632868 -0.18029886]\n",
            "   ... (total: 100 dimens√µes)\n",
            "   üí° Este √© apenas um exemplo - cada documento tem 100 dimens√µes\n",
            "\n",
            "üî¨ AN√ÅLISE ESPEC√çFICA - Word2Vec:\n",
            "   üìä M√âTODO: Skip-gram/CBOW com janela deslizante\n",
            "   üìä CARACTER√çSTICAS:\n",
            "      ‚Ä¢ Aprende contexto de palavras\n",
            "      ‚Ä¢ Palavras similares ficam pr√≥ximas\n",
            "      ‚Ä¢ Dimens√µes fixas (configur√°vel)\n",
            "      ‚Ä¢ Captura similaridade sem√¢ntica\n",
            "   üìä TREINAMENTO: Modelo treinado em 9,085 documentos\n",
            "   üìä DIMENS√ïES: 100 (configur√°vel, geralmente 100-300)\n",
            "\n",
            "üí° INTERPRETA√á√ÉO DID√ÅTICA:\n",
            "   üéØ Para clustering, este embedding √©:\n",
            "      ‚Ä¢ Denso: bom para algoritmos que precisam de dados densos\n",
            "      ‚Ä¢ N√£o normalizado: considere normalizar antes do clustering\n",
            "      ‚Ä¢ Dimens√µes: 100 - baixa dimensionalidade\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: BERT\n",
            "============================================================\n",
            "üìê DIMENS√ïES E ESTRUTURA:\n",
            "   üìä Forma: (9085, 768)\n",
            "   üìä Documentos: 9,085\n",
            "   üìä Dimens√µes: 768\n",
            "   üíæ Mem√≥ria: 53.2 MB\n",
            "\n",
            "üìà AN√ÅLISE DE VALORES:\n",
            "   üìä M√©dia: -0.0015\n",
            "   üìä Mediana: -0.0015\n",
            "   üìä Desvio padr√£o: 0.0361\n",
            "   üìâ M√≠nimo: -0.5977\n",
            "   üìà M√°ximo: 0.1675\n",
            "\n",
            "üìä AN√ÅLISE DE DENSIDADE:\n",
            "   üìä Densidade: 1.000\n",
            "   ‚úÖ Embedding denso (mais de 80% dos valores s√£o n√£o-zero)\n",
            "\n",
            "üìè AN√ÅLISE DE NORMALIZA√á√ÉO:\n",
            "   üìä Norma m√©dia: 1.0000\n",
            "   üìä Norma desvio padr√£o: 0.0000\n",
            "   üìä Norma m√≠nima: 1.0000\n",
            "   üìä Norma m√°xima: 1.0000\n",
            "   ‚úÖ Embeddings normalizados (norma ‚âà 1.0)\n",
            "   üí° Normaliza√ß√£o √© importante para m√©tricas de dist√¢ncia\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [-0.00132143 -0.02328889  0.07774121  0.01896514  0.03875779 -0.01545336\n",
            " -0.00867832  0.07599301  0.02009529 -0.03681596  0.01122527 -0.01863559\n",
            " -0.02169822  0.03908618 -0.01034616  0.03337554 -0.00158272  0.03089168\n",
            " -0.0148504   0.04084001]\n",
            "   ... (total: 768 dimens√µes)\n",
            "   üí° Este √© apenas um exemplo - cada documento tem 768 dimens√µes\n",
            "\n",
            "üî¨ AN√ÅLISE ESPEC√çFICA - BERT:\n",
            "   üìä M√âTODO: Bidirectional Encoder Representations from Transformers\n",
            "   üìä CARACTER√çSTICAS:\n",
            "      ‚Ä¢ Bidirecional (l√™ em ambas as dire√ß√µes)\n",
            "      ‚Ä¢ Attention mechanism\n",
            "      ‚Ä¢ Contextualizado (mesma palavra, contextos diferentes)\n",
            "      ‚Ä¢ Dimens√µes: 768 (bert-base-uncased)\n",
            "   üìä PROCESSAMENTO: Local (sem internet)\n",
            "   üìä QUALIDADE: Alta para tarefas de NLP\n",
            "\n",
            "üí° INTERPRETA√á√ÉO DID√ÅTICA:\n",
            "   üéØ Para clustering, este embedding √©:\n",
            "      ‚Ä¢ Denso: bom para algoritmos que precisam de dados densos\n",
            "      ‚Ä¢ Normalizado: ideal para m√©tricas de dist√¢ncia (cosine, euclidiana)\n",
            "      ‚Ä¢ Dimens√µes: 768 - alta dimensionalidade\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: Sentence-BERT\n",
            "============================================================\n",
            "üìê DIMENS√ïES E ESTRUTURA:\n",
            "   üìä Forma: (9085, 384)\n",
            "   üìä Documentos: 9,085\n",
            "   üìä Dimens√µes: 384\n",
            "   üíæ Mem√≥ria: 26.6 MB\n",
            "\n",
            "üìà AN√ÅLISE DE VALORES:\n",
            "   üìä M√©dia: -0.0004\n",
            "   üìä Mediana: -0.0000\n",
            "   üìä Desvio padr√£o: 0.0510\n",
            "   üìâ M√≠nimo: -0.2516\n",
            "   üìà M√°ximo: 0.2628\n",
            "\n",
            "üìä AN√ÅLISE DE DENSIDADE:\n",
            "   üìä Densidade: 1.000\n",
            "   ‚úÖ Embedding denso (mais de 80% dos valores s√£o n√£o-zero)\n",
            "\n",
            "üìè AN√ÅLISE DE NORMALIZA√á√ÉO:\n",
            "   üìä Norma m√©dia: 1.0000\n",
            "   üìä Norma desvio padr√£o: 0.0000\n",
            "   üìä Norma m√≠nima: 1.0000\n",
            "   üìä Norma m√°xima: 1.0000\n",
            "   ‚úÖ Embeddings normalizados (norma ‚âà 1.0)\n",
            "   üí° Normaliza√ß√£o √© importante para m√©tricas de dist√¢ncia\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [ 0.03204788 -0.02588454 -0.00725068  0.00876473 -0.04122781  0.00462309\n",
            " -0.08446515  0.01132435 -0.12879238 -0.03445601 -0.03699411 -0.01095315\n",
            " -0.0206078  -0.05314306 -0.02424822  0.04395417 -0.02956954  0.04954473\n",
            "  0.02622971 -0.03960365]\n",
            "   ... (total: 384 dimens√µes)\n",
            "   üí° Este √© apenas um exemplo - cada documento tem 384 dimens√µes\n",
            "\n",
            "üî¨ AN√ÅLISE ESPEC√çFICA - Sentence-BERT:\n",
            "   üìä M√âTODO: Sentence-BERT (otimizado para similaridade)\n",
            "   üìä CARACTER√çSTICAS:\n",
            "      ‚Ä¢ Otimizado para similaridade de senten√ßas\n",
            "      ‚Ä¢ Ideal para clustering\n",
            "      ‚Ä¢ Dimens√µes: 384 (all-MiniLM-L6-v2)\n",
            "      ‚Ä¢ Normalizado por padr√£o\n",
            "   üìä PROCESSAMENTO: Local (sem internet)\n",
            "   üìä QUALIDADE: Excelente para clustering sem√¢ntico\n",
            "\n",
            "üí° INTERPRETA√á√ÉO DID√ÅTICA:\n",
            "   üéØ Para clustering, este embedding √©:\n",
            "      ‚Ä¢ Denso: bom para algoritmos que precisam de dados densos\n",
            "      ‚Ä¢ Normalizado: ideal para m√©tricas de dist√¢ncia (cosine, euclidiana)\n",
            "      ‚Ä¢ Dimens√µes: 384 - baixa dimensionalidade\n",
            "\n",
            "üîç AN√ÅLISE DETALHADA: OpenAI\n",
            "============================================================\n",
            "üìê DIMENS√ïES E ESTRUTURA:\n",
            "   üìä Forma: (9085, 1536)\n",
            "   üìä Documentos: 9,085\n",
            "   üìä Dimens√µes: 1,536\n",
            "   üíæ Mem√≥ria: 106.5 MB\n",
            "\n",
            "üìà AN√ÅLISE DE VALORES:\n",
            "   üìä M√©dia: -0.0003\n",
            "   üìä Mediana: -0.0003\n",
            "   üìä Desvio padr√£o: 0.0255\n",
            "   üìâ M√≠nimo: -0.2014\n",
            "   üìà M√°ximo: 0.1754\n",
            "\n",
            "üìä AN√ÅLISE DE DENSIDADE:\n",
            "   üìä Densidade: 1.000\n",
            "   ‚úÖ Embedding denso (mais de 80% dos valores s√£o n√£o-zero)\n",
            "\n",
            "üìè AN√ÅLISE DE NORMALIZA√á√ÉO:\n",
            "   üìä Norma m√©dia: 0.9992\n",
            "   üìä Norma desvio padr√£o: 0.0190\n",
            "   üìä Norma m√≠nima: 0.3829\n",
            "   üìä Norma m√°xima: 1.0000\n",
            "   ‚úÖ Embeddings normalizados (norma ‚âà 1.0)\n",
            "   üí° Normaliza√ß√£o √© importante para m√©tricas de dist√¢ncia\n",
            "\n",
            "üìù EXEMPLO DO EMBEDDING:\n",
            "   Amostra (primeiros 20 valores): [ 0.00597357 -0.00375218  0.02465809 -0.03065394 -0.00513975 -0.01811485\n",
            "  0.03108676  0.01975702 -0.05127661  0.02125917  0.04137264  0.0061868\n",
            " -0.03773184  0.0560631   0.01997344  0.00975758  0.00673419 -0.0227995\n",
            " -0.06446493  0.05804899]\n",
            "   ... (total: 1,536 dimens√µes)\n",
            "   üí° Este √© apenas um exemplo - cada documento tem 1,536 dimens√µes\n",
            "\n",
            "üî¨ AN√ÅLISE ESPEC√çFICA - OpenAI:\n",
            "   üìä M√âTODO: OpenAI Embeddings (text-embedding-3-small)\n",
            "   üìä CARACTER√çSTICAS:\n",
            "      ‚Ä¢ √öltima gera√ß√£o de embeddings\n",
            "      ‚Ä¢ Treinado especificamente para similaridade\n",
            "      ‚Ä¢ Dimens√µes: 1536 (text-embedding-3-small)\n",
            "      ‚Ä¢ Normalizado por padr√£o\n",
            "   üìä PROCESSAMENTO: API externa (requer internet)\n",
            "   üìä QUALIDADE: M√°xima qualidade em benchmarks\n",
            "\n",
            "üí° INTERPRETA√á√ÉO DID√ÅTICA:\n",
            "   üéØ Para clustering, este embedding √©:\n",
            "      ‚Ä¢ Denso: bom para algoritmos que precisam de dados densos\n",
            "      ‚Ä¢ Normalizado: ideal para m√©tricas de dist√¢ncia (cosine, euclidiana)\n",
            "      ‚Ä¢ Dimens√µes: 1,536 - alta dimensionalidade\n",
            "\n",
            "üîç COMPARA√á√ÉO ABRANGENTE DOS EMBEDDINGS\n",
            "======================================================================\n",
            "\n",
            "üìä TABELA COMPARATIVA:\n",
            "================================================================================\n",
            "         Tipo Dimens√µes Mem√≥ria (MB) Densidade Normalizado Norma M√©dia Processamento\n",
            "     Word2Vec       100          6.9     1.000         N√£o       1.647         Local\n",
            "         BERT       768         53.2     1.000         Sim       1.000         Local\n",
            "Sentence-BERT       384         26.6     1.000         Sim       1.000         Local\n",
            "       OpenAI     1,536        106.5     1.000         Sim       0.999   API Externa\n",
            "\n",
            "‚öñÔ∏è  AN√ÅLISE DE TRADE-OFFS:\n",
            "==================================================\n",
            "   üèÜ Melhor para clustering: BERT (score: 1.000)\n",
            "\n",
            "üí° RECOMENDA√á√ïES:\n",
            "==============================\n",
            "   üéØ Para clustering sem√¢ntico: Sentence-BERT\n",
            "   üéØ Para baseline r√°pido: Word2Vec\n",
            "   üéØ Para m√°xima qualidade: OpenAI (se or√ßamento permitir)\n",
            "   üéØ Para interpretabilidade: TF-IDF\n",
            "   üéØ Para processamento local: BERT ou Sentence-BERT\n",
            "\n",
            "‚úÖ An√°lise detalhada conclu√≠da!\n",
            "üìä 4 tipos de embeddings analisados\n",
            "üí° Use os resultados para escolher o melhor embedding para seu caso de uso\n"
          ]
        }
      ],
      "source": [
        "# üß† An√°lise Detalhada e Comparativa dos Embeddings\n",
        "def analyze_embedding_detailed(name, embeddings, vectorizer=None):\n",
        "    \"\"\"An√°lise detalhada de um embedding espec√≠fico com explica√ß√µes did√°ticas\"\"\"\n",
        "    print(f\"\\nüîç AN√ÅLISE DETALHADA: {name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. ESTAT√çSTICAS B√ÅSICAS\n",
        "    print(f\"üìê DIMENS√ïES E ESTRUTURA:\")\n",
        "    print(f\"   üìä Forma: {embeddings.shape}\")\n",
        "    print(f\"   üìä Documentos: {embeddings.shape[0]:,}\")\n",
        "    print(f\"   üìä Dimens√µes: {embeddings.shape[1]:,}\")\n",
        "    print(f\"   üíæ Mem√≥ria: {embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
        "    \n",
        "    # 2. AN√ÅLISE DE VALORES\n",
        "    print(f\"\\nüìà AN√ÅLISE DE VALORES:\")\n",
        "    print(f\"   üìä M√©dia: {embeddings.mean():.4f}\")\n",
        "    print(f\"   üìä Mediana: {np.median(embeddings):.4f}\")\n",
        "    print(f\"   üìä Desvio padr√£o: {embeddings.std():.4f}\")\n",
        "    print(f\"   üìâ M√≠nimo: {embeddings.min():.4f}\")\n",
        "    print(f\"   üìà M√°ximo: {embeddings.max():.4f}\")\n",
        "    \n",
        "    # 3. AN√ÅLISE DE DENSIDADE\n",
        "    density = (embeddings != 0).mean()\n",
        "    print(f\"\\nüìä AN√ÅLISE DE DENSIDADE:\")\n",
        "    print(f\"   üìä Densidade: {density:.3f}\")\n",
        "    if density > 0.8:\n",
        "        print(f\"   ‚úÖ Embedding denso (mais de 80% dos valores s√£o n√£o-zero)\")\n",
        "    elif density > 0.3:\n",
        "        print(f\"   ‚ö†Ô∏è  Embedding moderadamente esparso (30-80% dos valores s√£o n√£o-zero)\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Embedding muito esparso (menos de 30% dos valores s√£o n√£o-zero)\")\n",
        "    \n",
        "    # 4. AN√ÅLISE DE NORMALIZA√á√ÉO\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "    print(f\"\\nüìè AN√ÅLISE DE NORMALIZA√á√ÉO:\")\n",
        "    print(f\"   üìä Norma m√©dia: {norms.mean():.4f}\")\n",
        "    print(f\"   üìä Norma desvio padr√£o: {norms.std():.4f}\")\n",
        "    print(f\"   üìä Norma m√≠nima: {norms.min():.4f}\")\n",
        "    print(f\"   üìä Norma m√°xima: {norms.max():.4f}\")\n",
        "    \n",
        "    if abs(norms.mean() - 1.0) < 0.1:\n",
        "        print(f\"   ‚úÖ Embeddings normalizados (norma ‚âà 1.0)\")\n",
        "        print(f\"   üí° Normaliza√ß√£o √© importante para m√©tricas de dist√¢ncia\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Embeddings n√£o normalizados (norma ‚â† 1.0)\")\n",
        "        print(f\"   üí° Considere normalizar para clustering\")\n",
        "    \n",
        "    # 5. EXIBI√á√ÉO DE EXEMPLO DO EMBEDDING\n",
        "    print(f\"\\nüìù EXEMPLO DO EMBEDDING:\")\n",
        "    if embeddings.shape[1] <= 20:\n",
        "        print(f\"   Embedding completo (pequeno): {embeddings[0]}\")\n",
        "    else:\n",
        "        sample_size = min(20, embeddings.shape[1])\n",
        "        print(f\"   Amostra (primeiros {sample_size} valores): {embeddings[0][:sample_size]}\")\n",
        "        print(f\"   ... (total: {embeddings.shape[1]:,} dimens√µes)\")\n",
        "        print(f\"   üí° Este √© apenas um exemplo - cada documento tem {embeddings.shape[1]:,} dimens√µes\")\n",
        "    \n",
        "    # 6. AN√ÅLISE ESPEC√çFICA POR TIPO\n",
        "    print(f\"\\nüî¨ AN√ÅLISE ESPEC√çFICA - {name}:\")\n",
        "    \n",
        "    if name == 'TF-IDF':\n",
        "        print(f\"   üìä M√âTODO: Term Frequency √ó Inverse Document Frequency\")\n",
        "        print(f\"   üìä CARACTER√çSTICAS:\")\n",
        "        print(f\"      ‚Ä¢ Baseado em frequ√™ncia de palavras\")\n",
        "        print(f\"      ‚Ä¢ Matriz esparsa (muitos zeros)\")\n",
        "        print(f\"      ‚Ä¢ Dimens√µes = tamanho do vocabul√°rio\")\n",
        "        print(f\"      ‚Ä¢ Interpret√°vel (palavras importantes)\")\n",
        "        \n",
        "        if vectorizer is not None:\n",
        "            print(f\"   üìä VOCABUL√ÅRIO: {len(vectorizer.vocabulary_):,} palavras √∫nicas\")\n",
        "            print(f\"   üìä FEATURES MAIS IMPORTANTES:\")\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            mean_tfidf = embeddings.mean(axis=0)\n",
        "            top_features = np.argsort(mean_tfidf)[-10:][::-1]\n",
        "            for i, idx in enumerate(top_features):\n",
        "                print(f\"      {i+1:2d}. {feature_names[idx]:<20}: {mean_tfidf[idx]:.4f}\")\n",
        "    \n",
        "    elif name == 'Word2Vec':\n",
        "        print(f\"   üìä M√âTODO: Skip-gram/CBOW com janela deslizante\")\n",
        "        print(f\"   üìä CARACTER√çSTICAS:\")\n",
        "        print(f\"      ‚Ä¢ Aprende contexto de palavras\")\n",
        "        print(f\"      ‚Ä¢ Palavras similares ficam pr√≥ximas\")\n",
        "        print(f\"      ‚Ä¢ Dimens√µes fixas (configur√°vel)\")\n",
        "        print(f\"      ‚Ä¢ Captura similaridade sem√¢ntica\")\n",
        "        print(f\"   üìä TREINAMENTO: Modelo treinado em {embeddings.shape[0]:,} documentos\")\n",
        "        print(f\"   üìä DIMENS√ïES: {embeddings.shape[1]} (configur√°vel, geralmente 100-300)\")\n",
        "    \n",
        "    elif name == 'BERT':\n",
        "        print(f\"   üìä M√âTODO: Bidirectional Encoder Representations from Transformers\")\n",
        "        print(f\"   üìä CARACTER√çSTICAS:\")\n",
        "        print(f\"      ‚Ä¢ Bidirecional (l√™ em ambas as dire√ß√µes)\")\n",
        "        print(f\"      ‚Ä¢ Attention mechanism\")\n",
        "        print(f\"      ‚Ä¢ Contextualizado (mesma palavra, contextos diferentes)\")\n",
        "        print(f\"      ‚Ä¢ Dimens√µes: {embeddings.shape[1]} (bert-base-uncased)\")\n",
        "        print(f\"   üìä PROCESSAMENTO: Local (sem internet)\")\n",
        "        print(f\"   üìä QUALIDADE: Alta para tarefas de NLP\")\n",
        "    \n",
        "    elif name == 'Sentence-BERT':\n",
        "        print(f\"   üìä M√âTODO: Sentence-BERT (otimizado para similaridade)\")\n",
        "        print(f\"   üìä CARACTER√çSTICAS:\")\n",
        "        print(f\"      ‚Ä¢ Otimizado para similaridade de senten√ßas\")\n",
        "        print(f\"      ‚Ä¢ Ideal para clustering\")\n",
        "        print(f\"      ‚Ä¢ Dimens√µes: {embeddings.shape[1]} (all-MiniLM-L6-v2)\")\n",
        "        print(f\"      ‚Ä¢ Normalizado por padr√£o\")\n",
        "        print(f\"   üìä PROCESSAMENTO: Local (sem internet)\")\n",
        "        print(f\"   üìä QUALIDADE: Excelente para clustering sem√¢ntico\")\n",
        "    \n",
        "    elif name == 'OpenAI':\n",
        "        print(f\"   üìä M√âTODO: OpenAI Embeddings (text-embedding-3-small)\")\n",
        "        print(f\"   üìä CARACTER√çSTICAS:\")\n",
        "        print(f\"      ‚Ä¢ √öltima gera√ß√£o de embeddings\")\n",
        "        print(f\"      ‚Ä¢ Treinado especificamente para similaridade\")\n",
        "        print(f\"      ‚Ä¢ Dimens√µes: {embeddings.shape[1]} (text-embedding-3-small)\")\n",
        "        print(f\"      ‚Ä¢ Normalizado por padr√£o\")\n",
        "        print(f\"   üìä PROCESSAMENTO: API externa (requer internet)\")\n",
        "        print(f\"   üìä QUALIDADE: M√°xima qualidade em benchmarks\")\n",
        "    \n",
        "    # 7. INTERPRETA√á√ÉO DID√ÅTICA\n",
        "    print(f\"\\nüí° INTERPRETA√á√ÉO DID√ÅTICA:\")\n",
        "    print(f\"   üéØ Para clustering, este embedding √©:\")\n",
        "    if density > 0.8:\n",
        "        print(f\"      ‚Ä¢ Denso: bom para algoritmos que precisam de dados densos\")\n",
        "    else:\n",
        "        print(f\"      ‚Ä¢ Esparso: bom para algoritmos que lidam bem com esparsidade\")\n",
        "    \n",
        "    if abs(norms.mean() - 1.0) < 0.1:\n",
        "        print(f\"      ‚Ä¢ Normalizado: ideal para m√©tricas de dist√¢ncia (cosine, euclidiana)\")\n",
        "    else:\n",
        "        print(f\"      ‚Ä¢ N√£o normalizado: considere normalizar antes do clustering\")\n",
        "    \n",
        "    print(f\"      ‚Ä¢ Dimens√µes: {embeddings.shape[1]:,} - {'baixa' if embeddings.shape[1] < 500 else 'alta'} dimensionalidade\")\n",
        "    \n",
        "    return {\n",
        "        'density': density,\n",
        "        'normalized': abs(norms.mean() - 1.0) < 0.1,\n",
        "        'mean_norm': norms.mean(),\n",
        "        'std_norm': norms.std(),\n",
        "        'memory_mb': embeddings.nbytes / 1024 / 1024\n",
        "    }\n",
        "\n",
        "def compare_embeddings_comprehensive(embeddings_dict):\n",
        "    \"\"\"Compara√ß√£o abrangente entre todos os tipos de embeddings\"\"\"\n",
        "    print(f\"\\nüîç COMPARA√á√ÉO ABRANGENTE DOS EMBEDDINGS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Criar tabela comparativa\n",
        "    comparison_data = []\n",
        "    \n",
        "    for name, embeddings in embeddings_dict.items():\n",
        "        if embeddings is not None:\n",
        "            # An√°lise b√°sica\n",
        "            density = (embeddings != 0).mean()\n",
        "            norms = np.linalg.norm(embeddings, axis=1)\n",
        "            is_normalized = abs(norms.mean() - 1.0) < 0.1\n",
        "            \n",
        "            comparison_data.append({\n",
        "                'Tipo': name,\n",
        "                'Dimens√µes': f\"{embeddings.shape[1]:,}\",\n",
        "                'Mem√≥ria (MB)': f\"{embeddings.nbytes / 1024 / 1024:.1f}\",\n",
        "                'Densidade': f\"{density:.3f}\",\n",
        "                'Normalizado': 'Sim' if is_normalized else 'N√£o',\n",
        "                'Norma M√©dia': f\"{norms.mean():.3f}\",\n",
        "                'Processamento': 'Local' if 'OpenAI' not in name else 'API Externa'\n",
        "            })\n",
        "    \n",
        "    # Criar DataFrame e exibir\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    print(f\"\\nüìä TABELA COMPARATIVA:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "    \n",
        "    # An√°lise de trade-offs\n",
        "    print(f\"\\n‚öñÔ∏è  AN√ÅLISE DE TRADE-OFFS:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Melhor para clustering\n",
        "    best_clustering = None\n",
        "    best_score = 0\n",
        "    for name, embeddings in embeddings_dict.items():\n",
        "        if embeddings is not None:\n",
        "            # Score baseado em normaliza√ß√£o + densidade + dimens√µes\n",
        "            norms = np.linalg.norm(embeddings, axis=1)\n",
        "            is_normalized = abs(norms.mean() - 1.0) < 0.1\n",
        "            density = (embeddings != 0).mean()\n",
        "            \n",
        "            # Score: normaliza√ß√£o (40%) + densidade (30%) + dimens√µes adequadas (30%)\n",
        "            norm_score = 1.0 if is_normalized else 0.0\n",
        "            density_score = min(density * 2, 1.0)  # Normalizar densidade\n",
        "            dim_score = 1.0 if 100 <= embeddings.shape[1] <= 1000 else 0.5\n",
        "            \n",
        "            total_score = 0.4 * norm_score + 0.3 * density_score + 0.3 * dim_score\n",
        "            \n",
        "            if total_score > best_score:\n",
        "                best_score = total_score\n",
        "                best_clustering = name\n",
        "    \n",
        "    print(f\"   üèÜ Melhor para clustering: {best_clustering} (score: {best_score:.3f})\")\n",
        "    \n",
        "    # Recomenda√ß√µes\n",
        "    print(f\"\\nüí° RECOMENDA√á√ïES:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"   üéØ Para clustering sem√¢ntico: Sentence-BERT\")\n",
        "    print(f\"   üéØ Para baseline r√°pido: Word2Vec\")\n",
        "    print(f\"   üéØ Para m√°xima qualidade: OpenAI (se or√ßamento permitir)\")\n",
        "    print(f\"   üéØ Para interpretabilidade: TF-IDF\")\n",
        "    print(f\"   üéØ Para processamento local: BERT ou Sentence-BERT\")\n",
        "    \n",
        "    return df_comparison\n",
        "\n",
        "# Executar an√°lise detalhada\n",
        "print(\"üß† AN√ÅLISE DETALHADA E COMPARATIVA DOS EMBEDDINGS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"üìö Esta an√°lise examina cada tipo de embedding em detalhes\")\n",
        "print(\"üéØ Objetivo: Entender caracter√≠sticas e escolher o melhor para clustering\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Preparar dados para an√°lise\n",
        "embeddings_dict = {\n",
        "    'Word2Vec': word2vec_embeddings,\n",
        "    'BERT': bert_embeddings,\n",
        "    'Sentence-BERT': sbert_embeddings\n",
        "}\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    embeddings_dict['OpenAI'] = openai_embeddings\n",
        "\n",
        "# An√°lise individual de cada embedding\n",
        "detailed_results = {}\n",
        "for name, embeddings in embeddings_dict.items():\n",
        "    if embeddings is not None:\n",
        "        detailed_results[name] = analyze_embedding_detailed(name, embeddings)\n",
        "\n",
        "# Compara√ß√£o abrangente\n",
        "comparison_table = compare_embeddings_comprehensive(embeddings_dict)\n",
        "\n",
        "print(f\"\\n‚úÖ An√°lise detalhada conclu√≠da!\")\n",
        "print(f\"üìä {len(detailed_results)} tipos de embeddings analisados\")\n",
        "print(f\"üí° Use os resultados para escolher o melhor embedding para seu caso de uso\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Visualiza√ß√£o para Clustering: Redu√ß√£o Dimensional\n",
        "\n",
        "### **Por que Reduzir Dimens√µes?**\n",
        "- **Visualiza√ß√£o**: Humanos s√≥ conseguem ver em 2D/3D\n",
        "- **Inspe√ß√£o**: Verificar qualidade dos clusters\n",
        "- **Debugging**: Identificar problemas nos embeddings\n",
        "- **An√°lise**: Entender estrutura dos dados\n",
        "\n",
        "### **T√©cnicas de Redu√ß√£o Dimensional**\n",
        "1. **PCA**: Preserva vari√¢ncia global, linear\n",
        "2. **t-SNE**: Preserva estrutura local, n√£o-linear\n",
        "3. **UMAP**: Balanceia local/global, mais r√°pido que t-SNE\n",
        "\n",
        "### **O que Procurar nas Visualiza√ß√µes?**\n",
        "- **Agrupamentos**: Pontos similares devem estar pr√≥ximos\n",
        "- **Separabilidade**: Classes diferentes devem estar distantes\n",
        "- **Densidade**: Clusters devem ser densos internamente\n",
        "- **Outliers**: Pontos isolados podem ser anomalias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç AN√ÅLISE DETALHADA DOS EMBEDDINGS\n",
            "============================================================\n",
            "üß† ESTAT√çSTICAS DOS EMBEDDINGS\n",
            "============================================================\n",
            "\n",
            "üîπ TF-IDF:\n",
            "   üìê Dimens√µes: (9085, 4096)\n",
            "   üìä Tipo: Local\n",
            "   üíæ Tamanho em mem√≥ria: 283.9 MB\n",
            "   üìà Valores m√©dios: 0.0012\n",
            "   üìä Desvio padr√£o: 0.0156\n",
            "   üìâ Valor m√≠nimo: -0.0417\n",
            "   üìà Valor m√°ximo: 1.0000\n",
            "   üìè Norma m√©dia: 0.9997\n",
            "   üìè Norma std: 0.0107\n",
            "   ‚úÖ Embeddings normalizados\n",
            "\n",
            "üîπ Word2Vec:\n",
            "   üìê Dimens√µes: (9085, 100)\n",
            "   üìä Tipo: Local\n",
            "   üíæ Tamanho em mem√≥ria: 6.9 MB\n",
            "   üìà Valores m√©dios: 0.0090\n",
            "   üìä Desvio padr√£o: 0.1650\n",
            "   üìâ Valor m√≠nimo: -0.8833\n",
            "   üìà Valor m√°ximo: 1.0701\n",
            "   üìè Norma m√©dia: 1.6469\n",
            "   üìè Norma std: 0.1415\n",
            "   ‚ö†Ô∏è  Embeddings n√£o normalizados\n",
            "\n",
            "üîπ BERT:\n",
            "   üìê Dimens√µes: (9085, 768)\n",
            "   üìä Tipo: Local\n",
            "   üíæ Tamanho em mem√≥ria: 53.2 MB\n",
            "   üìà Valores m√©dios: -0.0015\n",
            "   üìä Desvio padr√£o: 0.0361\n",
            "   üìâ Valor m√≠nimo: -0.5977\n",
            "   üìà Valor m√°ximo: 0.1675\n",
            "   üìè Norma m√©dia: 1.0000\n",
            "   üìè Norma std: 0.0000\n",
            "   ‚úÖ Embeddings normalizados\n",
            "\n",
            "üîπ Sentence-BERT:\n",
            "   üìê Dimens√µes: (9085, 384)\n",
            "   üìä Tipo: Local\n",
            "   üíæ Tamanho em mem√≥ria: 26.6 MB\n",
            "   üìà Valores m√©dios: -0.0004\n",
            "   üìä Desvio padr√£o: 0.0510\n",
            "   üìâ Valor m√≠nimo: -0.2516\n",
            "   üìà Valor m√°ximo: 0.2628\n",
            "   üìè Norma m√©dia: 1.0000\n",
            "   üìè Norma std: 0.0000\n",
            "   ‚úÖ Embeddings normalizados\n",
            "\n",
            "üîπ OpenAI:\n",
            "   üìê Dimens√µes: (9085, 1536)\n",
            "   üìä Tipo: API Externa\n",
            "   üíæ Tamanho em mem√≥ria: 106.5 MB\n",
            "   üìà Valores m√©dios: -0.0003\n",
            "   üìä Desvio padr√£o: 0.0255\n",
            "   üìâ Valor m√≠nimo: -0.2014\n",
            "   üìà Valor m√°ximo: 0.1754\n",
            "   üìè Norma m√©dia: 0.9992\n",
            "   üìè Norma std: 0.0190\n",
            "   ‚úÖ Embeddings normalizados\n"
          ]
        }
      ],
      "source": [
        "# üß† Estat√≠sticas Detalhadas dos Embeddings\n",
        "def print_embeddings_statistics(embeddings_dict):\n",
        "    \"\"\"Imprime estat√≠sticas detalhadas dos embeddings\"\"\"\n",
        "    print(\"üß† ESTAT√çSTICAS DOS EMBEDDINGS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for name, embeddings in embeddings_dict.items():\n",
        "        print(f\"\\nüîπ {name}:\")\n",
        "        print(f\"   üìê Dimens√µes: {embeddings.shape}\")\n",
        "        print(f\"   üìä Tipo: {'Local' if 'OpenAI' not in name else 'API Externa'}\")\n",
        "        print(f\"   üíæ Tamanho em mem√≥ria: {embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
        "        print(f\"   üìà Valores m√©dios: {embeddings.mean():.4f}\")\n",
        "        print(f\"   üìä Desvio padr√£o: {embeddings.std():.4f}\")\n",
        "        print(f\"   üìâ Valor m√≠nimo: {embeddings.min():.4f}\")\n",
        "        print(f\"   üìà Valor m√°ximo: {embeddings.max():.4f}\")\n",
        "        \n",
        "        # An√°lise de normaliza√ß√£o\n",
        "        norms = np.linalg.norm(embeddings, axis=1)\n",
        "        print(f\"   üìè Norma m√©dia: {norms.mean():.4f}\")\n",
        "        print(f\"   üìè Norma std: {norms.std():.4f}\")\n",
        "        \n",
        "        # Verificar se est√° normalizado\n",
        "        if abs(norms.mean() - 1.0) < 0.1:\n",
        "            print(f\"   ‚úÖ Embeddings normalizados\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Embeddings n√£o normalizados\")\n",
        "\n",
        "# Executar an√°lise dos embeddings\n",
        "print(\"üîç AN√ÅLISE DETALHADA DOS EMBEDDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Preparar dados para an√°lise\n",
        "embeddings_dict = {\n",
        "    'TF-IDF': tfidf_embeddings,\n",
        "    'Word2Vec': word2vec_embeddings,\n",
        "    'BERT': bert_embeddings,\n",
        "    'Sentence-BERT': sbert_embeddings\n",
        "}\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    embeddings_dict['OpenAI'] = openai_embeddings\n",
        "\n",
        "print_embeddings_statistics(embeddings_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Resumo Final da Gera√ß√£o de Embeddings\n",
        "\n",
        "### **Vis√£o Geral do Processo**\n",
        "Agora que geramos todos os embeddings, vamos consolidar as informa√ß√µes:\n",
        "\n",
        "### **O que Geramos?**\n",
        "1. **TF-IDF**: M√©todo cl√°ssico baseado em frequ√™ncia\n",
        "2. **Word2Vec**: Embeddings de palavras com contexto\n",
        "3. **BERT**: Contextualiza√ß√£o bidirecional profunda\n",
        "4. **Sentence-BERT**: Otimizado para similaridade de senten√ßas\n",
        "5. **OpenAI**: √öltima gera√ß√£o com m√°xima qualidade\n",
        "\n",
        "### **Pr√≥ximos Passos**\n",
        "- Visualizar embeddings em 2D/3D\n",
        "- Aplicar algoritmos de clustering\n",
        "- Comparar resultados entre diferentes m√©todos\n",
        "- Avaliar qual funciona melhor para nossos dados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Visualiza√ß√£o e An√°lise dos Embeddings\n",
        "\n",
        "### **Por que Visualizar Embeddings?**\n",
        "**Visualiza√ß√£o** √© uma ferramenta poderosa para entender como diferentes tipos de embeddings capturam similaridade sem√¢ntica. Atrav√©s de t√©cnicas de **redu√ß√£o dimensional**, podemos:\n",
        "\n",
        "- **Ver clusters sem√¢nticos**: Documentos similares ficam pr√≥ximos no espa√ßo 2D\n",
        "- **Comparar m√©todos**: Diferentes embeddings podem agrupar documentos de forma distinta\n",
        "- **Validar qualidade**: Clusters bem definidos indicam embeddings de alta qualidade\n",
        "- **Identificar padr√µes**: Padr√µes visuais revelam caracter√≠sticas dos dados\n",
        "\n",
        "### **T√©cnicas de Redu√ß√£o Dimensional**\n",
        "\n",
        "#### **1. PCA (Principal Component Analysis)**\n",
        "- **O que faz**: Encontra as dire√ß√µes de maior vari√¢ncia nos dados\n",
        "- **Caracter√≠sticas**: Linear, r√°pido, preserva estrutura global\n",
        "- **Melhor para**: Dados com estrutura linear clara\n",
        "- **Interpreta√ß√£o**: Primeiros componentes capturam mais informa√ß√£o\n",
        "\n",
        "#### **2. t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
        "- **O que faz**: Preserva dist√¢ncias locais, cria clusters bem separados\n",
        "- **Caracter√≠sticas**: N√£o-linear, lento, foca em estrutura local\n",
        "- **Melhor para**: Identificar clusters distintos\n",
        "- **Interpreta√ß√£o**: Dist√¢ncias pr√≥ximas = similaridade sem√¢ntica\n",
        "\n",
        "#### **3. UMAP (Uniform Manifold Approximation and Projection)**\n",
        "- **O que faz**: Balanceia estrutura local e global\n",
        "- **Caracter√≠sticas**: N√£o-linear, r√°pido, preserva tanto local quanto global\n",
        "- **Melhor para**: An√°lise geral de embeddings\n",
        "- **Interpreta√ß√£o**: Estrutura preservada = qualidade do embedding\n",
        "\n",
        "### **O que Esperamos Ver?**\n",
        "\n",
        "#### **‚úÖ Embeddings de Alta Qualidade:**\n",
        "- **Clusters bem definidos**: Documentos da mesma categoria ficam pr√≥ximos\n",
        "- **Separa√ß√£o clara**: Diferentes categorias formam grupos distintos\n",
        "- **Densidade adequada**: N√£o muito espalhados, n√£o muito agrupados\n",
        "\n",
        "#### **‚ùå Embeddings de Baixa Qualidade:**\n",
        "- **Clusters misturados**: Categorias diferentes se sobrep√µem\n",
        "- **Estrutura aleat√≥ria**: Sem padr√µes visuais claros\n",
        "- **Muito espalhados**: Dificulta identifica√ß√£o de grupos\n",
        "\n",
        "### **Interpreta√ß√£o dos Gr√°ficos**\n",
        "\n",
        "#### **Cores e Categorias:**\n",
        "- **Cada cor** representa uma categoria do dataset\n",
        "- **Pontos pr√≥ximos** = documentos semanticamente similares\n",
        "- **Clusters densos** = categorias bem definidas\n",
        "- **Sobreposi√ß√£o** = categorias com conte√∫do similar\n",
        "\n",
        "#### **M√©tricas de Qualidade:**\n",
        "- **Silhouette Score**: Mede qu√£o bem separados est√£o os clusters\n",
        "- **ARI (Adjusted Rand Index)**: Compara com categorias reais\n",
        "- **NMI (Normalized Mutual Information)**: Informa√ß√£o m√∫tua normalizada\n",
        "\n",
        "### **Pr√≥ximos Passos**\n",
        "Ap√≥s a visualiza√ß√£o, vamos:\n",
        "1. **Analisar padr√µes** visuais de cada embedding\n",
        "2. **Comparar m√©todos** lado a lado\n",
        "3. **Identificar o melhor** para clustering\n",
        "4. **Validar com m√©tricas** quantitativas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã RESUMO FINAL DA GERA√á√ÉO DE EMBEDDINGS\n",
            "============================================================\n",
            "üìä Dataset processado:\n",
            "   üìã Total de documentos: 9,085\n",
            "   üè∑Ô∏è  Classes: 10\n",
            "   üìè Tamanho m√©dio: 1130 caracteres\n",
            "\n",
            "üß† Embeddings gerados:\n",
            "   TF-IDF         : (9085, 4096) (283.9 MB)\n",
            "   Word2Vec       : (9085, 100) (6.9 MB)\n",
            "   BERT           : (9085, 768) (53.2 MB)\n",
            "   Sentence-BERT  : (9085, 384) (26.6 MB)\n",
            "   OpenAI         : (9085, 1536) (106.5 MB)\n",
            "\n",
            "üíæ Uso total de mem√≥ria: 477.2 MB\n",
            "\n",
            "‚öôÔ∏è  Configura√ß√µes utilizadas:\n",
            "   MAX_CHARS_PER_REQUEST: 30000\n",
            "   BATCH_SIZE_SMALL_TEXTS: 8\n",
            "   BATCH_SIZE_MEDIUM_TEXTS: 4\n",
            "   BATCH_SIZE_LARGE_TEXTS: 2\n",
            "   TEXT_MIN_LENGTH: 50\n",
            "\n",
            "‚úÖ Pr√≥ximos passos:\n",
            "   1. Visualizar embeddings com PCA/t-SNE/UMAP\n",
            "   2. Aplicar algoritmos de clustering\n",
            "   3. Avaliar qualidade dos clusters\n",
            "   4. Comparar diferentes tipos de embeddings\n"
          ]
        }
      ],
      "source": [
        "# üìã Resumo Final da Gera√ß√£o de Embeddings\n",
        "def print_final_summary(embeddings_dict, df):\n",
        "    \"\"\"Imprime resumo final da gera√ß√£o de embeddings\"\"\"\n",
        "    print(\"üìã RESUMO FINAL DA GERA√á√ÉO DE EMBEDDINGS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"üìä Dataset processado:\")\n",
        "    print(f\"   üìã Total de documentos: {len(df):,}\")\n",
        "    print(f\"   üè∑Ô∏è  Classes: {df['category'].nunique()}\")\n",
        "    print(f\"   üìè Tamanho m√©dio: {df['text'].str.len().mean():.0f} caracteres\")\n",
        "    \n",
        "    print(f\"\\nüß† Embeddings gerados:\")\n",
        "    total_memory = 0\n",
        "    for name, embeddings in embeddings_dict.items():\n",
        "        memory_mb = embeddings.nbytes / 1024 / 1024\n",
        "        total_memory += memory_mb\n",
        "        print(f\"   {name:<15}: {embeddings.shape} ({memory_mb:.1f} MB)\")\n",
        "    \n",
        "    print(f\"\\nüíæ Uso total de mem√≥ria: {total_memory:.1f} MB\")\n",
        "    \n",
        "    print(f\"\\n‚öôÔ∏è  Configura√ß√µes utilizadas:\")\n",
        "    print(f\"   MAX_CHARS_PER_REQUEST: {MAX_CHARS_PER_REQUEST}\")\n",
        "    print(f\"   BATCH_SIZE_SMALL_TEXTS: {BATCH_SIZE_SMALL_TEXTS}\")\n",
        "    print(f\"   BATCH_SIZE_MEDIUM_TEXTS: {BATCH_SIZE_MEDIUM_TEXTS}\")\n",
        "    print(f\"   BATCH_SIZE_LARGE_TEXTS: {BATCH_SIZE_LARGE_TEXTS}\")\n",
        "    print(f\"   TEXT_MIN_LENGTH: {TEXT_MIN_LENGTH}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Pr√≥ximos passos:\")\n",
        "    print(f\"   1. Visualizar embeddings com PCA/t-SNE/UMAP\")\n",
        "    print(f\"   2. Aplicar algoritmos de clustering\")\n",
        "    print(f\"   3. Avaliar qualidade dos clusters\")\n",
        "    print(f\"   4. Comparar diferentes tipos de embeddings\")\n",
        "\n",
        "# Executar resumo final\n",
        "print_final_summary(embeddings_dict, df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé® VISUALIZA√á√ÉO DOS EMBEDDINGS\n",
            "==================================================\n",
            "üìä Reduzindo dimens√µes com PCA...\n",
            "üìä PCA: 29.2% da vari√¢ncia explicada\n",
            "üìä PCA: 16.2% da vari√¢ncia explicada\n",
            "üìä PCA: 8.0% da vari√¢ncia explicada\n",
            "üìä PCA: 9.7% da vari√¢ncia explicada\n",
            "üé® Gerando visualiza√ß√µes...\n",
            "üìä PCA: 100.0% da vari√¢ncia explicada\n",
            "üìä PCA: 100.0% da vari√¢ncia explicada\n",
            "üìä PCA: 100.0% da vari√¢ncia explicada\n",
            "üìä PCA: 100.0% da vari√¢ncia explicada\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Plotar compara√ß√£o\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müé® Gerando visualiza√ß√µes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43mplot_embeddings_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpca_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPCA\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mplot_embeddings_comparison\u001b[39m\u001b[34m(embeddings_dict, labels, method)\u001b[39m\n\u001b[32m     90\u001b[39m         fig.add_trace(\n\u001b[32m     91\u001b[39m             go.Scatter(\n\u001b[32m     92\u001b[39m                 x=embeddings_2d[mask, \u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m             row=\u001b[32m1\u001b[39m, col=i+\u001b[32m1\u001b[39m\n\u001b[32m    100\u001b[39m         )\n\u001b[32m    102\u001b[39m fig.update_layout(\n\u001b[32m    103\u001b[39m     title=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCompara√ß√£o de Embeddings - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m    104\u001b[39m     height=\u001b[32m500\u001b[39m,\n\u001b[32m    105\u001b[39m     width=\u001b[32m300\u001b[39m * n_embeddings\n\u001b[32m    106\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGEÃÇNCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUCÃßAÃÉO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/.venv/lib/python3.12/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGEÃÇNCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUCÃßAÃÉO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/.venv/lib/python3.12/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
          ]
        }
      ],
      "source": [
        "# üé® Fun√ß√µes de Visualiza√ß√£o para Clustering\n",
        "def reduce_dimensions(embeddings, method='pca', n_components=2, **kwargs):\n",
        "    \"\"\"Reduz dimens√µes dos embeddings para visualiza√ß√£o\"\"\"\n",
        "    \n",
        "    if method.lower() == 'pca':\n",
        "        from sklearn.decomposition import PCA\n",
        "        reducer = PCA(n_components=n_components, random_state=42)\n",
        "        reduced = reducer.fit_transform(embeddings)\n",
        "        explained_var = reducer.explained_variance_ratio_.sum()\n",
        "        print(f\"üìä PCA: {explained_var:.1%} da vari√¢ncia explicada\")\n",
        "        \n",
        "    elif method.lower() == 'tsne':\n",
        "        from sklearn.manifold import TSNE\n",
        "        reducer = TSNE(n_components=n_components, random_state=42, **kwargs)\n",
        "        reduced = reducer.fit_transform(embeddings)\n",
        "        print(f\"üìä t-SNE: Redu√ß√£o dimensional conclu√≠da\")\n",
        "        \n",
        "    elif method.lower() == 'umap':\n",
        "        try:\n",
        "            import umap\n",
        "            reducer = umap.UMAP(n_components=n_components, random_state=42, **kwargs)\n",
        "            reduced = reducer.fit_transform(embeddings)\n",
        "            print(f\"üìä UMAP: Redu√ß√£o dimensional conclu√≠da\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  UMAP n√£o dispon√≠vel, usando PCA como fallback\")\n",
        "            from sklearn.decomposition import PCA\n",
        "            reducer = PCA(n_components=n_components, random_state=42)\n",
        "            reduced = reducer.fit_transform(embeddings)\n",
        "            explained_var = reducer.explained_variance_ratio_.sum()\n",
        "            print(f\"üìä PCA (fallback): {explained_var:.1%} da vari√¢ncia explicada\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  M√©todo '{method}' n√£o reconhecido, usando PCA como padr√£o\")\n",
        "        from sklearn.decomposition import PCA\n",
        "        reducer = PCA(n_components=n_components, random_state=42)\n",
        "        reduced = reducer.fit_transform(embeddings)\n",
        "        explained_var = reducer.explained_variance_ratio_.sum()\n",
        "        print(f\"üìä PCA (padr√£o): {explained_var:.1%} da vari√¢ncia explicada\")\n",
        "    \n",
        "    return reduced\n",
        "\n",
        "def plot_embeddings_2d(embeddings_2d, labels, title, method='PCA'):\n",
        "    \"\"\"Plota embeddings em 2D com cores por classe\"\"\"\n",
        "    \n",
        "    # Criar DataFrame para plotly\n",
        "    df_plot = pd.DataFrame({\n",
        "        'x': embeddings_2d[:, 0],\n",
        "        'y': embeddings_2d[:, 1],\n",
        "        'category': labels,\n",
        "        'text': df['text'].str[:100] + '...'  # Primeiros 100 caracteres\n",
        "    })\n",
        "    \n",
        "    # Plot interativo\n",
        "    fig = px.scatter(\n",
        "        df_plot, \n",
        "        x='x', y='y', \n",
        "        color='category',\n",
        "        hover_data=['text'],\n",
        "        title=f'{method} - Embeddings por Classe',\n",
        "        width=800, height=600\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        xaxis_title=f'{method} Component 1',\n",
        "        yaxis_title=f'{method} Component 2',\n",
        "        legend_title=\"Categoria\"\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "def plot_embeddings_comparison(embeddings_dict, labels, method='PCA'):\n",
        "    \"\"\"Compara diferentes tipos de embeddings\"\"\"\n",
        "    \n",
        "    n_embeddings = len(embeddings_dict)\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=n_embeddings,\n",
        "        subplot_titles=list(embeddings_dict.keys()),\n",
        "        specs=[[{'type': 'scatter'} for _ in range(n_embeddings)]]\n",
        "    )\n",
        "    \n",
        "    colors = px.colors.qualitative.Set1\n",
        "    \n",
        "    for i, (name, embeddings) in enumerate(embeddings_dict.items()):\n",
        "        # Reduzir dimens√µes\n",
        "        embeddings_2d = reduce_dimensions(embeddings, method=method)\n",
        "        \n",
        "        # Plot\n",
        "        for j, category in enumerate(np.unique(labels)):\n",
        "            mask = labels == category\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=embeddings_2d[mask, 0],\n",
        "                    y=embeddings_2d[mask, 1],\n",
        "                    mode='markers',\n",
        "                    name=category,\n",
        "                    marker=dict(color=colors[j % len(colors)], size=4),\n",
        "                    showlegend=(i == 0)  # S√≥ mostrar legenda no primeiro plot\n",
        "                ),\n",
        "                row=1, col=i+1\n",
        "            )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title=f'Compara√ß√£o de Embeddings - {method}',\n",
        "        height=500,\n",
        "        width=300 * n_embeddings\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "# Visualizar embeddings\n",
        "print(\"üé® VISUALIZA√á√ÉO DOS EMBEDDINGS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Preparar dados para visualiza√ß√£o\n",
        "embeddings_dict = {\n",
        "    'Word2Vec': word2vec_embeddings,\n",
        "    'BERT': bert_embeddings,\n",
        "    'Sentence-BERT': sbert_embeddings\n",
        "}\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    embeddings_dict['OpenAI'] = openai_embeddings\n",
        "\n",
        "# Reduzir dimens√µes com PCA\n",
        "print(\"üìä Reduzindo dimens√µes com PCA...\")\n",
        "pca_embeddings = {}\n",
        "for name, embeddings in embeddings_dict.items():\n",
        "    pca_embeddings[name] = reduce_dimensions(embeddings, method='pca')\n",
        "\n",
        "# Plotar compara√ß√£o\n",
        "print(\"üé® Gerando visualiza√ß√µes...\")\n",
        "plot_embeddings_comparison(pca_embeddings, df['category'], method='PCA')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Algoritmos de Clustering: Cl√°ssicos e Modernos\n",
        "\n",
        "### **K-Means - O Cl√°ssico**\n",
        "- **Funcionamento**: Divide dados em k clusters esf√©ricos\n",
        "- **Vantagens**: Simples, r√°pido, escal√°vel\n",
        "- **Desvantagens**: Assume clusters esf√©ricos, sens√≠vel a inicializa√ß√£o\n",
        "- **Par√¢metros**: n_clusters (k), inicializa√ß√£o\n",
        "\n",
        "### **DBSCAN - Baseado em Densidade**\n",
        "- **Funcionamento**: Agrupa pontos densos, identifica outliers\n",
        "- **Vantagens**: N√£o precisa especificar k, encontra clusters de formas variadas\n",
        "- **Desvantagens**: Sens√≠vel a par√¢metros eps e min_samples\n",
        "- **Par√¢metros**: eps (raio), min_samples (densidade m√≠nima)\n",
        "\n",
        "### **HDBSCAN - Hier√°rquico e Robusto**\n",
        "- **Funcionamento**: Clustering hier√°rquico baseado em densidade\n",
        "- **Vantagens**: Mais robusto que DBSCAN, clusters de tamanhos variados\n",
        "- **Desvantagens**: Mais lento, par√¢metros complexos\n",
        "- **Par√¢metros**: min_cluster_size, min_samples\n",
        "\n",
        "### **M√©tricas de Avalia√ß√£o**\n",
        "- **Internas**: Silhouette, Calinski-Harabasz, Davies-Bouldin\n",
        "- **Externas**: ARI, NMI, Homogeneity, Completeness\n",
        "- **Interpreta√ß√£o**: Valores mais altos = melhor clustering (exceto Davies-Bouldin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Implementa√ß√£o de Algoritmos de Clustering\n",
        "def evaluate_clustering(embeddings, true_labels, cluster_labels, algorithm_name):\n",
        "    \"\"\"Avalia qualidade do clustering\"\"\"\n",
        "    \n",
        "    # M√©tricas internas\n",
        "    silhouette = silhouette_score(embeddings, cluster_labels)\n",
        "    calinski = calinski_harabasz_score(embeddings, cluster_labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, cluster_labels)\n",
        "    \n",
        "    # M√©tricas externas\n",
        "    ari = adjusted_rand_score(true_labels, cluster_labels)\n",
        "    nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
        "    homogeneity = homogeneity_score(true_labels, cluster_labels)\n",
        "    completeness = completeness_score(true_labels, cluster_labels)\n",
        "    v_measure = v_measure_score(true_labels, cluster_labels)\n",
        "    \n",
        "    results = {\n",
        "        'algorithm': algorithm_name,\n",
        "        'silhouette': silhouette,\n",
        "        'calinski_harabasz': calinski,\n",
        "        'davies_bouldin': davies_bouldin,\n",
        "        'ari': ari,\n",
        "        'nmi': nmi,\n",
        "        'homogeneity': homogeneity,\n",
        "        'completeness': completeness,\n",
        "        'v_measure': v_measure,\n",
        "        'n_clusters': len(np.unique(cluster_labels)),\n",
        "        'n_outliers': np.sum(cluster_labels == -1) if -1 in cluster_labels else 0\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "def find_optimal_k(embeddings, max_k=20):\n",
        "    \"\"\"Encontra k √≥timo para K-Means usando m√©todo do cotovelo\"\"\"\n",
        "    \n",
        "    inertias = []\n",
        "    silhouettes = []\n",
        "    k_range = range(2, max_k + 1)\n",
        "    \n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        cluster_labels = kmeans.fit_predict(embeddings)\n",
        "        \n",
        "        inertias.append(kmeans.inertia_)\n",
        "        silhouettes.append(silhouette_score(embeddings, cluster_labels))\n",
        "    \n",
        "    # Encontrar k √≥timo (m√°ximo silhouette)\n",
        "    optimal_k = k_range[np.argmax(silhouettes)]\n",
        "    \n",
        "    return k_range, inertias, silhouettes, optimal_k\n",
        "\n",
        "def cluster_with_kmeans(embeddings, true_labels, k=None):\n",
        "    \"\"\"Clustering com K-Means\"\"\"\n",
        "    \n",
        "    if k is None:\n",
        "        print(\"üîç Encontrando k √≥timo...\")\n",
        "        k_range, inertias, silhouettes, optimal_k = find_optimal_k(embeddings)\n",
        "        k = optimal_k\n",
        "        print(f\"‚úÖ K √≥timo encontrado: {k}\")\n",
        "    \n",
        "    print(f\"üîÑ Executando K-Means com k={k}...\")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    results = evaluate_clustering(embeddings, true_labels, cluster_labels, 'K-Means')\n",
        "    return cluster_labels, results\n",
        "\n",
        "def cluster_with_dbscan(embeddings, true_labels, eps=0.5, min_samples=5):\n",
        "    \"\"\"Clustering com DBSCAN\"\"\"\n",
        "    \n",
        "    print(f\"üîÑ Executando DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    cluster_labels = dbscan.fit_predict(embeddings)\n",
        "    \n",
        "    results = evaluate_clustering(embeddings, true_labels, cluster_labels, 'DBSCAN')\n",
        "    return cluster_labels, results\n",
        "\n",
        "def cluster_with_hdbscan(embeddings, true_labels, min_cluster_size=10, min_samples=5):\n",
        "    \"\"\"Clustering com HDBSCAN\"\"\"\n",
        "    \n",
        "    if not HDBSCAN_AVAILABLE:\n",
        "        print(\"‚ùå HDBSCAN n√£o dispon√≠vel\")\n",
        "        return None, None\n",
        "    \n",
        "    print(f\"üîÑ Executando HDBSCAN (min_cluster_size={min_cluster_size})...\")\n",
        "    hdbscan_clusterer = hdbscan.HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples\n",
        "    )\n",
        "    cluster_labels = hdbscan_clusterer.fit_predict(embeddings)\n",
        "    \n",
        "    results = evaluate_clustering(embeddings, true_labels, cluster_labels, 'HDBSCAN')\n",
        "    return cluster_labels, results\n",
        "\n",
        "# Executar clustering em todos os tipos de embeddings\n",
        "print(\"üîç EXECUTANDO CLUSTERING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Preparar dados\n",
        "true_labels = df['category'].values\n",
        "clustering_results = {}\n",
        "\n",
        "# Preparar embeddings para clustering\n",
        "embeddings_dict = {\n",
        "    'TF-IDF': tfidf_embeddings,\n",
        "    'Word2Vec': word2vec_embeddings,\n",
        "    'BERT': bert_embeddings,\n",
        "    'Sentence-BERT': sbert_embeddings\n",
        "}\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    embeddings_dict['OpenAI'] = openai_embeddings\n",
        "\n",
        "# Testar cada tipo de embedding\n",
        "for name, embeddings in embeddings_dict.items():\n",
        "    print(f\"\\nüìä Clustering com {name}...\")\n",
        "    \n",
        "    # K-Means\n",
        "    kmeans_labels, kmeans_results = cluster_with_kmeans(embeddings, true_labels)\n",
        "    clustering_results[f'{name}_KMeans'] = {\n",
        "        'labels': kmeans_labels,\n",
        "        'results': kmeans_results\n",
        "    }\n",
        "    \n",
        "    # DBSCAN\n",
        "    dbscan_labels, dbscan_results = cluster_with_dbscan(embeddings, true_labels)\n",
        "    clustering_results[f'{name}_DBSCAN'] = {\n",
        "        'labels': dbscan_labels,\n",
        "        'results': dbscan_results\n",
        "    }\n",
        "    \n",
        "    # HDBSCAN\n",
        "    if HDBSCAN_AVAILABLE:\n",
        "        hdbscan_labels, hdbscan_results = cluster_with_hdbscan(embeddings, true_labels)\n",
        "        if hdbscan_results is not None:\n",
        "            clustering_results[f'{name}_HDBSCAN'] = {\n",
        "                'labels': hdbscan_labels,\n",
        "                'results': hdbscan_results\n",
        "            }\n",
        "\n",
        "print(f\"\\n‚úÖ Clustering conclu√≠do para {len(clustering_results)} combina√ß√µes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä An√°lise Comparativa dos Resultados\n",
        "\n",
        "### **Comparando Embeddings e Algoritmos**\n",
        "Agora vamos analisar qual combina√ß√£o de embedding + algoritmo de clustering produz os melhores resultados:\n",
        "\n",
        "### **M√©tricas de Avalia√ß√£o**\n",
        "- **ARI (Adjusted Rand Index)**: Mede similaridade entre clusters e classes reais (0-1, maior √© melhor)\n",
        "- **NMI (Normalized Mutual Information)**: Mede informa√ß√£o compartilhada (0-1, maior √© melhor)\n",
        "- **Silhouette Score**: Mede qualidade interna dos clusters (-1 a 1, maior √© melhor)\n",
        "- **Homogeneity**: Mede se clusters cont√™m apenas uma classe (0-1, maior √© melhor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä An√°lise Comparativa dos Resultados\n",
        "def create_comparison_table(clustering_results):\n",
        "    \"\"\"Cria tabela comparativa dos resultados\"\"\"\n",
        "    \n",
        "    # Extrair resultados\n",
        "    results_data = []\n",
        "    for name, data in clustering_results.items():\n",
        "        result = data['results'].copy()\n",
        "        result['combination'] = name\n",
        "        results_data.append(result)\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    df_results = pd.DataFrame(results_data)\n",
        "    \n",
        "    # Ordenar por ARI (m√©trica principal)\n",
        "    df_results = df_results.sort_values('ari', ascending=False)\n",
        "    \n",
        "    return df_results\n",
        "\n",
        "def plot_metrics_comparison(df_results):\n",
        "    \"\"\"Plota compara√ß√£o das m√©tricas\"\"\"\n",
        "    \n",
        "    # Selecionar m√©tricas principais\n",
        "    metrics = ['ari', 'nmi', 'silhouette', 'homogeneity']\n",
        "    metric_names = ['ARI', 'NMI', 'Silhouette', 'Homogeneity']\n",
        "    \n",
        "    # Criar subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=metric_names,\n",
        "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
        "               [{'type': 'bar'}, {'type': 'bar'}]]\n",
        "    )\n",
        "    \n",
        "    for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "        row = i // 2 + 1\n",
        "        col = i % 2 + 1\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=df_results['combination'],\n",
        "                y=df_results[metric],\n",
        "                name=name,\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title='Compara√ß√£o de M√©tricas de Clustering',\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "def plot_clustering_visualization(embeddings, labels, title, method='PCA'):\n",
        "    \"\"\"Visualiza clusters em 2D\"\"\"\n",
        "    \n",
        "    # Reduzir dimens√µes\n",
        "    embeddings_2d = reduce_dimensions(embeddings, method=method)\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    df_plot = pd.DataFrame({\n",
        "        'x': embeddings_2d[:, 0],\n",
        "        'y': embeddings_2d[:, 1],\n",
        "        'cluster': labels,\n",
        "        'text': df['text'].str[:50] + '...'\n",
        "    })\n",
        "    \n",
        "    # Plot\n",
        "    fig = px.scatter(\n",
        "        df_plot,\n",
        "        x='x', y='y',\n",
        "        color='cluster',\n",
        "        hover_data=['text'],\n",
        "        title=title,\n",
        "        width=800, height=600\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        xaxis_title=f'{method} Component 1',\n",
        "        yaxis_title=f'{method} Component 2'\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "# Criar tabela comparativa\n",
        "print(\"üìä AN√ÅLISE COMPARATIVA DOS RESULTADOS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "df_comparison = create_comparison_table(clustering_results)\n",
        "\n",
        "# Mostrar tabela\n",
        "print(\"üìã RANKING DOS RESULTADOS (ordenado por ARI):\")\n",
        "print(\"=\" * 80)\n",
        "print(df_comparison[['combination', 'ari', 'nmi', 'silhouette', 'homogeneity', 'n_clusters']].to_string(index=False))\n",
        "\n",
        "# Plotar compara√ß√£o\n",
        "print(\"\\nüé® Gerando visualiza√ß√µes...\")\n",
        "plot_metrics_comparison(df_comparison)\n",
        "\n",
        "# Mostrar melhor resultado\n",
        "best_result = df_comparison.iloc[0]\n",
        "print(f\"\\nüèÜ MELHOR RESULTADO:\")\n",
        "print(f\"   Combina√ß√£o: {best_result['combination']}\")\n",
        "print(f\"   ARI: {best_result['ari']:.3f}\")\n",
        "print(f\"   NMI: {best_result['nmi']:.3f}\")\n",
        "print(f\"   Silhouette: {best_result['silhouette']:.3f}\")\n",
        "print(f\"   Clusters: {best_result['n_clusters']}\")\n",
        "\n",
        "# Visualizar melhor resultado\n",
        "best_name = best_result['combination']\n",
        "best_embedding_name = best_name.split('_')[0]\n",
        "best_algorithm = best_name.split('_')[1]\n",
        "\n",
        "if best_embedding_name in embeddings_dict:\n",
        "    best_embeddings = embeddings_dict[best_embedding_name]\n",
        "    best_labels = clustering_results[best_name]['labels']\n",
        "    \n",
        "    print(f\"\\nüé® Visualizando melhor resultado: {best_name}\")\n",
        "    plot_clustering_visualization(\n",
        "        best_embeddings, \n",
        "        best_labels, \n",
        "        f'Melhor Clustering: {best_name}',\n",
        "        method='PCA'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé Integra√ß√£o com Elasticsearch: Armazenamento e Busca\n",
        "\n",
        "### **Por que Elasticsearch para Embeddings?**\n",
        "- **Busca sem√¢ntica**: Encontrar documentos similares usando KNN\n",
        "- **Escalabilidade**: Lida com milh√µes de documentos\n",
        "- **Flexibilidade**: Suporte a diferentes tipos de embeddings\n",
        "- **Visualiza√ß√£o**: Kibana para dashboards interativos\n",
        "- **APIs**: Integra√ß√£o f√°cil com aplica√ß√µes\n",
        "\n",
        "### **Configura√ß√£o do Ambiente**\n",
        "Vamos usar Docker para facilitar o setup:\n",
        "```bash\n",
        "# docker-compose.yml\n",
        "version: '3.8'\n",
        "services:\n",
        "  elasticsearch:\n",
        "    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n",
        "    environment:\n",
        "      - discovery.type=single-node\n",
        "      - xpack.security.enabled=false\n",
        "    ports:\n",
        "      - \"9200:9200\"\n",
        "    volumes:\n",
        "      - es_data:/usr/share/elasticsearch/data\n",
        "\n",
        "  kibana:\n",
        "    image: docker.elastic.co/kibana/kibana:8.11.0\n",
        "    ports:\n",
        "      - \"5601:5601\"\n",
        "    environment:\n",
        "      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n",
        "\n",
        "volumes:\n",
        "  es_data:\n",
        "```\n",
        "\n",
        "### **Estrutura do Index**\n",
        "- **Campos de texto**: Para busca tradicional\n",
        "- **Campos dense_vector**: Para embeddings\n",
        "- **Metadados**: Classes, clusters, timestamps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîé Integra√ß√£o com Elasticsearch\n",
        "def setup_elasticsearch():\n",
        "    \"\"\"Configura conex√£o com Elasticsearch\"\"\"\n",
        "    \n",
        "    if not ELASTICSEARCH_AVAILABLE:\n",
        "        print(\"‚ùå Elasticsearch n√£o dispon√≠vel. Instale com: pip install elasticsearch\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Conectar ao Elasticsearch\n",
        "        es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
        "        \n",
        "        # Verificar conex√£o\n",
        "        if es.ping():\n",
        "            print(\"‚úÖ Conectado ao Elasticsearch\")\n",
        "            return es\n",
        "        else:\n",
        "            print(\"‚ùå N√£o foi poss√≠vel conectar ao Elasticsearch\")\n",
        "            print(\"üí° Dica: Execute 'docker-compose up -d' para iniciar o Elasticsearch\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao conectar com Elasticsearch: {e}\")\n",
        "        print(\"üí° Dica: Verifique se o Elasticsearch est√° rodando na porta 9200\")\n",
        "        return None\n",
        "\n",
        "def create_embeddings_index(es, index_name='embeddings_clustering'):\n",
        "    \"\"\"Cria index para armazenar embeddings\"\"\"\n",
        "    \n",
        "    if es is None:\n",
        "        return False\n",
        "    \n",
        "    # Mapeamento do index\n",
        "    mapping = {\n",
        "        \"mappings\": {\n",
        "            \"properties\": {\n",
        "                \"text\": {\n",
        "                    \"type\": \"text\",\n",
        "                    \"analyzer\": \"standard\"\n",
        "                },\n",
        "                \"category\": {\n",
        "                    \"type\": \"keyword\"\n",
        "                },\n",
        "                \"target\": {\n",
        "                    \"type\": \"integer\"\n",
        "                },\n",
        "                \"word2vec_embedding\": {\n",
        "                    \"type\": \"dense_vector\",\n",
        "                    \"dims\": 100\n",
        "                },\n",
        "                \"bert_embedding\": {\n",
        "                    \"type\": \"dense_vector\",\n",
        "                    \"dims\": 768\n",
        "                },\n",
        "                \"sbert_embedding\": {\n",
        "                    \"type\": \"dense_vector\",\n",
        "                    \"dims\": 384\n",
        "                },\n",
        "                \"openai_embedding\": {\n",
        "                    \"type\": \"dense_vector\",\n",
        "                    \"dims\": 1536\n",
        "                },\n",
        "                \"cluster_labels\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"kmeans\": {\"type\": \"integer\"},\n",
        "                        \"dbscan\": {\"type\": \"integer\"},\n",
        "                        \"hdbscan\": {\"type\": \"integer\"}\n",
        "                    }\n",
        "                },\n",
        "                \"metadata\": {\n",
        "                    \"type\": \"object\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Deletar index se existir\n",
        "        if es.indices.exists(index=index_name):\n",
        "            es.indices.delete(index=index_name)\n",
        "            print(f\"üóëÔ∏è  Index {index_name} deletado\")\n",
        "        \n",
        "        # Criar novo index\n",
        "        es.indices.create(index=index_name, body=mapping)\n",
        "        print(f\"‚úÖ Index {index_name} criado com sucesso\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao criar index: {e}\")\n",
        "        return False\n",
        "\n",
        "def index_embeddings(es, df, embeddings_dict, clustering_results, index_name='embeddings_clustering'):\n",
        "    \"\"\"Indexa embeddings no Elasticsearch\"\"\"\n",
        "    \n",
        "    if es is None:\n",
        "        return False\n",
        "    \n",
        "    print(\"üîÑ Indexando embeddings no Elasticsearch...\")\n",
        "    \n",
        "    # Preparar documentos para indexa√ß√£o\n",
        "    documents = []\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        doc = {\n",
        "            \"text\": row['text'],\n",
        "            \"category\": row['category'],\n",
        "            \"target\": int(row['target']),\n",
        "            \"metadata\": {\n",
        "                \"text_length\": len(row['text']),\n",
        "                \"word_count\": len(row['text'].split())\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Adicionar embeddings\n",
        "        if 'Word2Vec' in embeddings_dict:\n",
        "            doc['word2vec_embedding'] = embeddings_dict['Word2Vec'][i].tolist()\n",
        "        \n",
        "        if 'BERT' in embeddings_dict:\n",
        "            doc['bert_embedding'] = embeddings_dict['BERT'][i].tolist()\n",
        "        \n",
        "        if 'Sentence-BERT' in embeddings_dict:\n",
        "            doc['sbert_embedding'] = embeddings_dict['Sentence-BERT'][i].tolist()\n",
        "        \n",
        "        if 'OpenAI' in embeddings_dict and embeddings_dict['OpenAI'] is not None:\n",
        "            doc['openai_embedding'] = embeddings_dict['OpenAI'][i].tolist()\n",
        "        \n",
        "        # Adicionar labels de clustering\n",
        "        cluster_labels = {}\n",
        "        for name, data in clustering_results.items():\n",
        "            if 'KMeans' in name:\n",
        "                cluster_labels['kmeans'] = int(data['labels'][i])\n",
        "            elif 'DBSCAN' in name:\n",
        "                cluster_labels['dbscan'] = int(data['labels'][i])\n",
        "            elif 'HDBSCAN' in name:\n",
        "                cluster_labels['hdbscan'] = int(data['labels'][i])\n",
        "        \n",
        "        doc['cluster_labels'] = cluster_labels\n",
        "        documents.append(doc)\n",
        "    \n",
        "    # Indexar em lotes\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "        \n",
        "        # Preparar bulk request\n",
        "        bulk_body = []\n",
        "        for j, doc in enumerate(batch):\n",
        "            bulk_body.append({\n",
        "                \"index\": {\n",
        "                    \"_index\": index_name,\n",
        "                    \"_id\": i + j\n",
        "                }\n",
        "            })\n",
        "            bulk_body.append(doc)\n",
        "        \n",
        "        # Executar bulk request\n",
        "        try:\n",
        "            es.bulk(body=bulk_body)\n",
        "            print(f\"   Indexados: {min(i+batch_size, len(documents))}/{len(documents)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao indexar lote {i//batch_size + 1}: {e}\")\n",
        "            return False\n",
        "    \n",
        "    print(f\"‚úÖ {len(documents)} documentos indexados com sucesso\")\n",
        "    return True\n",
        "\n",
        "def search_similar_documents(es, query_text, embedding_type='sbert', top_k=5, index_name='embeddings_clustering'):\n",
        "    \"\"\"Busca documentos similares usando embeddings\"\"\"\n",
        "    \n",
        "    if es is None:\n",
        "        return None\n",
        "    \n",
        "    # Gerar embedding da query\n",
        "    if embedding_type == 'sbert':\n",
        "        query_embedding = generate_sbert_embeddings([query_text])[0]\n",
        "        embedding_field = 'sbert_embedding'\n",
        "    elif embedding_type == 'bert':\n",
        "        query_embedding = generate_bert_embeddings([query_text])[0]\n",
        "        embedding_field = 'bert_embedding'\n",
        "    elif embedding_type == 'word2vec':\n",
        "        query_embedding = get_document_embeddings_word2vec(word2vec_model, [query_text])[0]\n",
        "        embedding_field = 'word2vec_embedding'\n",
        "    else:\n",
        "        print(f\"‚ùå Tipo de embedding n√£o suportado: {embedding_type}\")\n",
        "        return None\n",
        "    \n",
        "    # Query de busca sem√¢ntica\n",
        "    query = {\n",
        "        \"knn\": {\n",
        "            \"field\": embedding_field,\n",
        "            \"query_vector\": query_embedding.tolist(),\n",
        "            \"k\": top_k,\n",
        "            \"num_candidates\": 100\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = es.search(index=index_name, body=query)\n",
        "        \n",
        "        results = []\n",
        "        for hit in response['hits']['hits']:\n",
        "            results.append({\n",
        "                'score': hit['_score'],\n",
        "                'text': hit['_source']['text'][:200] + '...',\n",
        "                'category': hit['_source']['category'],\n",
        "                'cluster_labels': hit['_source'].get('cluster_labels', {})\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro na busca: {e}\")\n",
        "        return None\n",
        "\n",
        "# Configurar Elasticsearch\n",
        "print(\"üîé CONFIGURA√á√ÉO DO ELASTICSEARCH\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "es = setup_elasticsearch()\n",
        "\n",
        "if es is not None:\n",
        "    # Criar index\n",
        "    if create_embeddings_index(es):\n",
        "        # Indexar dados\n",
        "        index_embeddings(es, df, embeddings_dict, clustering_results)\n",
        "        \n",
        "        # Testar busca sem√¢ntica\n",
        "        print(\"\\nüîç TESTANDO BUSCA SEM√ÇNTICA\")\n",
        "        print(\"=\" * 30)\n",
        "        \n",
        "        test_queries = [\n",
        "            \"computer graphics and design\",\n",
        "            \"car engine problems\",\n",
        "            \"medical research study\",\n",
        "            \"religious beliefs and faith\"\n",
        "        ]\n",
        "        \n",
        "        for query in test_queries:\n",
        "            print(f\"\\nüîç Query: '{query}'\")\n",
        "            results = search_similar_documents(es, query, embedding_type='sbert', top_k=3)\n",
        "            \n",
        "            if results:\n",
        "                for i, result in enumerate(results, 1):\n",
        "                    print(f\"   {i}. [{result['category']}] {result['text']}\")\n",
        "                    print(f\"      Score: {result['score']:.3f}\")\n",
        "            else:\n",
        "                print(\"   Nenhum resultado encontrado\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Elasticsearch n√£o dispon√≠vel. Pulando integra√ß√£o.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Aplica√ß√µes Pr√°ticas: Upload de Textos e Classifica√ß√£o\n",
        "\n",
        "### **Sistema de Upload e Classifica√ß√£o**\n",
        "Vamos criar um sistema que permite:\n",
        "1. **Upload de textos personalizados**\n",
        "2. **Classifica√ß√£o autom√°tica** nos clusters existentes\n",
        "3. **An√°lise de similaridade** com documentos conhecidos\n",
        "4. **Identifica√ß√£o de outliers** (textos que n√£o se encaixam bem)\n",
        "\n",
        "### **Funcionalidades do Sistema**\n",
        "- **Classifica√ß√£o**: Determinar qual cluster o texto pertence\n",
        "- **Similaridade**: Encontrar documentos mais similares\n",
        "- **Confian√ßa**: Medir qu√£o confi√°vel √© a classifica√ß√£o\n",
        "- **Outlier Detection**: Identificar textos at√≠picos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Sistema de Upload e Classifica√ß√£o de Textos\n",
        "class TextClassifier:\n",
        "    \"\"\"Sistema de classifica√ß√£o de textos usando embeddings e clustering\"\"\"\n",
        "    \n",
        "    def __init__(self, embeddings_dict, clustering_results, df):\n",
        "        self.embeddings_dict = embeddings_dict\n",
        "        self.clustering_results = clustering_results\n",
        "        self.df = df\n",
        "        self.best_combination = self._find_best_combination()\n",
        "        \n",
        "    def _find_best_combination(self):\n",
        "        \"\"\"Encontra a melhor combina√ß√£o de embedding + algoritmo\"\"\"\n",
        "        best_ari = -1\n",
        "        best_combination = None\n",
        "        \n",
        "        for name, data in self.clustering_results.items():\n",
        "            if data['results']['ari'] > best_ari:\n",
        "                best_ari = data['results']['ari']\n",
        "                best_combination = name\n",
        "        \n",
        "        return best_combination\n",
        "    \n",
        "    def classify_text(self, text, embedding_type=None, algorithm=None):\n",
        "        \"\"\"Classifica um texto nos clusters existentes\"\"\"\n",
        "        \n",
        "        if embedding_type is None:\n",
        "            embedding_type = self.best_combination.split('_')[0]\n",
        "        \n",
        "        if algorithm is None:\n",
        "            algorithm = self.best_combination.split('_')[1]\n",
        "        \n",
        "        # Gerar embedding do texto\n",
        "        if embedding_type == 'Word2Vec':\n",
        "            text_embedding = get_document_embeddings_word2vec(word2vec_model, [text])[0]\n",
        "        elif embedding_type == 'BERT':\n",
        "            text_embedding = generate_bert_embeddings([text])[0]\n",
        "        elif embedding_type == 'Sentence-BERT':\n",
        "            text_embedding = generate_sbert_embeddings([text])[0]\n",
        "        elif embedding_type == 'OpenAI' and openai_embeddings is not None:\n",
        "            text_embedding = generate_openai_embeddings([text])[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Tipo de embedding n√£o suportado: {embedding_type}\")\n",
        "        \n",
        "        # Encontrar cluster mais pr√≥ximo\n",
        "        combination_name = f\"{embedding_type}_{algorithm}\"\n",
        "        if combination_name not in self.clustering_results:\n",
        "            raise ValueError(f\"Combina√ß√£o n√£o encontrada: {combination_name}\")\n",
        "        \n",
        "        # Usar centroides dos clusters para classifica√ß√£o\n",
        "        cluster_labels = self.clustering_results[combination_name]['labels']\n",
        "        unique_clusters = np.unique(cluster_labels[cluster_labels != -1])  # Excluir outliers\n",
        "        \n",
        "        # Calcular dist√¢ncias para cada cluster\n",
        "        distances = []\n",
        "        for cluster_id in unique_clusters:\n",
        "            cluster_mask = cluster_labels == cluster_id\n",
        "            cluster_embeddings = self.embeddings_dict[embedding_type][cluster_mask]\n",
        "            cluster_center = np.mean(cluster_embeddings, axis=0)\n",
        "            \n",
        "            # Dist√¢ncia euclidiana\n",
        "            distance = np.linalg.norm(text_embedding - cluster_center)\n",
        "            distances.append((cluster_id, distance))\n",
        "        \n",
        "        # Ordenar por dist√¢ncia\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        predicted_cluster = distances[0][0]\n",
        "        confidence = 1 / (1 + distances[0][1])  # Confian√ßa baseada na dist√¢ncia\n",
        "        \n",
        "        # Encontrar documentos mais similares\n",
        "        similar_docs = self._find_similar_documents(text_embedding, embedding_type, top_k=5)\n",
        "        \n",
        "        return {\n",
        "            'predicted_cluster': int(predicted_cluster),\n",
        "            'confidence': float(confidence),\n",
        "            'distances': distances[:5],  # Top 5 clusters\n",
        "            'similar_documents': similar_docs\n",
        "        }\n",
        "    \n",
        "    def _find_similar_documents(self, text_embedding, embedding_type, top_k=5):\n",
        "        \"\"\"Encontra documentos mais similares\"\"\"\n",
        "        \n",
        "        # Calcular similaridade com todos os documentos\n",
        "        all_embeddings = self.embeddings_dict[embedding_type]\n",
        "        similarities = []\n",
        "        \n",
        "        for i, doc_embedding in enumerate(all_embeddings):\n",
        "            # Similaridade cosseno\n",
        "            similarity = np.dot(text_embedding, doc_embedding) / (\n",
        "                np.linalg.norm(text_embedding) * np.linalg.norm(doc_embedding)\n",
        "            )\n",
        "            similarities.append((i, similarity))\n",
        "        \n",
        "        # Ordenar por similaridade\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Retornar top_k documentos\n",
        "        similar_docs = []\n",
        "        for i, (doc_idx, similarity) in enumerate(similarities[:top_k]):\n",
        "            similar_docs.append({\n",
        "                'index': doc_idx,\n",
        "                'text': self.df.iloc[doc_idx]['text'][:200] + '...',\n",
        "                'category': self.df.iloc[doc_idx]['category'],\n",
        "                'similarity': float(similarity)\n",
        "            })\n",
        "        \n",
        "        return similar_docs\n",
        "    \n",
        "    def detect_outlier(self, text, threshold=0.3):\n",
        "        \"\"\"Detecta se um texto √© outlier\"\"\"\n",
        "        \n",
        "        classification = self.classify_text(text)\n",
        "        return classification['confidence'] < threshold\n",
        "\n",
        "# Criar sistema de classifica√ß√£o\n",
        "print(\"üöÄ SISTEMA DE CLASSIFICA√á√ÉO DE TEXTOS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "classifier = TextClassifier(embeddings_dict, clustering_results, df)\n",
        "\n",
        "print(f\"‚úÖ Sistema criado com melhor combina√ß√£o: {classifier.best_combination}\")\n",
        "\n",
        "# Testar com textos de exemplo\n",
        "test_texts = [\n",
        "    \"I'm having trouble with my computer graphics card. The display is flickering and showing artifacts.\",\n",
        "    \"My car engine is making strange noises and the check engine light is on.\",\n",
        "    \"I'm looking for information about a new medical study on diabetes treatment.\",\n",
        "    \"I want to discuss my religious beliefs and how they influence my daily life.\",\n",
        "    \"This is completely unrelated to any of the topics we've seen before. Random text about cooking pasta.\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüîç TESTANDO CLASSIFICA√á√ÉO COM TEXTOS DE EXEMPLO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\nüìù Texto {i}: {text[:100]}...\")\n",
        "    \n",
        "    try:\n",
        "        result = classifier.classify_text(text)\n",
        "        is_outlier = classifier.detect_outlier(text)\n",
        "        \n",
        "        print(f\"   üéØ Cluster previsto: {result['predicted_cluster']}\")\n",
        "        print(f\"   üìä Confian√ßa: {result['confidence']:.3f}\")\n",
        "        print(f\"   ‚ö†Ô∏è  Outlier: {'Sim' if is_outlier else 'N√£o'}\")\n",
        "        \n",
        "        print(f\"   üìö Documentos similares:\")\n",
        "        for j, similar in enumerate(result['similar_documents'][:3], 1):\n",
        "            print(f\"      {j}. [{similar['category']}] {similar['text']}\")\n",
        "            print(f\"         Similaridade: {similar['similarity']:.3f}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Erro na classifica√ß√£o: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Sistema de classifica√ß√£o testado com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Interpretabilidade e An√°lise de Clusters\n",
        "\n",
        "### **Por que Interpretabilidade √© Importante?**\n",
        "- **Confian√ßa**: Entender por que o modelo fez certas decis√µes\n",
        "- **Debugging**: Identificar problemas no clustering\n",
        "- **Valida√ß√£o**: Verificar se os clusters fazem sentido\n",
        "- **Melhoria**: Ajustar par√¢metros baseado em insights\n",
        "\n",
        "### **T√©cnicas de Interpretabilidade**\n",
        "1. **An√°lise de palavras**: Palavras mais frequentes por cluster\n",
        "2. **Exemplos representativos**: Documentos t√≠picos de cada cluster\n",
        "3. **Visualiza√ß√£o**: Redu√ß√£o dimensional para inspe√ß√£o visual\n",
        "4. **M√©tricas de qualidade**: Silhouette, ARI, NMI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç An√°lise de Interpretabilidade dos Clusters\n",
        "def analyze_cluster_interpretability(df, clustering_results, best_combination, top_words=10):\n",
        "    \"\"\"Analisa interpretabilidade dos clusters\"\"\"\n",
        "    \n",
        "    print(\"üîç AN√ÅLISE DE INTERPRETABILIDADE DOS CLUSTERS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Obter labels do melhor clustering\n",
        "    best_labels = clustering_results[best_combination]['labels']\n",
        "    unique_clusters = np.unique(best_labels[best_labels != -1])\n",
        "    \n",
        "    print(f\"üìä Analisando {len(unique_clusters)} clusters do melhor resultado: {best_combination}\")\n",
        "    \n",
        "    # An√°lise por cluster\n",
        "    for cluster_id in unique_clusters:\n",
        "        print(f\"\\nüîπ CLUSTER {cluster_id}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Documentos do cluster\n",
        "        cluster_mask = best_labels == cluster_id\n",
        "        cluster_docs = df[cluster_mask]\n",
        "        \n",
        "        print(f\"üìä Tamanho: {len(cluster_docs)} documentos\")\n",
        "        \n",
        "        # Distribui√ß√£o de classes reais\n",
        "        class_distribution = cluster_docs['category'].value_counts()\n",
        "        print(f\"üè∑Ô∏è  Classes reais:\")\n",
        "        for category, count in class_distribution.head(3).items():\n",
        "            percentage = (count / len(cluster_docs)) * 100\n",
        "            print(f\"   {category}: {count} ({percentage:.1f}%)\")\n",
        "        \n",
        "        # Palavras mais frequentes\n",
        "        print(f\"üìù Palavras mais frequentes:\")\n",
        "        all_text = ' '.join(cluster_docs['text'].str.lower())\n",
        "        words = re.findall(r'\\b\\w+\\b', all_text)\n",
        "        word_counts = Counter(words)\n",
        "        \n",
        "        # Filtrar palavras comuns\n",
        "        common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'a', 'an', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "        \n",
        "        filtered_words = {word: count for word, count in word_counts.items() \n",
        "                         if word not in common_words and len(word) > 3}\n",
        "        \n",
        "        for word, count in Counter(filtered_words).most_common(top_words):\n",
        "            print(f\"   {word}: {count}\")\n",
        "        \n",
        "        # Exemplos representativos\n",
        "        print(f\"üìÑ Exemplos representativos:\")\n",
        "        for i, (_, doc) in enumerate(cluster_docs.head(2).iterrows()):\n",
        "            print(f\"   {i+1}. {doc['text'][:150]}...\")\n",
        "            print(f\"      Categoria real: {doc['category']}\")\n",
        "\n",
        "def create_cluster_summary_table(df, clustering_results, best_combination):\n",
        "    \"\"\"Cria tabela resumo dos clusters\"\"\"\n",
        "    \n",
        "    best_labels = clustering_results[best_combination]['labels']\n",
        "    unique_clusters = np.unique(best_labels[best_labels != -1])\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for cluster_id in unique_clusters:\n",
        "        cluster_mask = best_labels == cluster_id\n",
        "        cluster_docs = df[cluster_mask]\n",
        "        \n",
        "        # Classe dominante\n",
        "        dominant_class = cluster_docs['category'].mode().iloc[0]\n",
        "        dominant_count = cluster_docs['category'].value_counts().iloc[0]\n",
        "        purity = dominant_count / len(cluster_docs)\n",
        "        \n",
        "        summary_data.append({\n",
        "            'Cluster': cluster_id,\n",
        "            'Size': len(cluster_docs),\n",
        "            'Dominant_Class': dominant_class,\n",
        "            'Purity': purity,\n",
        "            'Avg_Text_Length': cluster_docs['text'].str.len().mean()\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(summary_data)\n",
        "\n",
        "# Executar an√°lise de interpretabilidade\n",
        "print(\"üîç EXECUTANDO AN√ÅLISE DE INTERPRETABILIDADE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Encontrar a melhor combina√ß√£o de embedding + algoritmo\n",
        "best_ari = -1\n",
        "best_combination = None\n",
        "for name, data in clustering_results.items():\n",
        "    if data['results']['ari'] > best_ari:\n",
        "        best_ari = data['results']['ari']\n",
        "        best_combination = name\n",
        "\n",
        "print(f\"üìä Melhor combina√ß√£o encontrada: {best_combination} (ARI: {best_ari:.3f})\")\n",
        "\n",
        "# Analisar clusters\n",
        "analyze_cluster_interpretability(df, clustering_results, best_combination)\n",
        "\n",
        "# Criar tabela resumo\n",
        "print(f\"\\nüìã TABELA RESUMO DOS CLUSTERS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "summary_table = create_cluster_summary_table(df, clustering_results, best_combination)\n",
        "print(summary_table.to_string(index=False))\n",
        "\n",
        "# Visualizar distribui√ß√£o de pureza\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(summary_table['Cluster'], summary_table['Purity'], color='skyblue', alpha=0.7)\n",
        "plt.title('Pureza dos Clusters')\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('Pureza')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(summary_table['Cluster'], summary_table['Size'], color='lightcoral', alpha=0.7)\n",
        "plt.title('Tamanho dos Clusters')\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('N√∫mero de Documentos')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ An√°lise de interpretabilidade conclu√≠da!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Conclus√µes e Li√ß√µes Aprendidas\n",
        "\n",
        "### **Principais Descobertas**\n",
        "\n",
        "#### **1. Evolu√ß√£o dos Embeddings**\n",
        "- **Word2Vec**: Boa baseline, mas limitado por n√£o capturar contexto\n",
        "- **BERT**: Melhora significativa com contextualiza√ß√£o bidirecional\n",
        "- **Sentence-BERT**: Otimizado para similaridade, excelente para clustering\n",
        "- **OpenAI**: Qualidade superior, mas requer API e tem custos\n",
        "\n",
        "#### **2. Algoritmos de Clustering**\n",
        "- **K-Means**: Simples e eficaz para clusters esf√©ricos\n",
        "- **DBSCAN**: Excelente para detectar outliers e clusters de formas variadas\n",
        "- **HDBSCAN**: Mais robusto, mas computacionalmente mais caro\n",
        "\n",
        "#### **3. M√©tricas de Avalia√ß√£o**\n",
        "- **ARI**: Melhor m√©trica para comparar com ground truth\n",
        "- **Silhouette**: √ötil para avaliar qualidade interna dos clusters\n",
        "- **Homogeneity**: Importante para verificar pureza dos clusters\n",
        "\n",
        "### **Insights Pr√°ticos**\n",
        "\n",
        "#### **Para Clustering de Textos:**\n",
        "1. **Sentence-BERT** geralmente oferece o melhor custo-benef√≠cio\n",
        "2. **OpenAI Embeddings** s√£o superiores quando or√ßamento permite\n",
        "3. **DBSCAN** √© ideal quando voc√™ n√£o sabe o n√∫mero de clusters\n",
        "4. **K-Means** funciona bem quando clusters s√£o bem separados\n",
        "\n",
        "#### **Para Aplica√ß√µes Reais:**\n",
        "1. **Sempre valide** com m√©tricas externas quando poss√≠vel\n",
        "2. **Visualize** os resultados para entender a estrutura dos dados\n",
        "3. **Teste diferentes** combina√ß√µes de embedding + algoritmo\n",
        "4. **Considere o contexto** da aplica√ß√£o para escolher m√©tricas\n",
        "\n",
        "### **Pr√≥ximos Passos**\n",
        "\n",
        "#### **Melhorias Poss√≠veis:**\n",
        "1. **Fine-tuning**: Treinar modelos espec√≠ficos para seu dom√≠nio\n",
        "2. **Ensemble**: Combinar m√∫ltiplos tipos de embeddings\n",
        "3. **Otimiza√ß√£o**: Ajustar hiperpar√¢metros dos algoritmos\n",
        "4. **Valida√ß√£o**: Usar valida√ß√£o cruzada para estimar generaliza√ß√£o\n",
        "\n",
        "#### **Aplica√ß√µes Avan√ßadas:**\n",
        "1. **Sistemas de recomenda√ß√£o** baseados em similaridade sem√¢ntica\n",
        "2. **Detec√ß√£o de anomalias** em tempo real\n",
        "3. **An√°lise de sentimento** por cluster\n",
        "4. **Busca sem√¢ntica** em grandes corpora\n",
        "\n",
        "### **Recursos Adicionais**\n",
        "\n",
        "#### **Documenta√ß√£o:**\n",
        "- [Sentence Transformers](https://www.sbert.net/)\n",
        "- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [Elasticsearch KNN](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html)\n",
        "- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
        "\n",
        "#### **Datasets para Pr√°tica:**\n",
        "- [20 Newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n",
        "- [AG News](https://huggingface.co/datasets/ag_news)\n",
        "- [IMDB Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "---\n",
        "\n",
        "**üéì Parab√©ns!** Voc√™ completou uma jornada completa atrav√©s do mundo dos embeddings e clustering. Agora voc√™ tem as ferramentas e conhecimentos para aplicar essas t√©cnicas em seus pr√≥prios projetos!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
