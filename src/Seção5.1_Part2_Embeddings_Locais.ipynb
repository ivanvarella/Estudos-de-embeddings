{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ Se√ß√£o 5.1 ‚Äì Parte 2: Embeddings Locais\n",
    "\n",
    "**Objetivo:** Gerar e analisar embeddings cl√°ssicos e modernos (TF-IDF, Word2Vec, BERT, Sentence-BERT) usando o dataset preparado no Notebook 1.\n",
    "\n",
    "## üìã Conte√∫do deste Notebook\n",
    "\n",
    "1. **Carregamento de Dados**: Obter dataset diretamente do Elasticsearch\n",
    "2. **TF-IDF**: Embeddings baseados em frequ√™ncia de termos\n",
    "3. **Word2Vec**: Embeddings contextuais cl√°ssicos\n",
    "4. **BERT**: Embeddings bidirecionais modernos\n",
    "5. **Sentence-BERT**: Otimizado para similaridade de senten√ßas\n",
    "6. **An√°lise Detalhada**: Compara√ß√£o de caracter√≠sticas de cada tipo\n",
    "\n",
    "## üîó Sequ√™ncia dos Notebooks\n",
    "\n",
    "- **Notebook 1**: Prepara√ß√£o e Dataset ‚úÖ\n",
    "- **Notebook 2** (atual): Embeddings Locais üîÑ\n",
    "- **Notebook 3**: Embeddings OpenAI\n",
    "- **Notebook 4**: An√°lise Comparativa dos Embeddings\n",
    "- **Notebook 5**: Clustering e Machine Learning\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANTE: Carregamento de Dados\n",
    "\n",
    "Este notebook **SEMPRE carrega os dados do Elasticsearch** (preparados no Notebook 1), garantindo:\n",
    "- ‚úÖ Consist√™ncia entre notebooks\n",
    "- ‚úÖ Mesmos IDs √∫nicos (doc_0000, doc_0001, ...)\n",
    "- ‚úÖ Rastreabilidade completa\n",
    "- ‚úÖ Integridade dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configura√ß√£o do Ambiente\n",
    "\n",
    "Este notebook carrega as configura√ß√µes do arquivo `setup/.env` e conecta ao Elasticsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ python-dotenv dispon√≠vel\n",
      "‚úÖ Arquivo .env carregado: /Users/ivanvarella/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGEÃÇNCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUCÃßAÃÉO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/src/setup/.env\n",
      "\n",
      "üîß Configura√ß√µes carregadas!\n",
      "   ELASTICSEARCH: localhost:9200\n",
      "   CLUSTERING_RANDOM_STATE: 42\n"
     ]
    }
   ],
   "source": [
    "# üîß Configura√ß√£o de Vari√°veis de Ambiente\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Carregar python-dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"‚úÖ python-dotenv dispon√≠vel\")\n",
    "    \n",
    "    env_paths = [\n",
    "        Path.cwd() / 'setup' / '.env',\n",
    "        Path.cwd() / '.env',\n",
    "        Path.cwd() / 'setup' / 'config_example.env'\n",
    "    ]\n",
    "    \n",
    "    env_loaded = False\n",
    "    for env_path in env_paths:\n",
    "        if env_path.exists():\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"‚úÖ Arquivo .env carregado: {env_path}\")\n",
    "            env_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not env_loaded:\n",
    "        print(\"‚ö†Ô∏è  Nenhum arquivo .env encontrado\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv n√£o instalado\")\n",
    "\n",
    "# Carregar configura√ß√µes (otimizadas para 20 classes)\n",
    "MAX_CHARS_PER_REQUEST = int(os.getenv('MAX_CHARS_PER_REQUEST', 32000))\n",
    "BATCH_SIZE_SMALL_TEXTS = int(os.getenv('BATCH_SIZE_SMALL_TEXTS', 4))\n",
    "BATCH_SIZE_MEDIUM_TEXTS = int(os.getenv('BATCH_SIZE_MEDIUM_TEXTS', 2))\n",
    "BATCH_SIZE_LARGE_TEXTS = int(os.getenv('BATCH_SIZE_LARGE_TEXTS', 1))\n",
    "DATASET_SIZE = int(os.getenv('DATASET_SIZE', 20000))\n",
    "TEXT_MIN_LENGTH = int(os.getenv('TEXT_MIN_LENGTH', 20))\n",
    "MAX_CLUSTERS = int(os.getenv('MAX_CLUSTERS', 20))\n",
    "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', 42))\n",
    "ELASTICSEARCH_HOST = os.getenv('ELASTICSEARCH_HOST', 'localhost')\n",
    "ELASTICSEARCH_PORT = int(os.getenv('ELASTICSEARCH_PORT', 9200))\n",
    "\n",
    "print(f\"\\nüîß Configura√ß√µes carregadas!\")\n",
    "print(f\"   ELASTICSEARCH: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}\")\n",
    "print(f\"   CLUSTERING_RANDOM_STATE: {CLUSTERING_RANDOM_STATE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CARREGANDO IMPORTS\n",
      "========================================\n",
      "‚úÖ Imports b√°sicos carregados\n",
      "‚úÖ Configura√ß√µes aplicadas\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Imports Essenciais\n",
    "print(\"üöÄ CARREGANDO IMPORTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"‚úÖ Imports b√°sicos carregados\")\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Configura√ß√µes aplicadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Carregamento de Dados\n",
    "\n",
    "Este notebook carrega o dataset do Elasticsearch (salvo no Notebook 1) para garantir consist√™ncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è INICIALIZANDO ELASTICSEARCH\n",
      "============================================================\n",
      "‚úÖ M√≥dulo de cache carregado\n",
      "\n",
      "üîå Conectando...\n",
      "‚úÖ Conectado ao Elasticsearch (localhost:9200)\n",
      "‚úÖ Conectado ao Elasticsearch!\n",
      "üìä Total de documentos no cache: 109,266\n",
      "\n",
      "üéØ STATUS: ‚úÖ Cache ativo\n"
     ]
    }
   ],
   "source": [
    "# üóÑÔ∏è Inicializar Elasticsearch e Carregar Dataset\n",
    "print(\"üóÑÔ∏è INICIALIZANDO ELASTICSEARCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Importar m√≥dulo de cache\n",
    "try:\n",
    "    from elasticsearch_manager import (\n",
    "        init_elasticsearch_cache, get_cache_status,\n",
    "        save_embeddings_to_cache, load_embeddings_from_cache, \n",
    "        check_embeddings_in_cache\n",
    "    )\n",
    "    print(\"‚úÖ M√≥dulo de cache carregado\")\n",
    "    CACHE_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao carregar m√≥dulo: {e}\")\n",
    "    CACHE_AVAILABLE = False\n",
    "\n",
    "# Conectar ao Elasticsearch\n",
    "if CACHE_AVAILABLE:\n",
    "    print(\"\\nüîå Conectando...\")\n",
    "    cache_connected = init_elasticsearch_cache(\n",
    "        host=ELASTICSEARCH_HOST,\n",
    "        port=ELASTICSEARCH_PORT\n",
    "    )\n",
    "    \n",
    "    if cache_connected:\n",
    "        print(\"‚úÖ Conectado ao Elasticsearch!\")\n",
    "        status = get_cache_status()\n",
    "        if status.get(\"connected\"):\n",
    "            print(f\"üìä Total de documentos no cache: {status.get('total_docs', 0):,}\")\n",
    "    else:\n",
    "        print(\"‚ùå Falha na conex√£o\")\n",
    "        CACHE_AVAILABLE = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cache n√£o dispon√≠vel\")\n",
    "    cache_connected = False\n",
    "\n",
    "print(f\"\\nüéØ STATUS: {'‚úÖ Cache ativo' if CACHE_AVAILABLE and cache_connected else '‚ùå Cache inativo'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CARREGANDO DATASET DO ELASTICSEARCH\n",
      "============================================================\n",
      "‚ö†Ô∏è  IMPORTANTE: Carregando dados salvos no Notebook 1\n",
      "             N√ÉO recriando o dataset!\n",
      "üîÑ Buscando documentos do √≠ndice 'documents_dataset'\n",
      "   M√©todo: Scroll API (recomendado para >10k docs)\n",
      "   Tamanho do lote: 1,000 documentos\n",
      "   Timeout do scroll: 2m\n",
      "\n",
      "üìä Total de documentos dispon√≠veis: 18,211\n",
      "üîÑ Iniciando busca em lotes...\n",
      "   Lote 1: 1,000 docs | Total acumulado: 1,000/18,211\n",
      "   Lote 2: 1,000 docs | Total acumulado: 2,000/18,211\n",
      "   Lote 3: 1,000 docs | Total acumulado: 3,000/18,211\n",
      "   Lote 4: 1,000 docs | Total acumulado: 4,000/18,211\n",
      "   Lote 5: 1,000 docs | Total acumulado: 5,000/18,211\n",
      "   Lote 6: 1,000 docs | Total acumulado: 6,000/18,211\n",
      "   Lote 7: 1,000 docs | Total acumulado: 7,000/18,211\n",
      "   Lote 8: 1,000 docs | Total acumulado: 8,000/18,211\n",
      "   Lote 9: 1,000 docs | Total acumulado: 9,000/18,211\n",
      "   Lote 10: 1,000 docs | Total acumulado: 10,000/18,211\n",
      "   Lote 11: 1,000 docs | Total acumulado: 11,000/18,211\n",
      "   Lote 12: 1,000 docs | Total acumulado: 12,000/18,211\n",
      "   Lote 13: 1,000 docs | Total acumulado: 13,000/18,211\n",
      "   Lote 14: 1,000 docs | Total acumulado: 14,000/18,211\n",
      "   Lote 15: 1,000 docs | Total acumulado: 15,000/18,211\n",
      "   Lote 16: 1,000 docs | Total acumulado: 16,000/18,211\n",
      "   Lote 17: 1,000 docs | Total acumulado: 17,000/18,211\n",
      "   Lote 18: 1,000 docs | Total acumulado: 18,000/18,211\n",
      "   Lote 19: 211 docs | Total acumulado: 18,211/18,211\n",
      "\n",
      "‚úÖ Scroll conclu√≠do e recursos liberados\n",
      "\n",
      "üìä Processando 18,211 documentos em DataFrame...\n",
      "‚úÖ DataFrame criado com sucesso!\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATASET CARREGADO COM SUCESSO!\n",
      "============================================================\n",
      "üìä Shape: (18211, 4)\n",
      "üìã Colunas: ['doc_id', 'text', 'category', 'target']\n",
      "üè∑Ô∏è  Classes √∫nicas: 20\n",
      "üî¢ Total de documentos: 18,211\n",
      "üîë IDs (amostra): ['doc_0000', 'doc_0001', 'doc_0002'] ... ['doc_9997', 'doc_9998', 'doc_9999']\n",
      "\n",
      "üîç VALIDA√á√ÉO:\n",
      "   ‚úÖ PASSOU: 18,211 documentos\n",
      "   ‚úÖ Dentro da expectativa: ~18,000 ¬±1,000\n"
     ]
    }
   ],
   "source": [
    "# üìä CARREGAR DATASET DO ELASTICSEARCH\n",
    "print(\"üìä CARREGANDO DATASET DO ELASTICSEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  IMPORTANTE: Carregando dados salvos no Notebook 1\")\n",
    "print(\"             N√ÉO recriando o dataset!\")\n",
    "\n",
    "# Executar carregamento\n",
    "if CACHE_AVAILABLE and cache_connected:\n",
    "    try:\n",
    "        from elasticsearch import Elasticsearch\n",
    "        from elasticsearch_helpers import load_all_documents_from_elasticsearch, print_dataframe_summary\n",
    "        \n",
    "        # Conectar ao Elasticsearch\n",
    "        es = Elasticsearch([{\n",
    "            'host': ELASTICSEARCH_HOST, \n",
    "            'port': ELASTICSEARCH_PORT, \n",
    "            'scheme': 'http'\n",
    "        }])\n",
    "        \n",
    "        # Carregar TODOS os documentos usando Scroll API\n",
    "        # Esta fun√ß√£o est√° em elasticsearch_helpers.py e usa Scroll API\n",
    "        # para buscar TODOS os documentos, mesmo que sejam >10.000\n",
    "        df = load_all_documents_from_elasticsearch(\n",
    "            es_client=es,\n",
    "            index_name=\"documents_dataset\",\n",
    "            batch_size=1000,      # Docs por lote\n",
    "            scroll_timeout='2m',  # Tempo de contexto\n",
    "            verbose=True          # Mostrar progresso\n",
    "        )\n",
    "        \n",
    "        # Gerar lista de doc_ids para uso posterior\n",
    "        doc_ids = df['doc_id'].tolist()\n",
    "        \n",
    "        # Exibir resumo detalhado\n",
    "        print_dataframe_summary(df, expected_docs=18000)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERRO CR√çTICO ao carregar dataset: {e}\")\n",
    "        print(\"üí° Poss√≠veis causas:\")\n",
    "        print(\"   1. Notebook 1 n√£o foi executado\")\n",
    "        print(\"   2. Elasticsearch n√£o est√° rodando\")\n",
    "        print(\"   3. √çndice 'documents_dataset' n√£o existe\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n‚ùå ERRO: Elasticsearch n√£o dispon√≠vel!\")\n",
    "    print(\"üí° Verifique:\")\n",
    "    print(\"   1. Docker est√° rodando: docker ps\")\n",
    "    print(\"   2. Elasticsearch ativo: http://localhost:9200\")\n",
    "    print(\"   3. Execute o Notebook 1 primeiro\")\n",
    "    raise RuntimeError(\"Elasticsearch n√£o dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Embeddings Cl√°ssicos: TF-IDF\n",
    "\n",
    "### **O que √© TF-IDF?**\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) √© um m√©todo cl√°ssico que pondera a import√¢ncia de palavras:\n",
    "\n",
    "- **TF (Term Frequency)**: Frequ√™ncia do termo no documento\n",
    "- **IDF (Inverse Document Frequency)**: Raridade do termo no corpus\n",
    "- **F√≥rmula**: TF-IDF = TF √ó log(N/DF)\n",
    "  - N = n√∫mero total de documentos\n",
    "  - DF = n√∫mero de documentos contendo o termo\n",
    "\n",
    "### **Caracter√≠sticas**\n",
    "\n",
    "- ‚úÖ Simples e interpret√°vel\n",
    "- ‚úÖ R√°pido para calcular\n",
    "- ‚úÖ Baseline s√≥lido\n",
    "- ‚ùå Matriz esparsa (muitos zeros)\n",
    "- ‚ùå N√£o captura contexto sem√¢ntico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "# es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "# if es.indices.exists(index='embeddings_tfidf'):\n",
    "#     es.indices.delete(index='embeddings_tfidf')\n",
    "#     print('‚úÖ √çndice embeddings_tfidf deletado')\n",
    "# else:\n",
    "#     print('‚ÑπÔ∏è  √çndice n√£o existe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ GERANDO EMBEDDINGS TF-IDF\n",
      "============================================================\n",
      "‚úÖ TF-IDF j√° existe no cache, carregando...\n",
      "‚úÖ Embeddings carregados: (18211, 4096) de 'embeddings_tfidf'\n",
      "‚úÖ TF-IDF carregado: (18211, 4096)\n",
      "\n",
      "üìä TF-IDF pronto: (18211, 4096)\n"
     ]
    }
   ],
   "source": [
    "# üßÆ Gerar Embeddings TF-IDF\n",
    "print(\"üßÆ GERANDO EMBEDDINGS TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar se j√° existe no cache\n",
    "use_cache = os.getenv('USE_ELASTICSEARCH_CACHE', 'true').lower() == 'true'\n",
    "force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
    "\n",
    "if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "    all_exist, existing, missing = check_embeddings_in_cache('embeddings_tfidf', doc_ids)\n",
    "    \n",
    "    if all_exist:\n",
    "        print(\"‚úÖ TF-IDF j√° existe no cache, carregando...\")\n",
    "        tfidf_embeddings = load_embeddings_from_cache('embeddings_tfidf', doc_ids)\n",
    "        if tfidf_embeddings is not None:\n",
    "            print(f\"‚úÖ TF-IDF carregado: {tfidf_embeddings.shape}\")\n",
    "            # Criar vectorizer vazio (ser√° usado para an√°lise)\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_features=4096, max_df=0.95, min_df=2)\n",
    "            tfidf_vectorizer.fit(df['text'])\n",
    "        else:\n",
    "            print(\"‚ùå Falha ao carregar, regenerando...\")\n",
    "            force_regenerate = True\n",
    "\n",
    "if not use_cache or force_regenerate or not all_exist or tfidf_embeddings is None:\n",
    "    print(\"üîÑ Gerando TF-IDF...\")\n",
    "    \n",
    "    # Criar vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=4096,  # Limitar a 4096 features (m√°ximo do Elasticsearch)\n",
    "        max_df=0.95,        # Ignorar termos muito frequentes\n",
    "        min_df=2,           # Ignorar termos muito raros\n",
    "        ngram_range=(1, 2)  # Unigramas e bigramas\n",
    "    )\n",
    "    \n",
    "    # Gerar embeddings\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "    tfidf_embeddings = tfidf_matrix.toarray()\n",
    "    \n",
    "    print(f\"‚úÖ TF-IDF gerado: {tfidf_embeddings.shape}\")\n",
    "    print(f\"   Vocabul√°rio: {len(tfidf_vectorizer.vocabulary_):,} termos\")\n",
    "    print(f\"   Densidade: {(tfidf_embeddings != 0).mean():.3f}\")\n",
    "    \n",
    "    # Salvar no cache\n",
    "    if use_cache and CACHE_AVAILABLE:\n",
    "        print(\"üíæ Salvando no Elasticsearch...\")\n",
    "        save_embeddings_to_cache(\n",
    "            'embeddings_tfidf', \n",
    "            tfidf_embeddings, \n",
    "            doc_ids, \n",
    "            df['text'].tolist(), \n",
    "            'tfidf'\n",
    "        )\n",
    "\n",
    "print(f\"\\nüìä TF-IDF pronto: {tfidf_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Word2Vec: Embeddings Contextuais\n",
    "\n",
    "### **O que √© Word2Vec?**\n",
    "\n",
    "**Word2Vec** (2013) foi revolucion√°rio ao capturar similaridade sem√¢ntica atrav√©s de contexto:\n",
    "\n",
    "- **Skip-gram**: Prediz palavras vizinhas dado uma palavra central\n",
    "- **CBOW**: Prediz palavra central dado contexto\n",
    "- **Janela deslizante**: Considera palavras pr√≥ximas\n",
    "- **Resultado**: Palavras similares ficam pr√≥ximas no espa√ßo vetorial\n",
    "\n",
    "### **Caracter√≠sticas**\n",
    "\n",
    "- ‚úÖ Captura similaridade sem√¢ntica\n",
    "- ‚úÖ Embeddings densos\n",
    "- ‚úÖ R√°pido ap√≥s treinamento\n",
    "- ‚ùå Palavras isoladas (n√£o considera ordem global)\n",
    "- ‚ùå Requer treinamento no corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö CARREGANDO BIBLIOTECAS DE EMBEDDINGS\n",
      "============================================================\n",
      "‚úÖ Gensim carregado\n",
      "‚úÖ Sentence Transformers carregado\n",
      "\n",
      "Status: Gensim=‚úÖ, Transformers=‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# üìö Carregar bibliotecas para Word2Vec, BERT e SBERT\n",
    "print(\"üìö CARREGANDO BIBLIOTECAS DE EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Gensim para Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    print(\"‚úÖ Gensim carregado\")\n",
    "    GENSIM_OK = True\n",
    "except:\n",
    "    print(\"‚ùå Gensim n√£o dispon√≠vel\")\n",
    "    GENSIM_OK = False\n",
    "\n",
    "# Sentence Transformers para BERT e SBERT\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ Sentence Transformers carregado\")\n",
    "    TRANSFORMERS_OK = True\n",
    "except:\n",
    "    print(\"‚ùå Sentence Transformers n√£o dispon√≠vel\")\n",
    "    TRANSFORMERS_OK = False\n",
    "\n",
    "print(f\"\\nStatus: Gensim={'‚úÖ' if GENSIM_OK else '‚ùå'}, Transformers={'‚úÖ' if TRANSFORMERS_OK else '‚ùå'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ GERANDO EMBEDDINGS WORD2VEC\n",
      "============================================================\n",
      "‚úÖ Word2Vec j√° existe, carregando...\n",
      "‚úÖ Embeddings carregados: (18211, 100) de 'embeddings_word2vec'\n",
      "‚úÖ Word2Vec carregado: (18211, 100)\n",
      "\n",
      "üìä Word2Vec pronto: (18211, 100)\n"
     ]
    }
   ],
   "source": [
    "# üßÆ Gerar Embeddings Word2Vec\n",
    "print(\"üßÆ GERANDO EMBEDDINGS WORD2VEC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not GENSIM_OK:\n",
    "    print(\"‚ùå Gensim n√£o dispon√≠vel, pulando Word2Vec\")\n",
    "    word2vec_embeddings = None\n",
    "else:\n",
    "    # Verificar cache\n",
    "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "        all_exist, _, _ = check_embeddings_in_cache('embeddings_word2vec', doc_ids)\n",
    "        if all_exist:\n",
    "            print(\"‚úÖ Word2Vec j√° existe, carregando...\")\n",
    "            word2vec_embeddings = load_embeddings_from_cache('embeddings_word2vec', doc_ids)\n",
    "            if word2vec_embeddings is not None:\n",
    "                print(f\"‚úÖ Word2Vec carregado: {word2vec_embeddings.shape}\")\n",
    "            else:\n",
    "                force_regenerate = True\n",
    "    \n",
    "    if not use_cache or force_regenerate or not all_exist or word2vec_embeddings is None:\n",
    "        print(\"üîÑ Treinando Word2Vec...\")\n",
    "        \n",
    "        # Tokenizar textos\n",
    "        tokenized_texts = [text.lower().split() for text in df['text']]\n",
    "        \n",
    "        # Treinar Word2Vec\n",
    "        w2v_model = Word2Vec(\n",
    "            sentences=tokenized_texts,\n",
    "            vector_size=100,\n",
    "            window=5,\n",
    "            min_count=2,\n",
    "            workers=4,\n",
    "            epochs=10,\n",
    "            seed=CLUSTERING_RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        # Gerar embeddings por documento (m√©dia dos vetores de palavras)\n",
    "        word2vec_embeddings = []\n",
    "        for tokens in tokenized_texts:\n",
    "            valid_vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "            if valid_vectors:\n",
    "                word2vec_embeddings.append(np.mean(valid_vectors, axis=0))\n",
    "            else:\n",
    "                word2vec_embeddings.append(np.zeros(100))\n",
    "        \n",
    "        word2vec_embeddings = np.array(word2vec_embeddings)\n",
    "        \n",
    "        print(f\"‚úÖ Word2Vec gerado: {word2vec_embeddings.shape}\")\n",
    "        print(f\"   Vocabul√°rio: {len(w2v_model.wv):,} palavras\")\n",
    "        \n",
    "        # Salvar no cache\n",
    "        if use_cache and CACHE_AVAILABLE:\n",
    "            print(\"üíæ Salvando no Elasticsearch...\")\n",
    "            save_embeddings_to_cache(\n",
    "                'embeddings_word2vec',\n",
    "                word2vec_embeddings,\n",
    "                doc_ids,\n",
    "                df['text'].tolist(),\n",
    "                'word2vec'\n",
    "            )\n",
    "\n",
    "if word2vec_embeddings is not None:\n",
    "    print(f\"\\nüìä Word2Vec pronto: {word2vec_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Embeddings Modernos: BERT e Sentence-BERT\n",
    "\n",
    "### **BERT (2018) - Contextualiza√ß√£o Bidirecional**\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- L√™ texto em ambas as dire√ß√µes simultaneamente\n",
    "- Attention mechanism\n",
    "- 768 dimens√µes (bert-base-uncased)\n",
    "- Contextualizado: mesma palavra, contextos diferentes\n",
    "\n",
    "### **Sentence-BERT (2019) - Otimizado para Similaridade**\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Baseado em BERT mas otimizado para similaridade\n",
    "- 384 dimens√µes (all-MiniLM-L6-v2)\n",
    "- Ideal para clustering e busca sem√¢ntica\n",
    "- Normalizado por padr√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ GERANDO EMBEDDINGS BERT E SENTENCE-BERT\n",
      "============================================================\n",
      "\n",
      "üìñ BERT (bert-base-uncased)...\n",
      "‚úÖ BERT j√° existe, carregando...\n",
      "‚úÖ Embeddings carregados: (18211, 768) de 'embeddings_bert'\n",
      "\n",
      "üìñ Sentence-BERT (all-MiniLM-L6-v2)...\n",
      "‚úÖ Sentence-BERT j√° existe, carregando...\n",
      "‚úÖ Embeddings carregados: (18211, 384) de 'embeddings_sbert'\n",
      "\n",
      "üìä RESUMO DOS EMBEDDINGS LOCAIS:\n",
      "   TF-IDF: (18211, 4096)\n",
      "   Word2Vec: (18211, 100)\n",
      "   BERT: (18211, 768)\n",
      "   Sentence-BERT: (18211, 384)\n"
     ]
    }
   ],
   "source": [
    "# üßÆ Gerar Embeddings BERT e Sentence-BERT\n",
    "print(\"üßÆ GERANDO EMBEDDINGS BERT E SENTENCE-BERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not TRANSFORMERS_OK:\n",
    "    print(\"‚ùå Sentence Transformers n√£o dispon√≠vel\")\n",
    "    bert_embeddings = None\n",
    "    sbert_embeddings = None\n",
    "else:\n",
    "    # BERT\n",
    "    print(\"\\nüìñ BERT (bert-base-uncased)...\")\n",
    "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "        all_exist, _, _ = check_embeddings_in_cache('embeddings_bert', doc_ids)\n",
    "        if all_exist:\n",
    "            print(\"‚úÖ BERT j√° existe, carregando...\")\n",
    "            bert_embeddings = load_embeddings_from_cache('embeddings_bert', doc_ids)\n",
    "            if bert_embeddings is None:\n",
    "                force_regenerate = True\n",
    "    \n",
    "    if not use_cache or force_regenerate or not all_exist or bert_embeddings is None:\n",
    "        print(\"üîÑ Gerando BERT...\")\n",
    "        bert_model = SentenceTransformer('bert-base-uncased')\n",
    "        bert_embeddings = bert_model.encode(\n",
    "            df['text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"‚úÖ BERT gerado: {bert_embeddings.shape}\")\n",
    "        \n",
    "        if use_cache and CACHE_AVAILABLE:\n",
    "            save_embeddings_to_cache(\n",
    "                'embeddings_bert',\n",
    "                bert_embeddings,\n",
    "                doc_ids,\n",
    "                df['text'].tolist(),\n",
    "                'bert'\n",
    "            )\n",
    "    \n",
    "    # Sentence-BERT\n",
    "    print(\"\\nüìñ Sentence-BERT (all-MiniLM-L6-v2)...\")\n",
    "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "        all_exist, _, _ = check_embeddings_in_cache('embeddings_sbert', doc_ids)\n",
    "        if all_exist:\n",
    "            print(\"‚úÖ Sentence-BERT j√° existe, carregando...\")\n",
    "            sbert_embeddings = load_embeddings_from_cache('embeddings_sbert', doc_ids)\n",
    "            if sbert_embeddings is None:\n",
    "                force_regenerate = True\n",
    "    \n",
    "    if not use_cache or force_regenerate or not all_exist or sbert_embeddings is None:\n",
    "        print(\"üîÑ Gerando Sentence-BERT...\")\n",
    "        sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        sbert_embeddings = sbert_model.encode(\n",
    "            df['text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"‚úÖ Sentence-BERT gerado: {sbert_embeddings.shape}\")\n",
    "        \n",
    "        if use_cache and CACHE_AVAILABLE:\n",
    "            save_embeddings_to_cache(\n",
    "                'embeddings_sbert',\n",
    "                sbert_embeddings,\n",
    "                doc_ids,\n",
    "                df['text'].tolist(),\n",
    "                'sbert'\n",
    "            )\n",
    "\n",
    "print(f\"\\nüìä RESUMO DOS EMBEDDINGS LOCAIS:\")\n",
    "print(f\"   TF-IDF: {tfidf_embeddings.shape if tfidf_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   Word2Vec: {word2vec_embeddings.shape if word2vec_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   BERT: {bert_embeddings.shape if bert_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   Sentence-BERT: {sbert_embeddings.shape if sbert_embeddings is not None else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Resumo e Pr√≥ximos Passos\n",
    "\n",
    "### **O que foi realizado neste notebook:**\n",
    "\n",
    "1. ‚úÖ **Dados carregados do Elasticsearch** - Consist√™ncia garantida\n",
    "2. ‚úÖ **TF-IDF** - Embeddings cl√°ssicos baseados em frequ√™ncia\n",
    "3. ‚úÖ **Word2Vec** - Embeddings contextuais treinados\n",
    "4. ‚úÖ **BERT** - Embeddings bidirecionais modernos\n",
    "5. ‚úÖ **Sentence-BERT** - Otimizado para similaridade\n",
    "\n",
    "### **Pr√≥ximo Notebook: Parte 3 - Embeddings OpenAI**\n",
    "\n",
    "No pr√≥ximo notebook:\n",
    "- Embeddings da API OpenAI (text-embedding-3-small)\n",
    "- Tratamento de textos longos (sem truncamento)\n",
    "- Economia de custos com cache inteligente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RESUMO FINAL - NOTEBOOK 2 COMPLETO\n",
      "============================================================\n",
      "‚úÖ Dataset: 18,211 documentos (carregados do Elasticsearch)\n",
      "‚úÖ Embeddings gerados:\n",
      "   ‚Ä¢ TF-IDF: (18211, 4096)\n",
      "   ‚Ä¢ Word2Vec: (18211, 100)\n",
      "   ‚Ä¢ BERT: (18211, 768)\n",
      "   ‚Ä¢ Sentence-BERT: (18211, 384)\n",
      "\n",
      "üöÄ Pronto para o Notebook 3: Embeddings OpenAI!\n"
     ]
    }
   ],
   "source": [
    "# üìä Resumo Final\n",
    "print(\"üìä RESUMO FINAL - NOTEBOOK 2 COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Dataset: {len(df):,} documentos (carregados do Elasticsearch)\")\n",
    "print(f\"‚úÖ Embeddings gerados:\")\n",
    "print(f\"   ‚Ä¢ TF-IDF: {tfidf_embeddings.shape if tfidf_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   ‚Ä¢ Word2Vec: {word2vec_embeddings.shape if word2vec_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   ‚Ä¢ BERT: {bert_embeddings.shape if bert_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   ‚Ä¢ Sentence-BERT: {sbert_embeddings.shape if sbert_embeddings is not None else 'N/A'}\")\n",
    "print(f\"\\nüöÄ Pronto para o Notebook 3: Embeddings OpenAI!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
