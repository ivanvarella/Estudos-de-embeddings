{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔹 Seção 5.1 – Parte 2: Embeddings Locais\n",
    "\n",
    "**Objetivo:** Gerar e analisar embeddings clássicos e modernos (TF-IDF, Word2Vec, BERT, Sentence-BERT) usando o dataset preparado no Notebook 1.\n",
    "\n",
    "## 📋 Conteúdo deste Notebook\n",
    "\n",
    "1. **Carregamento de Dados**: Obter dataset diretamente do Elasticsearch\n",
    "2. **TF-IDF**: Embeddings baseados em frequência de termos\n",
    "3. **Word2Vec**: Embeddings contextuais clássicos\n",
    "4. **BERT**: Embeddings bidirecionais modernos\n",
    "5. **Sentence-BERT**: Otimizado para similaridade de sentenças\n",
    "6. **Análise Detalhada**: Comparação de características de cada tipo\n",
    "\n",
    "## 🔗 Sequência dos Notebooks\n",
    "\n",
    "- **Notebook 1**: Preparação e Dataset ✅\n",
    "- **Notebook 2** (atual): Embeddings Locais 🔄\n",
    "- **Notebook 3**: Embeddings OpenAI\n",
    "- **Notebook 4**: Análise Comparativa dos Embeddings\n",
    "- **Notebook 5**: Clustering e Machine Learning\n",
    "\n",
    "## ⚠️ IMPORTANTE: Carregamento de Dados\n",
    "\n",
    "Este notebook **SEMPRE carrega os dados do Elasticsearch** (preparados no Notebook 1), garantindo:\n",
    "- ✅ Consistência entre notebooks\n",
    "- ✅ Mesmos IDs únicos (doc_0000, doc_0001, ...)\n",
    "- ✅ Rastreabilidade completa\n",
    "- ✅ Integridade dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Configuração do Ambiente\n",
    "\n",
    "Este notebook carrega as configurações do arquivo `setup/.env` e conecta ao Elasticsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ python-dotenv disponível\n",
      "✅ Arquivo .env carregado: /Users/ivanvarella/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGÊNCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUÇÃO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/src/setup/.env\n",
      "\n",
      "🔧 Configurações carregadas!\n",
      "   ELASTICSEARCH: localhost:9200\n",
      "   CLUSTERING_RANDOM_STATE: 42\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Configuração de Variáveis de Ambiente\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Carregar python-dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"✅ python-dotenv disponível\")\n",
    "    \n",
    "    env_paths = [\n",
    "        Path.cwd() / 'setup' / '.env',\n",
    "        Path.cwd() / '.env',\n",
    "        Path.cwd() / 'setup' / 'config_example.env'\n",
    "    ]\n",
    "    \n",
    "    env_loaded = False\n",
    "    for env_path in env_paths:\n",
    "        if env_path.exists():\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"✅ Arquivo .env carregado: {env_path}\")\n",
    "            env_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not env_loaded:\n",
    "        print(\"⚠️  Nenhum arquivo .env encontrado\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"⚠️  python-dotenv não instalado\")\n",
    "\n",
    "# Carregar configurações (otimizadas para 20 classes)\n",
    "MAX_CHARS_PER_REQUEST = int(os.getenv('MAX_CHARS_PER_REQUEST', 32000))\n",
    "BATCH_SIZE_SMALL_TEXTS = int(os.getenv('BATCH_SIZE_SMALL_TEXTS', 4))\n",
    "BATCH_SIZE_MEDIUM_TEXTS = int(os.getenv('BATCH_SIZE_MEDIUM_TEXTS', 2))\n",
    "BATCH_SIZE_LARGE_TEXTS = int(os.getenv('BATCH_SIZE_LARGE_TEXTS', 1))\n",
    "DATASET_SIZE = int(os.getenv('DATASET_SIZE', 20000))\n",
    "TEXT_MIN_LENGTH = int(os.getenv('TEXT_MIN_LENGTH', 20))\n",
    "MAX_CLUSTERS = int(os.getenv('MAX_CLUSTERS', 20))\n",
    "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', 42))\n",
    "ELASTICSEARCH_HOST = os.getenv('ELASTICSEARCH_HOST', 'localhost')\n",
    "ELASTICSEARCH_PORT = int(os.getenv('ELASTICSEARCH_PORT', 9200))\n",
    "\n",
    "print(f\"\\n🔧 Configurações carregadas!\")\n",
    "print(f\"   ELASTICSEARCH: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}\")\n",
    "print(f\"   CLUSTERING_RANDOM_STATE: {CLUSTERING_RANDOM_STATE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CARREGANDO IMPORTS\n",
      "========================================\n",
      "✅ Imports básicos carregados\n",
      "✅ Configurações aplicadas\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Imports Essenciais\n",
    "print(\"🚀 CARREGANDO IMPORTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"✅ Imports básicos carregados\")\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✅ Configurações aplicadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗄️ Carregamento de Dados\n",
    "\n",
    "Este notebook carrega o dataset do Elasticsearch (salvo no Notebook 1) para garantir consistência.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️ INICIALIZANDO ELASTICSEARCH\n",
      "============================================================\n",
      "✅ Módulo de cache carregado\n",
      "\n",
      "🔌 Conectando...\n",
      "✅ Conectado ao Elasticsearch (localhost:9200)\n",
      "✅ Conectado ao Elasticsearch!\n",
      "📊 Total de documentos no cache: 109,266\n",
      "\n",
      "🎯 STATUS: ✅ Cache ativo\n"
     ]
    }
   ],
   "source": [
    "# 🗄️ Inicializar Elasticsearch e Carregar Dataset\n",
    "print(\"🗄️ INICIALIZANDO ELASTICSEARCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Importar módulo de cache\n",
    "try:\n",
    "    from elasticsearch_manager import (\n",
    "        init_elasticsearch_cache, get_cache_status,\n",
    "        save_embeddings_to_cache, load_embeddings_from_cache, \n",
    "        check_embeddings_in_cache\n",
    "    )\n",
    "    print(\"✅ Módulo de cache carregado\")\n",
    "    CACHE_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erro ao carregar módulo: {e}\")\n",
    "    CACHE_AVAILABLE = False\n",
    "\n",
    "# Conectar ao Elasticsearch\n",
    "if CACHE_AVAILABLE:\n",
    "    print(\"\\n🔌 Conectando...\")\n",
    "    cache_connected = init_elasticsearch_cache(\n",
    "        host=ELASTICSEARCH_HOST,\n",
    "        port=ELASTICSEARCH_PORT\n",
    "    )\n",
    "    \n",
    "    if cache_connected:\n",
    "        print(\"✅ Conectado ao Elasticsearch!\")\n",
    "        status = get_cache_status()\n",
    "        if status.get(\"connected\"):\n",
    "            print(f\"📊 Total de documentos no cache: {status.get('total_docs', 0):,}\")\n",
    "    else:\n",
    "        print(\"❌ Falha na conexão\")\n",
    "        CACHE_AVAILABLE = False\n",
    "else:\n",
    "    print(\"⚠️  Cache não disponível\")\n",
    "    cache_connected = False\n",
    "\n",
    "print(f\"\\n🎯 STATUS: {'✅ Cache ativo' if CACHE_AVAILABLE and cache_connected else '❌ Cache inativo'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CARREGANDO DATASET DO ELASTICSEARCH\n",
      "============================================================\n",
      "⚠️  IMPORTANTE: Carregando dados salvos no Notebook 1\n",
      "             NÃO recriando o dataset!\n",
      "🔄 Buscando documentos do índice 'documents_dataset'\n",
      "   Método: Scroll API (recomendado para >10k docs)\n",
      "   Tamanho do lote: 1,000 documentos\n",
      "   Timeout do scroll: 2m\n",
      "\n",
      "📊 Total de documentos disponíveis: 18,211\n",
      "🔄 Iniciando busca em lotes...\n",
      "   Lote 1: 1,000 docs | Total acumulado: 1,000/18,211\n",
      "   Lote 2: 1,000 docs | Total acumulado: 2,000/18,211\n",
      "   Lote 3: 1,000 docs | Total acumulado: 3,000/18,211\n",
      "   Lote 4: 1,000 docs | Total acumulado: 4,000/18,211\n",
      "   Lote 5: 1,000 docs | Total acumulado: 5,000/18,211\n",
      "   Lote 6: 1,000 docs | Total acumulado: 6,000/18,211\n",
      "   Lote 7: 1,000 docs | Total acumulado: 7,000/18,211\n",
      "   Lote 8: 1,000 docs | Total acumulado: 8,000/18,211\n",
      "   Lote 9: 1,000 docs | Total acumulado: 9,000/18,211\n",
      "   Lote 10: 1,000 docs | Total acumulado: 10,000/18,211\n",
      "   Lote 11: 1,000 docs | Total acumulado: 11,000/18,211\n",
      "   Lote 12: 1,000 docs | Total acumulado: 12,000/18,211\n",
      "   Lote 13: 1,000 docs | Total acumulado: 13,000/18,211\n",
      "   Lote 14: 1,000 docs | Total acumulado: 14,000/18,211\n",
      "   Lote 15: 1,000 docs | Total acumulado: 15,000/18,211\n",
      "   Lote 16: 1,000 docs | Total acumulado: 16,000/18,211\n",
      "   Lote 17: 1,000 docs | Total acumulado: 17,000/18,211\n",
      "   Lote 18: 1,000 docs | Total acumulado: 18,000/18,211\n",
      "   Lote 19: 211 docs | Total acumulado: 18,211/18,211\n",
      "\n",
      "✅ Scroll concluído e recursos liberados\n",
      "\n",
      "📊 Processando 18,211 documentos em DataFrame...\n",
      "✅ DataFrame criado com sucesso!\n",
      "\n",
      "============================================================\n",
      "✅ DATASET CARREGADO COM SUCESSO!\n",
      "============================================================\n",
      "📊 Shape: (18211, 4)\n",
      "📋 Colunas: ['doc_id', 'text', 'category', 'target']\n",
      "🏷️  Classes únicas: 20\n",
      "🔢 Total de documentos: 18,211\n",
      "🔑 IDs (amostra): ['doc_0000', 'doc_0001', 'doc_0002'] ... ['doc_9997', 'doc_9998', 'doc_9999']\n",
      "\n",
      "🔍 VALIDAÇÃO:\n",
      "   ✅ PASSOU: 18,211 documentos\n",
      "   ✅ Dentro da expectativa: ~18,000 ±1,000\n"
     ]
    }
   ],
   "source": [
    "# 📊 CARREGAR DATASET DO ELASTICSEARCH\n",
    "print(\"📊 CARREGANDO DATASET DO ELASTICSEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(\"⚠️  IMPORTANTE: Carregando dados salvos no Notebook 1\")\n",
    "print(\"             NÃO recriando o dataset!\")\n",
    "\n",
    "# Executar carregamento\n",
    "if CACHE_AVAILABLE and cache_connected:\n",
    "    try:\n",
    "        from elasticsearch import Elasticsearch\n",
    "        from elasticsearch_helpers import load_all_documents_from_elasticsearch, print_dataframe_summary\n",
    "        \n",
    "        # Conectar ao Elasticsearch\n",
    "        es = Elasticsearch([{\n",
    "            'host': ELASTICSEARCH_HOST, \n",
    "            'port': ELASTICSEARCH_PORT, \n",
    "            'scheme': 'http'\n",
    "        }])\n",
    "        \n",
    "        # Carregar TODOS os documentos usando Scroll API\n",
    "        # Esta função está em elasticsearch_helpers.py e usa Scroll API\n",
    "        # para buscar TODOS os documentos, mesmo que sejam >10.000\n",
    "        df = load_all_documents_from_elasticsearch(\n",
    "            es_client=es,\n",
    "            index_name=\"documents_dataset\",\n",
    "            batch_size=1000,      # Docs por lote\n",
    "            scroll_timeout='2m',  # Tempo de contexto\n",
    "            verbose=True          # Mostrar progresso\n",
    "        )\n",
    "        \n",
    "        # Gerar lista de doc_ids para uso posterior\n",
    "        doc_ids = df['doc_id'].tolist()\n",
    "        \n",
    "        # Exibir resumo detalhado\n",
    "        print_dataframe_summary(df, expected_docs=18000)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERRO CRÍTICO ao carregar dataset: {e}\")\n",
    "        print(\"💡 Possíveis causas:\")\n",
    "        print(\"   1. Notebook 1 não foi executado\")\n",
    "        print(\"   2. Elasticsearch não está rodando\")\n",
    "        print(\"   3. Índice 'documents_dataset' não existe\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n❌ ERRO: Elasticsearch não disponível!\")\n",
    "    print(\"💡 Verifique:\")\n",
    "    print(\"   1. Docker está rodando: docker ps\")\n",
    "    print(\"   2. Elasticsearch ativo: http://localhost:9200\")\n",
    "    print(\"   3. Execute o Notebook 1 primeiro\")\n",
    "    raise RuntimeError(\"Elasticsearch não disponível\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏛️ Embeddings Clássicos: TF-IDF\n",
    "\n",
    "### **O que é TF-IDF?**\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) é um método clássico que pondera a importância de palavras:\n",
    "\n",
    "- **TF (Term Frequency)**: Frequência do termo no documento\n",
    "- **IDF (Inverse Document Frequency)**: Raridade do termo no corpus\n",
    "- **Fórmula**: TF-IDF = TF × log(N/DF)\n",
    "  - N = número total de documentos\n",
    "  - DF = número de documentos contendo o termo\n",
    "\n",
    "### **Características**\n",
    "\n",
    "- ✅ Simples e interpretável\n",
    "- ✅ Rápido para calcular\n",
    "- ✅ Baseline sólido\n",
    "- ❌ Matriz esparsa (muitos zeros)\n",
    "- ❌ Não captura contexto semântico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "# es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "# if es.indices.exists(index='embeddings_tfidf'):\n",
    "#     es.indices.delete(index='embeddings_tfidf')\n",
    "#     print('✅ Índice embeddings_tfidf deletado')\n",
    "# else:\n",
    "#     print('ℹ️  Índice não existe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 GERANDO EMBEDDINGS TF-IDF\n",
      "============================================================\n",
      "✅ TF-IDF já existe no cache, carregando...\n",
      "✅ Embeddings carregados: (18211, 4096) de 'embeddings_tfidf'\n",
      "✅ TF-IDF carregado: (18211, 4096)\n",
      "\n",
      "📊 TF-IDF pronto: (18211, 4096)\n"
     ]
    }
   ],
   "source": [
    "# 🧮 Gerar Embeddings TF-IDF\n",
    "print(\"🧮 GERANDO EMBEDDINGS TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar se já existe no cache\n",
    "use_cache = os.getenv('USE_ELASTICSEARCH_CACHE', 'true').lower() == 'true'\n",
    "force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
    "\n",
    "if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "    all_exist, existing, missing = check_embeddings_in_cache('embeddings_tfidf', doc_ids)\n",
    "    \n",
    "    if all_exist:\n",
    "        print(\"✅ TF-IDF já existe no cache, carregando...\")\n",
    "        tfidf_embeddings = load_embeddings_from_cache('embeddings_tfidf', doc_ids)\n",
    "        if tfidf_embeddings is not None:\n",
    "            print(f\"✅ TF-IDF carregado: {tfidf_embeddings.shape}\")\n",
    "            # Criar vectorizer vazio (será usado para análise)\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_features=4096, max_df=0.95, min_df=2)\n",
    "            tfidf_vectorizer.fit(df['text'])\n",
    "        else:\n",
    "            print(\"❌ Falha ao carregar, regenerando...\")\n",
    "            force_regenerate = True\n",
    "\n",
    "if not use_cache or force_regenerate or not all_exist or tfidf_embeddings is None:\n",
    "    print(\"🔄 Gerando TF-IDF...\")\n",
    "    \n",
    "    # Criar vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=4096,  # Limitar a 4096 features (máximo do Elasticsearch)\n",
    "        max_df=0.95,        # Ignorar termos muito frequentes\n",
    "        min_df=2,           # Ignorar termos muito raros\n",
    "        ngram_range=(1, 2)  # Unigramas e bigramas\n",
    "    )\n",
    "    \n",
    "    # Gerar embeddings\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "    tfidf_embeddings = tfidf_matrix.toarray()\n",
    "    \n",
    "    print(f\"✅ TF-IDF gerado: {tfidf_embeddings.shape}\")\n",
    "    print(f\"   Vocabulário: {len(tfidf_vectorizer.vocabulary_):,} termos\")\n",
    "    print(f\"   Densidade: {(tfidf_embeddings != 0).mean():.3f}\")\n",
    "    \n",
    "    # Salvar no cache\n",
    "    if use_cache and CACHE_AVAILABLE:\n",
    "        print(\"💾 Salvando no Elasticsearch...\")\n",
    "        save_embeddings_to_cache(\n",
    "            'embeddings_tfidf', \n",
    "            tfidf_embeddings, \n",
    "            doc_ids, \n",
    "            df['text'].tolist(), \n",
    "            'tfidf'\n",
    "        )\n",
    "\n",
    "print(f\"\\n📊 TF-IDF pronto: {tfidf_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔤 Word2Vec: Embeddings Contextuais\n",
    "\n",
    "### **O que é Word2Vec?**\n",
    "\n",
    "**Word2Vec** (2013) foi revolucionário ao capturar similaridade semântica através de contexto:\n",
    "\n",
    "- **Skip-gram**: Prediz palavras vizinhas dado uma palavra central\n",
    "- **CBOW**: Prediz palavra central dado contexto\n",
    "- **Janela deslizante**: Considera palavras próximas\n",
    "- **Resultado**: Palavras similares ficam próximas no espaço vetorial\n",
    "\n",
    "### **Características**\n",
    "\n",
    "- ✅ Captura similaridade semântica\n",
    "- ✅ Embeddings densos\n",
    "- ✅ Rápido após treinamento\n",
    "- ❌ Palavras isoladas (não considera ordem global)\n",
    "- ❌ Requer treinamento no corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 CARREGANDO BIBLIOTECAS DE EMBEDDINGS\n",
      "============================================================\n",
      "✅ Gensim carregado\n",
      "✅ Sentence Transformers carregado\n",
      "\n",
      "Status: Gensim=✅, Transformers=✅\n"
     ]
    }
   ],
   "source": [
    "# 📚 Carregar bibliotecas para Word2Vec, BERT e SBERT\n",
    "print(\"📚 CARREGANDO BIBLIOTECAS DE EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Gensim para Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    print(\"✅ Gensim carregado\")\n",
    "    GENSIM_OK = True\n",
    "except:\n",
    "    print(\"❌ Gensim não disponível\")\n",
    "    GENSIM_OK = False\n",
    "\n",
    "# Sentence Transformers para BERT e SBERT\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"✅ Sentence Transformers carregado\")\n",
    "    TRANSFORMERS_OK = True\n",
    "except:\n",
    "    print(\"❌ Sentence Transformers não disponível\")\n",
    "    TRANSFORMERS_OK = False\n",
    "\n",
    "print(f\"\\nStatus: Gensim={'✅' if GENSIM_OK else '❌'}, Transformers={'✅' if TRANSFORMERS_OK else '❌'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 GERANDO EMBEDDINGS WORD2VEC\n",
      "============================================================\n",
      "✅ Word2Vec já existe, carregando...\n",
      "✅ Embeddings carregados: (18211, 100) de 'embeddings_word2vec'\n",
      "✅ Word2Vec carregado: (18211, 100)\n",
      "\n",
      "📊 Word2Vec pronto: (18211, 100)\n"
     ]
    }
   ],
   "source": [
    "# 🧮 Gerar Embeddings Word2Vec\n",
    "print(\"🧮 GERANDO EMBEDDINGS WORD2VEC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not GENSIM_OK:\n",
    "    print(\"❌ Gensim não disponível, pulando Word2Vec\")\n",
    "    word2vec_embeddings = None\n",
    "else:\n",
    "    # Verificar cache\n",
    "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "        all_exist, _, _ = check_embeddings_in_cache('embeddings_word2vec', doc_ids)\n",
    "        if all_exist:\n",
    "            print(\"✅ Word2Vec já existe, carregando...\")\n",
    "            word2vec_embeddings = load_embeddings_from_cache('embeddings_word2vec', doc_ids)\n",
    "            if word2vec_embeddings is not None:\n",
    "                print(f\"✅ Word2Vec carregado: {word2vec_embeddings.shape}\")\n",
    "            else:\n",
    "                force_regenerate = True\n",
    "    \n",
    "    if not use_cache or force_regenerate or not all_exist or word2vec_embeddings is None:\n",
    "        print(\"🔄 Treinando Word2Vec...\")\n",
    "        \n",
    "        # Tokenizar textos\n",
    "        tokenized_texts = [text.lower().split() for text in df['text']]\n",
    "        \n",
    "        # Treinar Word2Vec\n",
    "        w2v_model = Word2Vec(\n",
    "            sentences=tokenized_texts,\n",
    "            vector_size=100,\n",
    "            window=5,\n",
    "            min_count=2,\n",
    "            workers=4,\n",
    "            epochs=10,\n",
    "            seed=CLUSTERING_RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        # Gerar embeddings por documento (média dos vetores de palavras)\n",
    "        word2vec_embeddings = []\n",
    "        for tokens in tokenized_texts:\n",
    "            valid_vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "            if valid_vectors:\n",
    "                word2vec_embeddings.append(np.mean(valid_vectors, axis=0))\n",
    "            else:\n",
    "                word2vec_embeddings.append(np.zeros(100))\n",
    "        \n",
    "        word2vec_embeddings = np.array(word2vec_embeddings)\n",
    "        \n",
    "        print(f\"✅ Word2Vec gerado: {word2vec_embeddings.shape}\")\n",
    "        print(f\"   Vocabulário: {len(w2v_model.wv):,} palavras\")\n",
    "        \n",
    "        # Salvar no cache\n",
    "        if use_cache and CACHE_AVAILABLE:\n",
    "            print(\"💾 Salvando no Elasticsearch...\")\n",
    "            save_embeddings_to_cache(\n",
    "                'embeddings_word2vec',\n",
    "                word2vec_embeddings,\n",
    "                doc_ids,\n",
    "                df['text'].tolist(),\n",
    "                'word2vec'\n",
    "            )\n",
    "\n",
    "if word2vec_embeddings is not None:\n",
    "    print(f\"\\n📊 Word2Vec pronto: {word2vec_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Embeddings Modernos: BERT e Sentence-BERT\n",
    "\n",
    "### **BERT (2018) - Contextualização Bidirecional**\n",
    "\n",
    "**Características:**\n",
    "- Lê texto em ambas as direções simultaneamente\n",
    "- Attention mechanism\n",
    "- 768 dimensões (bert-base-uncased)\n",
    "- Contextualizado: mesma palavra, contextos diferentes\n",
    "\n",
    "### **Sentence-BERT (2019) - Otimizado para Similaridade**\n",
    "\n",
    "**Características:**\n",
    "- Baseado em BERT mas otimizado para similaridade\n",
    "- 384 dimensões (all-MiniLM-L6-v2)\n",
    "- Ideal para clustering e busca semântica\n",
    "- Normalizado por padrão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 GERANDO EMBEDDINGS BERT E SENTENCE-BERT\n",
      "============================================================\n",
      "\n",
      "📖 BERT (bert-base-uncased)...\n",
      "✅ BERT já existe, carregando...\n",
      "✅ Embeddings carregados: (18211, 768) de 'embeddings_bert'\n",
      "\n",
      "📖 Sentence-BERT (all-MiniLM-L6-v2)...\n",
      "✅ Sentence-BERT já existe, carregando...\n",
      "✅ Embeddings carregados: (18211, 384) de 'embeddings_sbert'\n",
      "\n",
      "📊 RESUMO DOS EMBEDDINGS LOCAIS:\n",
      "   TF-IDF: (18211, 4096)\n",
      "   Word2Vec: (18211, 100)\n",
      "   BERT: (18211, 768)\n",
      "   Sentence-BERT: (18211, 384)\n"
     ]
    }
   ],
   "source": [
    "# 🧮 Gerar Embeddings BERT e Sentence-BERT\n",
    "print(\"🧮 GERANDO EMBEDDINGS BERT E SENTENCE-BERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not TRANSFORMERS_OK:\n",
    "    print(\"❌ Sentence Transformers não disponível\")\n",
    "    bert_embeddings = None\n",
    "    sbert_embeddings = None\n",
    "else:\n",
    "    # BERT\n",
    "    print(\"\\n📖 BERT (bert-base-uncased)...\")\n",
    "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "        all_exist, _, _ = check_embeddings_in_cache('embeddings_bert', doc_ids)\n",
    "        if all_exist:\n",
    "            print(\"✅ BERT já existe, carregando...\")\n",
    "            bert_embeddings = load_embeddings_from_cache('embeddings_bert', doc_ids)\n",
    "            if bert_embeddings is None:\n",
    "                force_regenerate = True\n",
    "    \n",
    "    if not use_cache or force_regenerate or not all_exist or bert_embeddings is None:\n",
    "        print(\"🔄 Gerando BERT...\")\n",
    "        bert_model = SentenceTransformer('bert-base-uncased')\n",
    "        bert_embeddings = bert_model.encode(\n",
    "            df['text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"✅ BERT gerado: {bert_embeddings.shape}\")\n",
    "        \n",
    "        if use_cache and CACHE_AVAILABLE:\n",
    "            save_embeddings_to_cache(\n",
    "                'embeddings_bert',\n",
    "                bert_embeddings,\n",
    "                doc_ids,\n",
    "                df['text'].tolist(),\n",
    "                'bert'\n",
    "            )\n",
    "    \n",
    "    # Sentence-BERT\n",
    "    print(\"\\n📖 Sentence-BERT (all-MiniLM-L6-v2)...\")\n",
    "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
    "        all_exist, _, _ = check_embeddings_in_cache('embeddings_sbert', doc_ids)\n",
    "        if all_exist:\n",
    "            print(\"✅ Sentence-BERT já existe, carregando...\")\n",
    "            sbert_embeddings = load_embeddings_from_cache('embeddings_sbert', doc_ids)\n",
    "            if sbert_embeddings is None:\n",
    "                force_regenerate = True\n",
    "    \n",
    "    if not use_cache or force_regenerate or not all_exist or sbert_embeddings is None:\n",
    "        print(\"🔄 Gerando Sentence-BERT...\")\n",
    "        sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        sbert_embeddings = sbert_model.encode(\n",
    "            df['text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"✅ Sentence-BERT gerado: {sbert_embeddings.shape}\")\n",
    "        \n",
    "        if use_cache and CACHE_AVAILABLE:\n",
    "            save_embeddings_to_cache(\n",
    "                'embeddings_sbert',\n",
    "                sbert_embeddings,\n",
    "                doc_ids,\n",
    "                df['text'].tolist(),\n",
    "                'sbert'\n",
    "            )\n",
    "\n",
    "print(f\"\\n📊 RESUMO DOS EMBEDDINGS LOCAIS:\")\n",
    "print(f\"   TF-IDF: {tfidf_embeddings.shape if tfidf_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   Word2Vec: {word2vec_embeddings.shape if word2vec_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   BERT: {bert_embeddings.shape if bert_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   Sentence-BERT: {sbert_embeddings.shape if sbert_embeddings is not None else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Resumo e Próximos Passos\n",
    "\n",
    "### **O que foi realizado neste notebook:**\n",
    "\n",
    "1. ✅ **Dados carregados do Elasticsearch** - Consistência garantida\n",
    "2. ✅ **TF-IDF** - Embeddings clássicos baseados em frequência\n",
    "3. ✅ **Word2Vec** - Embeddings contextuais treinados\n",
    "4. ✅ **BERT** - Embeddings bidirecionais modernos\n",
    "5. ✅ **Sentence-BERT** - Otimizado para similaridade\n",
    "\n",
    "### **Próximo Notebook: Parte 3 - Embeddings OpenAI**\n",
    "\n",
    "No próximo notebook:\n",
    "- Embeddings da API OpenAI (text-embedding-3-small)\n",
    "- Tratamento de textos longos (sem truncamento)\n",
    "- Economia de custos com cache inteligente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 RESUMO FINAL - NOTEBOOK 2 COMPLETO\n",
      "============================================================\n",
      "✅ Dataset: 18,211 documentos (carregados do Elasticsearch)\n",
      "✅ Embeddings gerados:\n",
      "   • TF-IDF: (18211, 4096)\n",
      "   • Word2Vec: (18211, 100)\n",
      "   • BERT: (18211, 768)\n",
      "   • Sentence-BERT: (18211, 384)\n",
      "\n",
      "🚀 Pronto para o Notebook 3: Embeddings OpenAI!\n"
     ]
    }
   ],
   "source": [
    "# 📊 Resumo Final\n",
    "print(\"📊 RESUMO FINAL - NOTEBOOK 2 COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Dataset: {len(df):,} documentos (carregados do Elasticsearch)\")\n",
    "print(f\"✅ Embeddings gerados:\")\n",
    "print(f\"   • TF-IDF: {tfidf_embeddings.shape if tfidf_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   • Word2Vec: {word2vec_embeddings.shape if word2vec_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   • BERT: {bert_embeddings.shape if bert_embeddings is not None else 'N/A'}\")\n",
    "print(f\"   • Sentence-BERT: {sbert_embeddings.shape if sbert_embeddings is not None else 'N/A'}\")\n",
    "print(f\"\\n🚀 Pronto para o Notebook 3: Embeddings OpenAI!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
