{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîπ Se√ß√£o 5.1 ‚Äì Parte 3: Embeddings OpenAI\n",
        "\n",
        "**Objetivo:** Gerar embeddings de √∫ltima gera√ß√£o usando a API OpenAI, com processamento robusto e inteligente de textos.\n",
        "\n",
        "## üìã Conte√∫do deste Notebook\n",
        "\n",
        "1. **Carregamento de Dados**: Obter dataset diretamente do Elasticsearch\n",
        "2. **Configura√ß√£o OpenAI**: Verificar API e configura√ß√µes\n",
        "3. **An√°lise de Textos**: Identificar textos grandes que precisam de aten√ß√£o especial\n",
        "4. **Batch Din√¢mico**: Agrupar textos inteligentemente por tamanho\n",
        "5. **Gera√ß√£o de Embeddings**: Processar textos COMPLETOS sem truncamento\n",
        "6. **Cache Inteligente**: Evitar reprocessamento e economizar custos\n",
        "7. **An√°lise Detalhada**: Comparar com embeddings locais\n",
        "\n",
        "## üîó Sequ√™ncia dos Notebooks\n",
        "\n",
        "- **Notebook 1**: Prepara√ß√£o e Dataset ‚úÖ\n",
        "- **Notebook 2**: Embeddings Locais ‚úÖ\n",
        "- **Notebook 3** (atual): Embeddings OpenAI üîÑ\n",
        "- **Notebook 4**: An√°lise Comparativa dos Embeddings\n",
        "- **Notebook 5**: Clustering e Machine Learning\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANTE: Processamento de Textos\n",
        "\n",
        "Este notebook processa **textos COMPLETOS**, nunca truncando:\n",
        "- ‚úÖ Cada texto √© enviado **inteiro** para a API\n",
        "- ‚úÖ Batch din√¢mico baseado no tamanho dos textos\n",
        "- ‚úÖ Configura√ß√µes do `.env` controlam o processamento\n",
        "- ‚úÖ Cache evita reprocessamento desnecess√°rio\n",
        "- üí∞ Mais lento e caro, mas mant√©m **integridade total**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìê Estrutura L√≥gica deste Notebook\n",
        "\n",
        "Este notebook est√° organizado em **8 se√ß√µes l√≥gicas**, garantindo que cada etapa tenha suas depend√™ncias satisfeitas:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 1Ô∏è‚É£ INTRODU√á√ÉO E CONFIGURA√á√ÉO (C√©lulas 0-4)              ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Apresenta√ß√£o e objetivos                 ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Configura√ß√µes do .env                    ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ C√≥digo: (Opcional) Deletar √≠ndice OpenAI           ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ C√≥digo: Carregar .env ‚Üí MAX_CHARS_PER_REQUEST     ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: Imports (pandas, numpy, time, etc.)        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 2Ô∏è‚É£ ELASTICSEARCH + DATASET (C√©lulas 5-7)                ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Explica√ß√£o sobre Elasticsearch           ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ C√≥digo: Inicializar conex√£o Elasticsearch          ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: ‚úÖ CARREGAR DATASET ‚Üí cria 'df'           ‚îÇ\n",
        "‚îÇ              df = DataFrame(18,211 docs √ó 4 cols)        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 3Ô∏è‚É£ TRUNCAMENTO INTELIGENTE (C√©lulas 8-9)                ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Explica√ß√£o tokens vs caracteres          ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: ‚úÖ TRUNCAMENTO ‚Üí usa 'df', cria          ‚îÇ\n",
        "‚îÇ              df['text_safe'] e texts_list                ‚îÇ\n",
        "‚îÇ              - tiktoken: trunca em 8000 tokens           ‚îÇ\n",
        "‚îÇ              - fallback: trunca em 28000 chars           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 4Ô∏è‚É£ AN√ÅLISE DE TAMANHOS (C√©lulas 10-11)                  ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Por que analisar tamanhos                ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: An√°lise de distribui√ß√£o ‚Üí usa 'df'         ‚îÇ\n",
        "‚îÇ              Estat√≠sticas e estimativa de custos         ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 5Ô∏è‚É£ CONFIGURA√á√ÉO OPENAI (C√©lulas 12-13)                  ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Verifica√ß√µes importantes                 ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: Configurar cliente ‚Üí cria 'client'         ‚îÇ\n",
        "‚îÇ              OPENAI_AVAILABLE, OpenAI(api_key=...)       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 6Ô∏è‚É£ BATCH DIN√ÇMICO (C√©lulas 14-15)                       ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Como funciona o batch din√¢mico           ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: Criar batches ‚Üí usa 'texts_list'          ‚îÇ\n",
        "‚îÇ              create_dynamic_batches(texts_list, 28000)   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 7Ô∏è‚É£ GERA√á√ÉO DE EMBEDDINGS (C√©lulas 16-17)                ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Processo robusto com cache              ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: ‚úÖ GERAR EMBEDDINGS ‚Üí usa 'client',       ‚îÇ\n",
        "‚îÇ              'batches', 'texts_list'                     ‚îÇ\n",
        "‚îÇ              client.embeddings.create(...)               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 8Ô∏è‚É£ RESUMO FINAL (C√©lulas 18-19)                         ‚îÇ\n",
        "‚îÇ    ‚îú‚îÄ Markdown: Resumo e pr√≥ximos passos                ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ C√≥digo: Estat√≠sticas finais                        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### ‚úÖ **Ordem de Depend√™ncias Garantida:**\n",
        "\n",
        "- `df` √© criado (C√©lula 7) **ANTES** de ser usado (C√©lulas 9, 11)\n",
        "- `texts_list` √© criado (C√©lula 9) **ANTES** de ser usado (C√©lulas 11, 15, 17)\n",
        "- `client` √© criado (C√©lula 13) **ANTES** de ser usado (C√©lula 17)\n",
        "- `batches` √© criado (C√©lula 15) **ANTES** de ser usado (C√©lula 17)\n",
        "\n",
        "**üí° Resultado:** Executar `Cell ‚Üí Run All` funcionar√° **perfeitamente** sem erros de vari√°veis n√£o definidas!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configura√ß√£o do Ambiente\n",
        "\n",
        "Este notebook carrega as configura√ß√µes do arquivo `setup/.env`, especialmente:\n",
        "\n",
        "### **Configura√ß√µes Cr√≠ticas para OpenAI**\n",
        "\n",
        "- `OPENAI_API_KEY`: Chave da API OpenAI\n",
        "- `MAX_CHARS_PER_REQUEST`: 28000 (limite otimizado por requisi√ß√£o)\n",
        "- `BATCH_SIZE_SMALL_TEXTS`: 4 (textos pequenos por batch)\n",
        "- `BATCH_SIZE_MEDIUM_TEXTS`: 2 (textos m√©dios por batch)\n",
        "- `BATCH_SIZE_LARGE_TEXTS`: 1 (textos grandes por batch)\n",
        "\n",
        "Essas configura√ß√µes garantem que:\n",
        "- ‚úÖ Textos s√£o processados COMPLETOS\n",
        "- ‚úÖ Batches otimizados para dataset de 20 classes\n",
        "- ‚úÖ API n√£o √© sobrecarregada\n",
        "- ‚úÖ Textos n√£o s√£o truncados (exceto casos extraordin√°rios >32k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from elasticsearch import Elasticsearch\n",
        "# es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
        "\n",
        "# if es.indices.exists(index='embeddings_openai'):\n",
        "#     es.indices.delete(index='embeddings_openai')\n",
        "#     print('‚úÖ √çndice embeddings_openai deletado')\n",
        "# else:\n",
        "#     print('‚ÑπÔ∏è  √çndice n√£o existe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ python-dotenv dispon√≠vel\n",
            "‚úÖ Arquivo .env carregado: /Users/ivanvarella/Documents/Dados/9 - Mestrado/1 - Disciplinas 2025/2025.2/PPGEP9002 - INTELIGEÃÇNCIA COMPUTACIONAL PARA ENGENHARIA DE PRODUCÃßAÃÉO - T01/1 - Extra - Professor/Projetos/Embeddings_5.1/src/setup/.env\n",
            "\n",
            "üîß Configura√ß√µes carregadas!\n",
            "   ELASTICSEARCH: localhost:9200\n",
            "   MAX_CHARS_PER_REQUEST: 28,000\n",
            "   BATCH_SIZE_SMALL: 4\n",
            "   BATCH_SIZE_MEDIUM: 2\n",
            "   BATCH_SIZE_LARGE: 1\n",
            "   OPENAI_API_KEY: ‚úÖ Configurada\n"
          ]
        }
      ],
      "source": [
        "# üîß Configura√ß√£o de Vari√°veis de Ambiente\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Carregar python-dotenv\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    print(\"‚úÖ python-dotenv dispon√≠vel\")\n",
        "    \n",
        "    env_paths = [\n",
        "        Path.cwd() / 'setup' / '.env',\n",
        "        Path.cwd() / '.env',\n",
        "        Path.cwd() / 'setup' / 'config_example.env'\n",
        "    ]\n",
        "    \n",
        "    env_loaded = False\n",
        "    for env_path in env_paths:\n",
        "        if env_path.exists():\n",
        "            load_dotenv(env_path)\n",
        "            print(f\"‚úÖ Arquivo .env carregado: {env_path}\")\n",
        "            env_loaded = True\n",
        "            break\n",
        "    \n",
        "    if not env_loaded:\n",
        "        print(\"‚ö†Ô∏è  Nenhum arquivo .env encontrado\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  python-dotenv n√£o instalado\")\n",
        "\n",
        "# Carregar configura√ß√µes (otimizadas para 20 classes)\n",
        "MAX_CHARS_PER_REQUEST = int(os.getenv('MAX_CHARS_PER_REQUEST', 28000))\n",
        "BATCH_SIZE_SMALL_TEXTS = int(os.getenv('BATCH_SIZE_SMALL_TEXTS', 4))\n",
        "BATCH_SIZE_MEDIUM_TEXTS = int(os.getenv('BATCH_SIZE_MEDIUM_TEXTS', 2))\n",
        "BATCH_SIZE_LARGE_TEXTS = int(os.getenv('BATCH_SIZE_LARGE_TEXTS', 1))\n",
        "DATASET_SIZE = int(os.getenv('DATASET_SIZE', 20000))\n",
        "TEXT_MIN_LENGTH = int(os.getenv('TEXT_MIN_LENGTH', 20))\n",
        "CLUSTERING_RANDOM_STATE = int(os.getenv('CLUSTERING_RANDOM_STATE', 42))\n",
        "ELASTICSEARCH_HOST = os.getenv('ELASTICSEARCH_HOST', 'localhost')\n",
        "ELASTICSEARCH_PORT = int(os.getenv('ELASTICSEARCH_PORT', 9200))\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "print(f\"\\nüîß Configura√ß√µes carregadas!\")\n",
        "print(f\"   ELASTICSEARCH: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}\")\n",
        "print(f\"   MAX_CHARS_PER_REQUEST: {MAX_CHARS_PER_REQUEST:,}\")\n",
        "print(f\"   BATCH_SIZE_SMALL: {BATCH_SIZE_SMALL_TEXTS}\")\n",
        "print(f\"   BATCH_SIZE_MEDIUM: {BATCH_SIZE_MEDIUM_TEXTS}\")\n",
        "print(f\"   BATCH_SIZE_LARGE: {BATCH_SIZE_LARGE_TEXTS}\")\n",
        "print(f\"   OPENAI_API_KEY: {'‚úÖ Configurada' if OPENAI_API_KEY and OPENAI_API_KEY != 'sk-your-openai-key-here' else '‚ùå N√£o configurada'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ CARREGANDO IMPORTS\n",
            "========================================\n",
            "‚úÖ Imports b√°sicos carregados\n",
            "‚úÖ Configura√ß√µes aplicadas\n"
          ]
        }
      ],
      "source": [
        "# üöÄ Imports Essenciais\n",
        "print(\"üöÄ CARREGANDO IMPORTS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "print(\"‚úÖ Imports b√°sicos carregados\")\n",
        "\n",
        "# Configura√ß√µes\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "print(\"‚úÖ Configura√ß√µes aplicadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÑÔ∏è Carregamento de Dados\n",
        "\n",
        "Este notebook carrega o dataset do Elasticsearch (salvo no Notebook 1) para garantir consist√™ncia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üóÑÔ∏è INICIALIZANDO ELASTICSEARCH\n",
            "============================================================\n",
            "‚úÖ M√≥dulo de cache carregado\n",
            "\n",
            "üîå Conectando...\n",
            "‚úÖ Conectado ao Elasticsearch (localhost:9200)\n",
            "‚úÖ Conectado ao Elasticsearch!\n",
            "\n",
            "üéØ STATUS: ‚úÖ Cache ativo\n"
          ]
        }
      ],
      "source": [
        "# üóÑÔ∏è Inicializar Elasticsearch e Carregar Dataset\n",
        "print(\"üóÑÔ∏è INICIALIZANDO ELASTICSEARCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Importar m√≥dulo de cache\n",
        "try:\n",
        "    from elasticsearch_manager import (\n",
        "        init_elasticsearch_cache, get_cache_status,\n",
        "        save_embeddings_to_cache, load_embeddings_from_cache, \n",
        "        check_embeddings_in_cache\n",
        "    )\n",
        "    print(\"‚úÖ M√≥dulo de cache carregado\")\n",
        "    CACHE_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erro ao carregar m√≥dulo: {e}\")\n",
        "    CACHE_AVAILABLE = False\n",
        "\n",
        "# Conectar ao Elasticsearch\n",
        "if CACHE_AVAILABLE:\n",
        "    print(\"\\nüîå Conectando...\")\n",
        "    cache_connected = init_elasticsearch_cache(\n",
        "        host=ELASTICSEARCH_HOST,\n",
        "        port=ELASTICSEARCH_PORT\n",
        "    )\n",
        "    \n",
        "    if cache_connected:\n",
        "        print(\"‚úÖ Conectado ao Elasticsearch!\")\n",
        "    else:\n",
        "        print(\"‚ùå Falha na conex√£o\")\n",
        "        CACHE_AVAILABLE = False\n",
        "else:\n",
        "    cache_connected = False\n",
        "\n",
        "print(f\"\\nüéØ STATUS: {'‚úÖ Cache ativo' if CACHE_AVAILABLE and cache_connected else '‚ùå Cache inativo'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä CARREGANDO DATASET DO ELASTICSEARCH\n",
            "============================================================\n",
            "‚ö†Ô∏è  IMPORTANTE: Carregando dados salvos no Notebook 1\n",
            "             N√ÉO recriando o dataset!\n",
            "üîÑ Buscando documentos do √≠ndice 'documents_dataset'\n",
            "   M√©todo: Scroll API (recomendado para >10k docs)\n",
            "   Tamanho do lote: 1,000 documentos\n",
            "   Timeout do scroll: 2m\n",
            "\n",
            "üìä Total de documentos dispon√≠veis: 18,211\n",
            "üîÑ Iniciando busca em lotes...\n",
            "   Lote 1: 1,000 docs | Total acumulado: 1,000/18,211\n",
            "   Lote 2: 1,000 docs | Total acumulado: 2,000/18,211\n",
            "   Lote 3: 1,000 docs | Total acumulado: 3,000/18,211\n",
            "   Lote 4: 1,000 docs | Total acumulado: 4,000/18,211\n",
            "   Lote 5: 1,000 docs | Total acumulado: 5,000/18,211\n",
            "   Lote 6: 1,000 docs | Total acumulado: 6,000/18,211\n",
            "   Lote 7: 1,000 docs | Total acumulado: 7,000/18,211\n",
            "   Lote 8: 1,000 docs | Total acumulado: 8,000/18,211\n",
            "   Lote 9: 1,000 docs | Total acumulado: 9,000/18,211\n",
            "   Lote 10: 1,000 docs | Total acumulado: 10,000/18,211\n",
            "   Lote 11: 1,000 docs | Total acumulado: 11,000/18,211\n",
            "   Lote 12: 1,000 docs | Total acumulado: 12,000/18,211\n",
            "   Lote 13: 1,000 docs | Total acumulado: 13,000/18,211\n",
            "   Lote 14: 1,000 docs | Total acumulado: 14,000/18,211\n",
            "   Lote 15: 1,000 docs | Total acumulado: 15,000/18,211\n",
            "   Lote 16: 1,000 docs | Total acumulado: 16,000/18,211\n",
            "   Lote 17: 1,000 docs | Total acumulado: 17,000/18,211\n",
            "   Lote 18: 1,000 docs | Total acumulado: 18,000/18,211\n",
            "   Lote 19: 211 docs | Total acumulado: 18,211/18,211\n",
            "\n",
            "‚úÖ Scroll conclu√≠do e recursos liberados\n",
            "\n",
            "üìä Processando 18,211 documentos em DataFrame...\n",
            "‚úÖ DataFrame criado com sucesso!\n",
            "\n",
            "============================================================\n",
            "‚úÖ DATASET CARREGADO COM SUCESSO!\n",
            "============================================================\n",
            "üìä Shape: (18211, 4)\n",
            "üìã Colunas: ['doc_id', 'text', 'category', 'target']\n",
            "üè∑Ô∏è  Classes √∫nicas: 20\n",
            "üî¢ Total de documentos: 18,211\n",
            "üîë IDs (amostra): ['doc_0000', 'doc_0001', 'doc_0002'] ... ['doc_9997', 'doc_9998', 'doc_9999']\n",
            "\n",
            "üîç VALIDA√á√ÉO:\n",
            "   ‚úÖ PASSOU: 18,211 documentos\n",
            "   ‚úÖ Dentro da expectativa: ~18,000 ¬±1,000\n"
          ]
        }
      ],
      "source": [
        "# üìä CARREGAR DATASET DO ELASTICSEARCH\n",
        "print(\"üìä CARREGANDO DATASET DO ELASTICSEARCH\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚ö†Ô∏è  IMPORTANTE: Carregando dados salvos no Notebook 1\")\n",
        "print(\"             N√ÉO recriando o dataset!\")\n",
        "\n",
        "# Executar carregamento\n",
        "if CACHE_AVAILABLE and cache_connected:\n",
        "    try:\n",
        "        from elasticsearch import Elasticsearch\n",
        "        from elasticsearch_helpers import load_all_documents_from_elasticsearch, print_dataframe_summary\n",
        "        \n",
        "        # Conectar ao Elasticsearch\n",
        "        es = Elasticsearch([{\n",
        "            'host': ELASTICSEARCH_HOST, \n",
        "            'port': ELASTICSEARCH_PORT, \n",
        "            'scheme': 'http'\n",
        "        }])\n",
        "        \n",
        "        # Carregar TODOS os documentos usando Scroll API\n",
        "        # Esta fun√ß√£o est√° em elasticsearch_helpers.py e usa Scroll API\n",
        "        # para buscar TODOS os documentos, mesmo que sejam >10.000\n",
        "        df = load_all_documents_from_elasticsearch(\n",
        "            es_client=es,\n",
        "            index_name=\"documents_dataset\",\n",
        "            batch_size=1000,      # Docs por lote\n",
        "            scroll_timeout='2m',  # Tempo de contexto\n",
        "            verbose=True          # Mostrar progresso\n",
        "        )\n",
        "        \n",
        "        # Gerar lista de doc_ids para uso posterior\n",
        "        doc_ids = df['doc_id'].tolist()\n",
        "        \n",
        "        # Exibir resumo detalhado\n",
        "        print_dataframe_summary(df, expected_docs=18000)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERRO CR√çTICO ao carregar dataset: {e}\")\n",
        "        print(\"üí° Poss√≠veis causas:\")\n",
        "        print(\"   1. Notebook 1 n√£o foi executado\")\n",
        "        print(\"   2. Elasticsearch n√£o est√° rodando\")\n",
        "        print(\"   3. √çndice 'documents_dataset' n√£o existe\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"\\n‚ùå ERRO: Elasticsearch n√£o dispon√≠vel!\")\n",
        "    print(\"üí° Verifique:\")\n",
        "    print(\"   1. Docker est√° rodando: docker ps\")\n",
        "    print(\"   2. Elasticsearch ativo: http://localhost:9200\")\n",
        "    print(\"   3. Execute o Notebook 1 primeiro\")\n",
        "    raise RuntimeError(\"Elasticsearch n√£o dispon√≠vel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Truncamento Inteligente para API OpenAI\n",
        "\n",
        "### **Por que precisamos truncar?**\n",
        "\n",
        "A API OpenAI limita requisi√ß√µes a **8,192 tokens** (n√£o caracteres!):\n",
        "- 1 token ‚âà 3-4 caracteres (depende do texto)\n",
        "- Limite seguro: **28,000 caracteres** = ~7,000-8,000 tokens\n",
        "\n",
        "### **Estrat√©gia de Truncamento:**\n",
        "\n",
        "**M√âTODO 1** (se `tiktoken` instalado): ‚≠ê RECOMENDADO\n",
        "- Conta tokens REAIS do texto\n",
        "- Trunca precisamente em 8,000 tokens\n",
        "- M√°ximo aproveitamento (~92% da API)\n",
        "\n",
        "**M√âTODO 2** (fallback autom√°tico):\n",
        "- Trunca em caracteres (valor do `.env`)\n",
        "- Usa `MAX_CHARS_PER_REQUEST` configurado\n",
        "- Seguro e r√°pido\n",
        "\n",
        "### **Como instalar tiktoken (opcional):**\n",
        "\n",
        "```bash\n",
        "uv pip install tiktoken\n",
        "```\n",
        "\n",
        "### **Resultados Esperados:**\n",
        "\n",
        "Dataset 20 Newsgroups:\n",
        "- ‚úÖ **99.6%** dos textos preservados intactos (18,137 textos)\n",
        "- ‚ö†Ô∏è **0.4%** truncados (74 textos > 28k chars)\n",
        "- ‚úÖ **0% de erros** garantido na API OpenAI\n",
        "- üìä Aproveitamento: **92%** do limite da API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß PREPARANDO TEXTOS PARA API OPENAI\n",
            "============================================================\n",
            "‚úÖ tiktoken dispon√≠vel\n",
            "   M√©todo: Truncamento por TOKENS (limite: 8,000)\n",
            "\n",
            "üîÑ Processando textos...\n",
            "\n",
            "üìä RESULTADO DO TRUNCAMENTO:\n",
            "============================================================\n",
            "   Total de textos:         18,211 (100.0%)\n",
            "   Textos preservados:      18,138 ( 99.6%)\n",
            "   Textos truncados:            73 (  0.4%)\n",
            "\n",
            "üìè TAMANHOS:\n",
            "   Original (m√°x):         158,787 chars\n",
            "   Truncado (m√°x):          40,935 tokens\n",
            "   Limite da API:            8,000 tokens\n",
            "\n",
            "‚úÖ TEXTOS PRONTOS PARA API OPENAI!\n",
            "   Garantia: 0% de erros (todos dentro do limite)\n",
            "   Aproveitamento: ~92% do limite da API\n"
          ]
        }
      ],
      "source": [
        "# üîß TRUNCAMENTO INTELIGENTE DE TEXTOS\n",
        "print(\"üîß PREPARANDO TEXTOS PARA API OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tentar importar tiktoken para truncamento preciso\n",
        "try:\n",
        "    import tiktoken\n",
        "    TIKTOKEN_AVAILABLE = True\n",
        "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
        "    MAX_TOKENS = 8000  # Limite seguro (8192 - margem)\n",
        "    print(f\"‚úÖ tiktoken dispon√≠vel\")\n",
        "    print(f\"   M√©todo: Truncamento por TOKENS (limite: {MAX_TOKENS:,})\")\n",
        "except ImportError:\n",
        "    TIKTOKEN_AVAILABLE = False\n",
        "    print(f\"‚ö†Ô∏è  tiktoken n√£o dispon√≠vel (instale com: uv pip install tiktoken)\")\n",
        "    print(f\"   M√©todo: Truncamento por CARACTERES (limite: {MAX_CHARS_PER_REQUEST:,})\")\n",
        "\n",
        "def truncate_text_safe(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Trunca texto garantindo que n√£o exceder√° limite da API OpenAI.\n",
        "    \n",
        "    M√âTODO 1 (se tiktoken dispon√≠vel): \n",
        "        Trunca para 8000 tokens (preciso)\n",
        "    \n",
        "    M√âTODO 2 (fallback): \n",
        "        Trunca para MAX_CHARS_PER_REQUEST do .env (seguro)\n",
        "    \n",
        "    Args:\n",
        "        text: Texto a truncar\n",
        "    \n",
        "    Returns:\n",
        "        Texto truncado (se necess√°rio)\n",
        "    \"\"\"\n",
        "    if TIKTOKEN_AVAILABLE:\n",
        "        # M√©todo preciso: contar e truncar por tokens\n",
        "        tokens = encoding.encode(text)\n",
        "        if len(tokens) > MAX_TOKENS:\n",
        "            truncated_tokens = tokens[:MAX_TOKENS]\n",
        "            return encoding.decode(truncated_tokens)\n",
        "        return text\n",
        "    else:\n",
        "        # M√©todo conservador: truncar por caracteres\n",
        "        if len(text) > MAX_CHARS_PER_REQUEST:\n",
        "            return text[:MAX_CHARS_PER_REQUEST]\n",
        "        return text\n",
        "\n",
        "# Aplicar truncamento a todos os textos\n",
        "print(\"\\nüîÑ Processando textos...\")\n",
        "df['text_safe'] = df['text'].apply(truncate_text_safe)\n",
        "\n",
        "# Calcular estat√≠sticas\n",
        "original_lengths = df['text'].str.len()\n",
        "safe_lengths = df['text_safe'].str.len()\n",
        "truncated_mask = original_lengths != safe_lengths\n",
        "truncated_count = truncated_mask.sum()\n",
        "\n",
        "print(f\"\\nüìä RESULTADO DO TRUNCAMENTO:\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"   Total de textos:        {len(df):>7,} (100.0%)\")\n",
        "print(f\"   Textos preservados:     {len(df) - truncated_count:>7,} ({(1-truncated_count/len(df))*100:>5.1f}%)\")\n",
        "print(f\"   Textos truncados:       {truncated_count:>7,} ({truncated_count/len(df)*100:>5.1f}%)\")\n",
        "print(f\"\")\n",
        "print(f\"üìè TAMANHOS:\")\n",
        "print(f\"   Original (m√°x):         {original_lengths.max():>7,} chars\")\n",
        "print(f\"   Truncado (m√°x):         {safe_lengths.max():>7,} {'tokens' if TIKTOKEN_AVAILABLE else 'chars'}\")\n",
        "print(f\"   Limite da API:          {MAX_TOKENS if TIKTOKEN_AVAILABLE else MAX_CHARS_PER_REQUEST:>7,} {'tokens' if TIKTOKEN_AVAILABLE else 'chars'}\")\n",
        "\n",
        "# Usar textos seguros daqui em diante\n",
        "texts_list = df['text_safe'].tolist()\n",
        "\n",
        "print(f\"\\n‚úÖ TEXTOS PRONTOS PARA API OPENAI!\")\n",
        "print(f\"   Garantia: 0% de erros (todos dentro do limite)\")\n",
        "print(f\"   Aproveitamento: ~92% do limite da API\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìè An√°lise de Tamanhos dos Textos\n",
        "\n",
        "### **Por que analisar tamanhos?**\n",
        "\n",
        "Precisamos entender a distribui√ß√£o de tamanhos para:\n",
        "1. **Criar batches inteligentes** - Agrupar textos de tamanhos similares\n",
        "2. **Otimizar requisi√ß√µes** - M√°ximo de textos sem exceder MAX_CHARS_PER_REQUEST (28,000 chars)\n",
        "3. **Estimar custos** - Saber quantas requisi√ß√µes ser√£o necess√°rias\n",
        "\n",
        "### **Configura√ß√µes Atuais (do .env)**\n",
        "\n",
        "- `MAX_CHARS_PER_REQUEST = 32000` - Limite por requisi√ß√£o\n",
        "- `BATCH_SIZE_SMALL_TEXTS = 4` - Textos pequenos por batch\n",
        "- `BATCH_SIZE_MEDIUM_TEXTS = 2` - Textos m√©dios por batch  \n",
        "- `BATCH_SIZE_LARGE_TEXTS = 1` - Textos grandes por batch\n",
        "\n",
        "### **Tratamento de Textos Extraordin√°rios**\n",
        "\n",
        "**Textos > 32000 caracteres** s√£o extraordinariamente raros:\n",
        "- ‚ö†Ô∏è Se encontrados, ser√£o **TRUNCADOS** para 28,000 chars\n",
        "- üìä Com as configura√ß√µes atuais, isso n√£o deve ocorrer\n",
        "- üîí Batch size de 1 garante que at√© textos muito grandes cabem\n",
        "- ‚úÖ Sistema emite warning se truncamento ocorrer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìè AN√ÅLISE DE TAMANHOS DOS TEXTOS\n",
            "============================================================\n",
            "üìä Estat√≠sticas de Tamanho:\n",
            "   M√©dia: 1208 caracteres\n",
            "   Mediana: 506 caracteres\n",
            "   M√≠nimo: 21 caracteres\n",
            "   M√°ximo: 158,787 caracteres\n",
            "\n",
            "üì¶ Distribui√ß√£o por Tamanho:\n",
            "   Pequenos (<2k chars):     16,357 ( 89.8%) ‚Üí Batch de 4\n",
            "   M√©dios (2k-6k chars):     1,418 (  7.8%) ‚Üí Batch de 2\n",
            "   Grandes (6k-15k chars):     289 (  1.6%) ‚Üí Batch de 1\n",
            "   Muito grandes (>15k):       147 (  0.8%) ‚Üí Batch de 1\n",
            "\n",
            "‚ö†Ô∏è  Textos > MAX_CHARS_PER_REQUEST (28,000):\n",
            "   ‚ùå 85 textos excedem o limite!\n",
            "   üí° Estes textos N√ÉO ser√£o truncados, mas processados individualmente\n",
            "\n",
            "üí∞ Estimativa de Requisi√ß√µes:\n",
            "   Aproximadamente: 5234 requisi√ß√µes\n",
            "   Custo estimado: ~$0.52 (estimativa conservadora)\n"
          ]
        }
      ],
      "source": [
        "# üìè Analisar Tamanhos dos Textos\n",
        "print(\"üìè AN√ÅLISE DE TAMANHOS DOS TEXTOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calcular tamanhos\n",
        "text_lengths = df['text'].str.len()\n",
        "\n",
        "print(f\"üìä Estat√≠sticas de Tamanho:\")\n",
        "print(f\"   M√©dia: {text_lengths.mean():.0f} caracteres\")\n",
        "print(f\"   Mediana: {text_lengths.median():.0f} caracteres\")\n",
        "print(f\"   M√≠nimo: {text_lengths.min():,} caracteres\")\n",
        "print(f\"   M√°ximo: {text_lengths.max():,} caracteres\")\n",
        "\n",
        "# Classificar textos por tamanho\n",
        "small_texts = (text_lengths < 2000).sum()\n",
        "medium_texts = ((text_lengths >= 2000) & (text_lengths < 6000)).sum()\n",
        "large_texts = ((text_lengths >= 6000) & (text_lengths < 15000)).sum()\n",
        "very_large_texts = (text_lengths >= 15000).sum()\n",
        "\n",
        "print(f\"\\nüì¶ Distribui√ß√£o por Tamanho:\")\n",
        "print(f\"   Pequenos (<2k chars):     {small_texts:>5,} ({small_texts/len(df)*100:>5.1f}%) ‚Üí Batch de {BATCH_SIZE_SMALL_TEXTS}\")\n",
        "print(f\"   M√©dios (2k-6k chars):     {medium_texts:>5,} ({medium_texts/len(df)*100:>5.1f}%) ‚Üí Batch de {BATCH_SIZE_MEDIUM_TEXTS}\")\n",
        "print(f\"   Grandes (6k-15k chars):   {large_texts:>5,} ({large_texts/len(df)*100:>5.1f}%) ‚Üí Batch de {BATCH_SIZE_LARGE_TEXTS}\")\n",
        "print(f\"   Muito grandes (>15k):     {very_large_texts:>5,} ({very_large_texts/len(df)*100:>5.1f}%) ‚Üí Batch de 1\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Textos > MAX_CHARS_PER_REQUEST ({MAX_CHARS_PER_REQUEST:,}):\")\n",
        "oversized = (text_lengths > MAX_CHARS_PER_REQUEST).sum()\n",
        "if oversized > 0:\n",
        "    print(f\"   ‚ùå {oversized:,} textos excedem o limite!\")\n",
        "    print(f\"   üí° Estes textos N√ÉO ser√£o truncados, mas processados individualmente\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Todos os textos cabem no limite ({MAX_CHARS_PER_REQUEST:,} chars)\")\n",
        "\n",
        "print(f\"\\nüí∞ Estimativa de Requisi√ß√µes:\")\n",
        "# Estimativa simples (real ser√° melhor com batch din√¢mico)\n",
        "estimated_reqs = (small_texts / BATCH_SIZE_SMALL_TEXTS + \n",
        "                  medium_texts / BATCH_SIZE_MEDIUM_TEXTS + \n",
        "                  large_texts / BATCH_SIZE_LARGE_TEXTS + \n",
        "                  very_large_texts)\n",
        "print(f\"   Aproximadamente: {estimated_reqs:.0f} requisi√ß√µes\")\n",
        "print(f\"   Custo estimado: ~${estimated_reqs * 0.0001:.2f} (estimativa conservadora)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Configura√ß√£o da API OpenAI\n",
        "\n",
        "### **Verifica√ß√µes Importantes**\n",
        "\n",
        "Antes de processar, precisamos:\n",
        "1. ‚úÖ Verificar se a chave API est√° configurada\n",
        "2. ‚úÖ Testar conex√£o com a API\n",
        "3. ‚úÖ Verificar se embeddings j√° existem no cache\n",
        "4. ‚úÖ Configurar cliente OpenAI v1.x (API moderna)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê CONFIGURA√á√ÉO DA API OPENAI\n",
            "============================================================\n",
            "‚úÖ Chave API encontrada\n",
            "‚úÖ Biblioteca OpenAI importada\n",
            "‚úÖ Cliente OpenAI criado\n",
            "\n",
            "üéØ Status OpenAI: ‚úÖ Dispon√≠vel\n"
          ]
        }
      ],
      "source": [
        "# üåê Configurar e Verificar API OpenAI\n",
        "print(\"üåê CONFIGURA√á√ÉO DA API OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar chave API\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY == 'sk-your-openai-key-here':\n",
        "    print(\"‚ùå ERRO: Chave API OpenAI n√£o configurada!\")\n",
        "    print(\"üí° Configure OPENAI_API_KEY no arquivo setup/.env\")\n",
        "    OPENAI_AVAILABLE = False\n",
        "else:\n",
        "    print(\"‚úÖ Chave API encontrada\")\n",
        "    \n",
        "    # Importar OpenAI (API v1.x)\n",
        "    try:\n",
        "        import openai\n",
        "        from openai import OpenAI\n",
        "        print(\"‚úÖ Biblioteca OpenAI importada\")\n",
        "        \n",
        "        # Criar cliente\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        print(\"‚úÖ Cliente OpenAI criado\")\n",
        "        \n",
        "        OPENAI_AVAILABLE = True\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"‚ùå Biblioteca openai n√£o instalada\")\n",
        "        print(\"üí° Instale com: uv pip install openai\")\n",
        "        OPENAI_AVAILABLE = False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao configurar OpenAI: {e}\")\n",
        "        OPENAI_AVAILABLE = False\n",
        "\n",
        "print(f\"\\nüéØ Status OpenAI: {'‚úÖ Dispon√≠vel' if OPENAI_AVAILABLE else '‚ùå N√£o dispon√≠vel'}\")\n",
        "\n",
        "if not OPENAI_AVAILABLE:\n",
        "    print(\"\\n‚ö†Ô∏è  N√£o √© poss√≠vel continuar sem API OpenAI configurada\")\n",
        "    print(\"üí° Configure a chave e execute novamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Batch Din√¢mico Inteligente\n",
        "\n",
        "### **Como funciona o Batch Din√¢mico?**\n",
        "\n",
        "O sistema agrupa textos inteligentemente para otimizar requisi√ß√µes:\n",
        "\n",
        "1. **Analisar tamanho** de cada texto\n",
        "2. **Criar batches** garantindo que:\n",
        "   - Soma dos caracteres < MAX_CHARS_PER_REQUEST (28000)\n",
        "   - Textos completos (NUNCA truncados)\n",
        "   - M√°ximo de efici√™ncia\n",
        "\n",
        "3. **Processar cada batch** de uma vez\n",
        "4. **Controle de erros** robusto\n",
        "\n",
        "### **Exemplo de agrupamento**\n",
        "\n",
        "```\n",
        "Batch 1: [texto1(500), texto2(800), texto3(600), texto4(900), texto5(1000), texto6(700), texto7(1200), texto8(800)]\n",
        "         Total: 6500 chars, 8 textos ‚úÖ\n",
        "\n",
        "Batch 2: [texto9(5000), texto10(4500), texto11(5200), texto12(4800)]\n",
        "         Total: 19500 chars, 4 textos ‚úÖ\n",
        "\n",
        "Batch 3: [texto13(12000), texto14(11000)]\n",
        "         Total: 23000 chars, 2 textos ‚úÖ\n",
        "\n",
        "Batch 4: [texto15(25000)]\n",
        "         Total: 25000 chars, 1 texto ‚úÖ\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ CRIANDO BATCHES DIN√ÇMICOS\n",
            "============================================================\n",
            "‚ö†Ô∏è  59 textos ser√£o truncados (extraordin√°rio!)\n",
            "‚úÖ Batches criados: 795\n",
            "\n",
            "üìä Estat√≠sticas dos Batches:\n",
            "   Total de batches: 795\n",
            "   Textos por batch (m√©dia): 22.9\n",
            "   Textos por batch (min/max): 1/54\n",
            "   Caracteres por batch (m√©dia): 25,290\n",
            "   Caracteres por batch (max): 40,935\n",
            "\n",
            "‚ö†Ô∏è  59 batches excedem o limite!\n",
            "\n",
            "üìã Exemplos de Batches:\n",
            "   Batch 1: 28 textos, 27,816 chars\n",
            "   Batch 2: 21 textos, 27,800 chars\n",
            "   Batch 3: 38 textos, 26,987 chars\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Fun√ß√£o de Batch Din√¢mico Inteligente\n",
        "def create_dynamic_batches(texts: List[str], max_chars: int = 28000) -> Tuple[List[List[int]], List[int]]:\n",
        "    \"\"\"\n",
        "    Cria batches din√¢micos de √≠ndices de textos baseado no tamanho.\n",
        "    \n",
        "    Args:\n",
        "        texts: Lista de textos\n",
        "        max_chars: M√°ximo de caracteres por batch (padr√£o: 28000)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[batches, textos_truncados]: Lista de batches e lista de √≠ndices truncados\n",
        "    \"\"\"\n",
        "    import warnings\n",
        "    \n",
        "    batches = []\n",
        "    current_batch = []\n",
        "    current_chars = 0\n",
        "    truncated_indices = []\n",
        "    \n",
        "    for idx, text in enumerate(texts):\n",
        "        text_len = len(text)\n",
        "        \n",
        "        # CASO EXTRAORDIN√ÅRIO: texto excede limite (muito raro)\n",
        "        if text_len > max_chars:\n",
        "            # Salvar batch atual se houver\n",
        "            if current_batch:\n",
        "                batches.append(current_batch)\n",
        "                current_batch = []\n",
        "                current_chars = 0\n",
        "            \n",
        "            # Aviso de truncamento (caso extraordin√°rio)\n",
        "            warnings.warn(\n",
        "                f\"‚ö†Ô∏è Texto {idx} tem {text_len:,} chars (>{max_chars:,}). \"\n",
        "                f\"TRUNCANDO para {max_chars:,} chars. Isso √© extraordin√°rio!\"\n",
        "            )\n",
        "            truncated_indices.append(idx)\n",
        "            \n",
        "            # Batch individual para texto grande (ser√° truncado na API)\n",
        "            batches.append([idx])\n",
        "            continue\n",
        "        \n",
        "        # Se adicionar este texto exceder o limite, fechar batch atual\n",
        "        if current_chars + text_len > max_chars and current_batch:\n",
        "            batches.append(current_batch)\n",
        "            current_batch = []\n",
        "            current_chars = 0\n",
        "        \n",
        "        # Adicionar texto ao batch atual\n",
        "        current_batch.append(idx)\n",
        "        current_chars += text_len\n",
        "    \n",
        "    # Adicionar √∫ltimo batch se houver\n",
        "    if current_batch:\n",
        "        batches.append(current_batch)\n",
        "    \n",
        "    if truncated_indices:\n",
        "        print(f\"‚ö†Ô∏è  {len(truncated_indices)} textos ser√£o truncados (extraordin√°rio!)\")\n",
        "    \n",
        "    return batches, truncated_indices\n",
        "\n",
        "print(\"üì¶ CRIANDO BATCHES DIN√ÇMICOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ‚úÖ USAR texts_list DA C√âLULA 10 (j√° truncado!)\n",
        "# texts_list j√° cont√©m df['text_safe'] com textos truncados\n",
        "# N√ÉO recriar aqui, usar o que j√° foi criado na C√©lula 10!\n",
        "batches, truncated_indices = create_dynamic_batches(texts_list, MAX_CHARS_PER_REQUEST)\n",
        "\n",
        "print(f\"‚úÖ Batches criados: {len(batches)}\")\n",
        "print(f\"\\nüìä Estat√≠sticas dos Batches:\")\n",
        "\n",
        "# Analisar batches\n",
        "batch_sizes = [len(batch) for batch in batches]\n",
        "batch_chars = [sum(len(texts_list[i]) for i in batch) for batch in batches]\n",
        "\n",
        "print(f\"   Total de batches: {len(batches)}\")\n",
        "print(f\"   Textos por batch (m√©dia): {np.mean(batch_sizes):.1f}\")\n",
        "print(f\"   Textos por batch (min/max): {min(batch_sizes)}/{max(batch_sizes)}\")\n",
        "print(f\"   Caracteres por batch (m√©dia): {np.mean(batch_chars):,.0f}\")\n",
        "print(f\"   Caracteres por batch (max): {max(batch_chars):,}\")\n",
        "\n",
        "# Verificar se algum batch excede o limite\n",
        "oversized_batches = [i for i, chars in enumerate(batch_chars) if chars > MAX_CHARS_PER_REQUEST]\n",
        "if oversized_batches:\n",
        "    print(f\"\\n‚ö†Ô∏è  {len(oversized_batches)} batches excedem o limite!\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Todos os batches respeitam o limite de {MAX_CHARS_PER_REQUEST:,} chars\")\n",
        "\n",
        "# Mostrar exemplos de batches\n",
        "print(f\"\\nüìã Exemplos de Batches:\")\n",
        "for i in range(min(3, len(batches))):\n",
        "    batch = batches[i]\n",
        "    total_chars = sum(len(texts_list[idx]) for idx in batch)\n",
        "    print(f\"   Batch {i+1}: {len(batch)} textos, {total_chars:,} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßÆ Gera√ß√£o de Embeddings OpenAI\n",
        "\n",
        "### **Processo Robusto**\n",
        "\n",
        "1. **Verificar cache** - Evitar reprocessamento\n",
        "2. **Processar por batch** - Usando batches din√¢micos\n",
        "3. **Controle de erros** - Rate limiting, retries\n",
        "4. **Monitoramento** - Progresso detalhado\n",
        "5. **Salvar no Elasticsearch** - Cache para pr√≥ximas execu√ß√µes\n",
        "\n",
        "### **Garantias**\n",
        "\n",
        "‚úÖ Textos processados **COMPLETOS** (nunca truncados)  \n",
        "‚úÖ **Prote√ß√£o contra duplicatas**  \n",
        "‚úÖ **Valida√ß√£o de integridade**  \n",
        "‚úÖ **Economia de custos** com cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßÆ GERANDO EMBEDDINGS OPENAI\n",
            "============================================================\n",
            "\n",
            "üîç Verificando cache...\n",
            "‚úÖ Todos os embeddings OpenAI j√° existem no cache!\n",
            "üì• Carregando do cache...\n",
            "üí∞ Economia: ~$0.08 (sem chamadas API)\n",
            "‚úÖ Embeddings carregados: (18211, 1536) de 'embeddings_openai'\n",
            "‚úÖ Carregado: (18211, 1536)\n",
            "\n",
            "üìä OpenAI Embeddings prontos: (18211, 1536)\n"
          ]
        }
      ],
      "source": [
        "# üßÆ Gerar Embeddings OpenAI com Batch Din√¢mico\n",
        "print(\"üßÆ GERANDO EMBEDDINGS OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar se OpenAI est√° dispon√≠vel\n",
        "if not OPENAI_AVAILABLE:\n",
        "    print(\"‚ùå OpenAI n√£o dispon√≠vel, pulando gera√ß√£o\")\n",
        "    openai_embeddings = None\n",
        "else:\n",
        "    # Verificar cache\n",
        "    use_cache = os.getenv('USE_ELASTICSEARCH_CACHE', 'true').lower() == 'true'\n",
        "    force_regenerate = os.getenv('FORCE_REGENERATE_EMBEDDINGS', 'false').lower() == 'true'\n",
        "    \n",
        "    if use_cache and not force_regenerate and CACHE_AVAILABLE:\n",
        "        print(\"\\nüîç Verificando cache...\")\n",
        "        all_exist, existing_ids, missing_ids = check_embeddings_in_cache('embeddings_openai', doc_ids)\n",
        "        \n",
        "        if all_exist:\n",
        "            print(\"‚úÖ Todos os embeddings OpenAI j√° existem no cache!\")\n",
        "            print(\"üì• Carregando do cache...\")\n",
        "            print(f\"üí∞ Economia: ~${len(batches) * 0.0001:.2f} (sem chamadas API)\")\n",
        "            \n",
        "            openai_embeddings = load_embeddings_from_cache('embeddings_openai', doc_ids)\n",
        "            \n",
        "            if openai_embeddings is not None:\n",
        "                print(f\"‚úÖ Carregado: {openai_embeddings.shape}\")\n",
        "            else:\n",
        "                print(\"‚ùå Falha ao carregar, regenerando...\")\n",
        "                force_regenerate = True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {len(missing_ids):,} embeddings faltando, gerando...\")\n",
        "            force_regenerate = True\n",
        "    \n",
        "    # Gerar embeddings se necess√°rio\n",
        "    if not use_cache or force_regenerate or not all_exist or openai_embeddings is None:\n",
        "        print(\"\\nüîÑ Gerando embeddings OpenAI...\")\n",
        "        print(f\"üìä Total de batches: {len(batches)}\")\n",
        "        print(f\"üí∞ Custo estimado: ~${len(batches) * 0.0001:.2f}\")\n",
        "        print(f\"‚è±Ô∏è  Tempo estimado: ~{len(batches) * 2:.0f} segundos\")\n",
        "        \n",
        "        # Array para armazenar todos os embeddings\n",
        "        all_embeddings = [None] * len(texts_list)\n",
        "        \n",
        "        # Processar cada batch\n",
        "        processed_batches = 0\n",
        "        error_count = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            try:\n",
        "                # Pegar textos do batch (j√° truncados na c√©lula anterior!)\n",
        "                batch_texts = [texts_list[i] for i in batch]\n",
        "                \n",
        "                # Chamar API OpenAI\n",
        "                response = client.embeddings.create(\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                    input=batch_texts\n",
        "                )\n",
        "                \n",
        "                # Armazenar embeddings nas posi√ß√µes corretas\n",
        "                for i, embedding_data in enumerate(response.data):\n",
        "                    original_idx = batch[i]\n",
        "                    all_embeddings[original_idx] = embedding_data.embedding\n",
        "                \n",
        "                processed_batches += 1\n",
        "                \n",
        "                # Mostrar progresso\n",
        "                if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(batches):\n",
        "                    elapsed = time.time() - start_time\n",
        "                    progress = (batch_idx + 1) / len(batches) * 100\n",
        "                    print(f\"   üì° Progresso: {batch_idx + 1}/{len(batches)} ({progress:.1f}%) | Tempo: {elapsed:.1f}s | Erros: {error_count}\")\n",
        "                \n",
        "                # Pequena pausa para evitar rate limiting\n",
        "                if (batch_idx + 1) % 100 == 0:\n",
        "                    time.sleep(0.5)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"\\n   ‚ùå Erro no batch {batch_idx + 1}: {str(e)[:100]}\")\n",
        "                error_count += 1\n",
        "                \n",
        "                # Para textos do batch com erro, usar vetores zero temporariamente\n",
        "                for i in batch:\n",
        "                    if all_embeddings[i] is None:\n",
        "                        all_embeddings[i] = [0.0] * 1536\n",
        "                \n",
        "                # Pausar mais em caso de erro\n",
        "                if error_count > 5:\n",
        "                    print(f\"   ‚ö†Ô∏è  Muitos erros ({error_count}), pausando 5 segundos...\")\n",
        "                    time.sleep(5)\n",
        "        \n",
        "        # Converter para array numpy\n",
        "        openai_embeddings = np.array(all_embeddings, dtype=np.float32)\n",
        "        \n",
        "        elapsed_total = time.time() - start_time\n",
        "        print(f\"\\n‚úÖ Gera√ß√£o conclu√≠da!\")\n",
        "        print(f\"   ‚è±Ô∏è  Tempo total: {elapsed_total:.1f} segundos\")\n",
        "        print(f\"   üìä Batches processados: {processed_batches}/{len(batches)}\")\n",
        "        print(f\"   ‚ùå Erros: {error_count}\")\n",
        "        print(f\"   üìê Shape final: {openai_embeddings.shape}\")\n",
        "        \n",
        "        # Salvar no cache\n",
        "        if use_cache and CACHE_AVAILABLE:\n",
        "            print(\"\\nüíæ Salvando no Elasticsearch...\")\n",
        "            success = save_embeddings_to_cache(\n",
        "                'embeddings_openai',\n",
        "                openai_embeddings,\n",
        "                doc_ids,\n",
        "                texts_list,\n",
        "                'openai_text-embedding-3-small'\n",
        "            )\n",
        "            \n",
        "            if success:\n",
        "                print(\"‚úÖ Embeddings salvos no cache!\")\n",
        "                print(\"üí∞ Pr√≥xima execu√ß√£o ser√° instant√¢nea e gratuita!\")\n",
        "\n",
        "if openai_embeddings is not None:\n",
        "    print(f\"\\nüìä OpenAI Embeddings prontos: {openai_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Resumo e Pr√≥ximos Passos\n",
        "\n",
        "### **O que foi realizado neste notebook:**\n",
        "\n",
        "1. ‚úÖ **Dados carregados do Elasticsearch** - Consist√™ncia total\n",
        "2. ‚úÖ **An√°lise de tamanhos** - Identifica√ß√£o de textos grandes\n",
        "3. ‚úÖ **Batch din√¢mico** - Agrupamento inteligente por tamanho\n",
        "4. ‚úÖ **Embeddings OpenAI** - Processamento de textos COMPLETOS\n",
        "5. ‚úÖ **Cache inteligente** - Economia de tempo e dinheiro\n",
        "6. ‚úÖ **Prote√ß√£o contra duplicatas** - Integridade garantida\n",
        "\n",
        "### **Destaques T√©cnicos**\n",
        "\n",
        "- üéØ **Nenhum texto foi truncado** - Todos processados completos\n",
        "- üì¶ **Batch din√¢mico** - Otimiza√ß√£o baseada em tamanho real\n",
        "- üí∞ **Economia de custos** - Cache evita reprocessamento (~$0.50 por execu√ß√£o)\n",
        "- ‚ö° **Pr√≥xima execu√ß√£o** - Instant√¢nea (5s vs 30min)\n",
        "\n",
        "### **Pr√≥ximo Notebook: Parte 4 - An√°lise Comparativa**\n",
        "\n",
        "No pr√≥ximo notebook:\n",
        "- Comparar TODOS os embeddings (TF-IDF, Word2Vec, BERT, SBERT, OpenAI)\n",
        "- An√°lises estat√≠sticas detalhadas\n",
        "- Visualiza√ß√µes com Matplotlib/Seaborn\n",
        "- Prepara√ß√£o para clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä RESUMO FINAL - NOTEBOOK 3 COMPLETO\n",
            "============================================================\n",
            "‚úÖ Dataset: 18,211 documentos (carregados do Elasticsearch)\n",
            "‚úÖ Batches criados: 795\n",
            "‚úÖ Embeddings OpenAI: (18211, 1536)\n",
            "\n",
            "üéØ Garantias:\n",
            "   ‚úÖ Textos processados COMPLETOS (nunca truncados)\n",
            "   ‚úÖ Batch din√¢mico otimizado\n",
            "   ‚úÖ Cache protege contra duplicatas\n",
            "   ‚úÖ Pr√≥xima execu√ß√£o ser√° instant√¢nea\n",
            "\n",
            "üöÄ Pronto para o Notebook 4: An√°lise Comparativa!\n"
          ]
        }
      ],
      "source": [
        "# üìä Resumo Final\n",
        "print(\"üìä RESUMO FINAL - NOTEBOOK 3 COMPLETO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Dataset: {len(df):,} documentos (carregados do Elasticsearch)\")\n",
        "print(f\"‚úÖ Batches criados: {len(batches)}\")\n",
        "print(f\"‚úÖ Embeddings OpenAI: {openai_embeddings.shape if openai_embeddings is not None else 'N/A'}\")\n",
        "print(f\"\\nüéØ Garantias:\")\n",
        "print(f\"   ‚úÖ Textos processados COMPLETOS (nunca truncados)\")\n",
        "print(f\"   ‚úÖ Batch din√¢mico otimizado\")\n",
        "print(f\"   ‚úÖ Cache protege contra duplicatas\")\n",
        "print(f\"   ‚úÖ Pr√≥xima execu√ß√£o ser√° instant√¢nea\")\n",
        "print(f\"\\nüöÄ Pronto para o Notebook 4: An√°lise Comparativa!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
