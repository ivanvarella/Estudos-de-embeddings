#!/usr/bin/env python3
"""
Elasticsearch Helper Functions
===============================

Este m√≥dulo cont√©m fun√ß√µes auxiliares para trabalhar com Elasticsearch,
especialmente para buscar grandes volumes de dados (>10.000 documentos).

Autor: Sistema de Embeddings e Clustering
Data: 2025-10
"""

import pandas as pd
from typing import Optional
from elasticsearch import Elasticsearch


def load_all_documents_from_elasticsearch(
    es_client: Elasticsearch,
    index_name: str = "documents_dataset",
    batch_size: int = 1000,
    scroll_timeout: str = '2m',
    verbose: bool = True
) -> pd.DataFrame:
    """
    Carrega TODOS os documentos do Elasticsearch usando Scroll API.
    
    üéì CONCEITO EDUCACIONAL: SCROLL API DO ELASTICSEARCH
    ===================================================
    
    ### Por que precisamos da Scroll API?
    
    O Elasticsearch tem um limite padr√£o de **10.000 documentos** por busca simples
    (`index.max_result_window = 10000`). Isso √© uma prote√ß√£o de performance para
    evitar que buscas muito grandes consumam toda a mem√≥ria do servidor.
    
    Quando tentamos buscar mais de 10.000 documentos com uma query normal:
    ```python
    # ‚ùå ISSO FALHA com 18.211 documentos!
    response = es.search(index="documents_dataset", size=20000)
    # BadRequestError: Result window is too large
    ```
    
    ### Como a Scroll API resolve isso?
    
    A Scroll API funciona como um "cursor" de banco de dados:
    
    1. **Snapshot (Foto dos dados)**:
       - Cria uma "foto" consistente dos dados no momento da busca
       - Os dados n√£o mudam durante toda a busca
       - Garante que voc√™ n√£o perca nem duplique documentos
    
    2. **Busca em Lotes**:
       - Busca 1000 documentos por vez (ou outro tamanho definido)
       - Retorna um "scroll_id" (cursor) para continuar de onde parou
       - Muito mais eficiente em mem√≥ria
    
    3. **Itera√ß√£o Eficiente**:
       - Usa o scroll_id para buscar o pr√≥ximo lote
       - Continua at√© n√£o haver mais dados
       - Elasticsearch mant√©m contexto no servidor
    
    4. **Limpeza de Recursos**:
       - Ao final, limpa o scroll_id
       - Libera recursos no servidor
    
    ### Alternativas e por que N√ÉO us√°-las:
    
    ‚ùå **M√∫ltiplas buscas com FROM/SIZE**:
    ```python
    # Busca 1: from=0, size=10000
    # Busca 2: from=10000, size=10000
    ```
    Problemas:
    - Muito ineficiente: cada busca reprocessa TODOS os docs anteriores
    - Consome muita mem√≥ria no servidor
    - Pode ter limite total (from + size ‚â§ max_result_window)
    - Lenta para dados "profundos"
    
    ‚ùå **Aumentar max_result_window**:
    ```python
    PUT /index/_settings {"index.max_result_window": 50000}
    ```
    Problemas:
    - PERIGOSO: Pode crashar o Elasticsearch
    - Consome mem√≥ria excessiva
    - N√£o √© escal√°vel
    - Vai contra boas pr√°ticas oficiais
    
    ‚úÖ **Scroll API** (NOSSA ESCOLHA):
    - Recomenda√ß√£o oficial do Elasticsearch
    - Eficiente em mem√≥ria e processamento
    - Consistente e confi√°vel
    - Perfeita para "dump completo" de dados
    
    ### Fluxo de Execu√ß√£o:
    
    ```
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 1. INICIAR SCROLL                       ‚îÇ
    ‚îÇ    es.search(scroll='2m', size=1000)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì Retorna: scroll_id + 1000 docs
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 2. CONTINUAR SCROLL (loop)              ‚îÇ
    ‚îÇ    es.scroll(scroll_id=scroll_id)       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì Retorna: novo scroll_id + pr√≥ximos 1000 docs
                   ‚îÇ
                   ‚Üì Repete at√© n√£o haver mais docs
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 3. LIMPAR SCROLL                        ‚îÇ
    ‚îÇ    es.clear_scroll(scroll_id=scroll_id) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ```
    
    ### Par√¢metros:
    
    Args:
        es_client (Elasticsearch): 
            Cliente Elasticsearch j√° conectado.
            Exemplo: Elasticsearch([{'host': 'localhost', 'port': 9200}])
        
        index_name (str, optional): 
            Nome do √≠ndice a buscar. 
            Padr√£o: "documents_dataset"
        
        batch_size (int, optional): 
            N√∫mero de documentos por lote.
            - Menor (500): Mais requisi√ß√µes, menos mem√≥ria por vez
            - Maior (5000): Menos requisi√ß√µes, mais mem√≥ria por vez
            - Recomendado: 1000 (bom equil√≠brio)
            Padr√£o: 1000
        
        scroll_timeout (str, optional): 
            Tempo que o Elasticsearch mant√©m o contexto de scroll ativo.
            - '1m': 1 minuto (busca r√°pida)
            - '2m': 2 minutos (recomendado)
            - '5m': 5 minutos (busca lenta ou debug)
            Ap√≥s esse tempo, o scroll expira automaticamente.
            Padr√£o: '2m'
        
        verbose (bool, optional):
            Se True, mostra progresso detalhado.
            Se False, execu√ß√£o silenciosa.
            Padr√£o: True
    
    Returns:
        pd.DataFrame: 
            DataFrame com todos os documentos do √≠ndice, contendo colunas:
            - doc_id: ID √∫nico do documento (ex: "doc_0000")
            - text: Texto completo do documento
            - category: Categoria/classe do documento
            - target: C√≥digo num√©rico da categoria
            
            O DataFrame √© ordenado por doc_id para garantir consist√™ncia.
    
    Raises:
        Exception: 
            Qualquer erro durante a busca (conex√£o, √≠ndice n√£o existe, etc.)
            O scroll_id √© automaticamente limpo mesmo em caso de erro.
    
    Example:
        >>> from elasticsearch import Elasticsearch
        >>> es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
        >>> df = load_all_documents_from_elasticsearch(es)
        üìä Total de documentos dispon√≠veis: 18,211
        üîÑ Iniciando busca em lotes...
           Lote 1: 1,000 docs | Total acumulado: 1,000/18,211
           Lote 2: 1,000 docs | Total acumulado: 2,000/18,211
           ...
        ‚úÖ Scroll conclu√≠do e recursos liberados
        >>> print(df.shape)
        (18211, 4)
    
    Notes:
        - A Scroll API cria um snapshot dos dados no momento da busca inicial
        - Mudan√ßas no √≠ndice durante a busca N√ÉO ser√£o refletidas nos resultados
        - O scroll_id √© automaticamente limpo ao final ou em caso de erro
        - Esta fun√ß√£o √© thread-safe (cada chamada tem seu pr√≥prio scroll_id)
    
    See Also:
        - Documenta√ß√£o oficial: https://www.elastic.co/guide/en/elasticsearch/reference/current/scroll-api.html
        - Search After API: Alternativa moderna para pagina√ß√£o em tempo real
    """
    
    if verbose:
        print(f"üîÑ Buscando documentos do √≠ndice '{index_name}'")
        print(f"   M√©todo: Scroll API (recomendado para >10k docs)")
        print(f"   Tamanho do lote: {batch_size:,} documentos")
        print(f"   Timeout do scroll: {scroll_timeout}")
    
    all_documents = []
    scroll_id = None
    
    try:
        # =================================================================
        # PASSO 1: INICIAR SCROLL
        # =================================================================
        # Primeira busca que cria o "snapshot" e retorna o scroll_id
        
        response = es_client.search(
            index=index_name,
            scroll=scroll_timeout,  # Manter contexto ativo
            size=batch_size,        # Docs por lote
            body={
                "query": {"match_all": {}},  # Buscar todos os docs
                "_source": ["doc_id", "text", "category", "target"]  # Campos desejados
            }
        )
        
        # Extrair informa√ß√µes da resposta
        scroll_id = response['_scroll_id']  # Cursor para pr√≥ximas buscas
        hits = response['hits']['hits']      # Documentos encontrados
        total_docs = response['hits']['total']['value']  # Total dispon√≠vel
        
        if verbose:
            print(f"\nüìä Total de documentos dispon√≠veis: {total_docs:,}")
            print(f"üîÑ Iniciando busca em lotes...")
        
        # Adicionar primeiro lote
        all_documents.extend(hits)
        
        if verbose:
            print(f"   Lote 1: {len(hits):,} docs | Total acumulado: {len(all_documents):,}/{total_docs:,}")
        
        # =================================================================
        # PASSO 2: CONTINUAR SCROLL
        # =================================================================
        # Buscar pr√≥ximos lotes at√© n√£o haver mais dados
        
        batch_number = 2
        
        while len(hits) > 0:  # Enquanto houver documentos
            # Buscar pr√≥ximo lote usando scroll_id
            response = es_client.scroll(
                scroll_id=scroll_id,
                scroll=scroll_timeout  # Renovar timeout
            )
            
            # Atualizar scroll_id e hits
            scroll_id = response['_scroll_id']
            hits = response['hits']['hits']
            
            # Se houver documentos, adicionar √† lista
            if len(hits) > 0:
                all_documents.extend(hits)
                
                if verbose:
                    print(f"   Lote {batch_number}: {len(hits):,} docs | Total acumulado: {len(all_documents):,}/{total_docs:,}")
                
                batch_number += 1
        
        # =================================================================
        # PASSO 3: LIMPAR SCROLL
        # =================================================================
        # Liberar recursos no servidor Elasticsearch
        
        try:
            es_client.clear_scroll(scroll_id=scroll_id)
            if verbose:
                print(f"\n‚úÖ Scroll conclu√≠do e recursos liberados")
        except Exception:
            # Scroll pode ter expirado automaticamente
            if verbose:
                print(f"\n‚ö†Ô∏è  Scroll expirou automaticamente (normal)")
        
        # =================================================================
        # PASSO 4: CONVERTER PARA DATAFRAME
        # =================================================================
        # Processar documentos para formato pandas
        
        if verbose:
            print(f"\nüìä Processando {len(all_documents):,} documentos em DataFrame...")
        
        documents_data = []
        
        for hit in all_documents:
            source = hit['_source']
            documents_data.append({
                'doc_id': source['doc_id'],
                'text': source['text'],
                'category': source['category'],
                'target': source['target']
            })
        
        df = pd.DataFrame(documents_data)
        
        # Ordenar por doc_id para garantir ordem consistente entre notebooks
        df = df.sort_values('doc_id').reset_index(drop=True)
        
        if verbose:
            print(f"‚úÖ DataFrame criado com sucesso!")
        
        return df
        
    except Exception as e:
        if verbose:
            print(f"\n‚ùå ERRO ao carregar documentos: {e}")
        
        # Tentar limpar scroll mesmo em caso de erro
        if scroll_id:
            try:
                es_client.clear_scroll(scroll_id=scroll_id)
                if verbose:
                    print("   (Scroll limpo ap√≥s erro)")
            except:
                pass
        
        # Re-raise o erro original
        raise


def print_dataframe_summary(df: pd.DataFrame, expected_docs: Optional[int] = None):
    """
    Imprime um resumo detalhado do DataFrame carregado.
    
    Args:
        df: DataFrame com os documentos
        expected_docs: N√∫mero esperado de documentos (opcional)
    """
    print(f"\n{'='*60}")
    print(f"‚úÖ DATASET CARREGADO COM SUCESSO!")
    print(f"{'='*60}")
    print(f"üìä Shape: {df.shape}")
    print(f"üìã Colunas: {list(df.columns)}")
    print(f"üè∑Ô∏è  Classes √∫nicas: {df['category'].nunique()}")
    print(f"üî¢ Total de documentos: {len(df):,}")
    
    # Mostrar amostra de IDs
    doc_ids = df['doc_id'].tolist()
    print(f"üîë IDs (amostra): {doc_ids[:3]} ... {doc_ids[-3:]}")
    
    # Valida√ß√£o se fornecido expected_docs
    if expected_docs:
        tolerance = 1000
        print(f"\nüîç VALIDA√á√ÉO:")
        if abs(len(df) - expected_docs) <= tolerance:
            print(f"   ‚úÖ PASSOU: {len(df):,} documentos")
            print(f"   ‚úÖ Dentro da expectativa: ~{expected_docs:,} ¬±{tolerance:,}")
        else:
            print(f"   ‚ö†Ô∏è  AVISO: {len(df):,} documentos encontrados")
            print(f"   ‚ö†Ô∏è  Esperado: ~{expected_docs:,} ¬±{tolerance:,}")
            print(f"   üí° Verifique se o Notebook 1 foi executado completamente")

